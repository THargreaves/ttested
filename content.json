{"pages":[{"title":"","text":"Sorry, the page you were looking for could not be found. On the bright side, the inequalities for the above graphic could! Red: $$\\max\\left(\\min\\left(\\left(x-\\frac{9}{16}\\right)\\left(x-\\frac{3}{4}\\right),\\ \\left(y-\\frac{7}{16}\\right)\\left(y-\\frac{1}{4}\\right),\\\\\\max\\left(\\left(x-y+\\frac{7}{16}\\right)\\left(x-y+\\frac{3}{16}\\right),\\ \\left(x-\\frac{3}{4}\\right)\\left(y-\\frac{1}{4}\\right)\\right),0\\right),\\\\\\ \\max\\left(x\\left(x-1\\right),\\ y\\left(y-1\\right)\\right)\\right)\\le0$$ Green: $$\\min\\left(\\max\\left(\\frac{1}{128}-2\\left(x-\\frac{3}{2}\\right)^{4}+-\\left(y-\\frac{1}{2}\\right)^{4},\\\\2\\left(x-\\frac{3}{2}\\right)^{4}+\\left(y-\\frac{1}{2}\\right)^{4}-\\frac{1}{16}\\right),\\\\\\ \\left(x-\\frac{3}{2}\\right)^{2}+\\left(y-\\frac{1}{2}\\right)^{2}-\\frac{1}{256}\\right)\\le0$$ Blue: $$\\max\\left(\\min\\left(\\left(x-\\frac{41}{16}\\right)\\left(x-\\frac{11}{4}\\right),\\ \\left(y-\\frac{7}{16}\\right)\\left(y-\\frac{1}{4}\\right), \\\\\\max\\left(\\left(x-y-\\frac{25}{16}\\right)\\left(x-y-\\frac{29}{16}\\right),\\left(x-\\frac{11}{4}\\right)\\left(y-\\frac{1}{4}\\right)\\right),0\\right), \\\\\\ \\max\\left(\\left(x-2\\right)\\left(x-3\\right),\\ y\\left(y-1\\right)\\right)\\right)\\le0$$ Want a blog post explaining where these inequalities came from? Let me know and if there’s enough demand I will gladly supply. For now, perhaps you’d have better luck finding what you’re after here.","link":"/404/index.html"},{"title":"About","text":"Hello WorldHello and welcome to my blog. T-Tested is a portal into the data-y part of my mind. Here, I explore aspects of many disciplines ranging from Computer Science to Ethics, and Statistics to Business. Specifically, I look at how these fields can be combined together, with a sprinkling data-goodness, to tell interesting stories, offer new insight into the world, and help us to make better decisions. If you’re new to this site and don’t know where to start. Here are some of the articles that I would recommend to get your feet wet: The Inaccuracy of Accuracy Rethinking the T9 Layout A Statistician’s Guide to Playing Darts Happy reading! About MeFor an in-depth overview of my eductation, experiences, and project work, please see my CV at this link. EducationI am currently between my second and third year of a degree in Mathematics and Statistics at the University of Warwick, undertaking a placement year at AstraZeneca. I have aimed to keep a wide foundation through my module choices, covering several topics in pure mathematics, operational research, and computer science over my first two years. Work ExperienceAs mentioned above, I am currently working as a data science placement student at AstraZeneca. This work involves the use of R, Python, and SQL, as well as some more traditional skills such as statistics and communication to tackle a wide array of problems in the world of chemical development. I have previous experience working with dunnhumby—a global data science consultancy firm—and Bet365. I have also worked as a tutor, preparing students for SATs, GCSEs, and A-levels in mathematics as well as the 11+ entrance exam. ProjectsI am a sucker for an interesting project. Many of the projects that I get involved in or start by myself end up on this website. There are some which are less sorted to this format though, and so I will simply link to their origin locations from here. An Introduction to the Tidyverse The 12 Days of Python (Link coming soon) MyTrix—a custom-built linear algebra package CaboPy (Link coming soon) OpinionsThis will not come as a surprise to anyone who knows me—I have opinions; lots of them. I will include a small selection here to be taken with a pinch of salt. The refusal to follow PEP 8/257 in large, collaborative projects makes one no better than a wild animal The way you cut your toast in the morning is an important detail worthy of interrogation The default ggplot2 theme is an aesthetic-abomination Java isn’t that bad. Stop whining (I will throw Linux in the same pile) Pandas is the wild west of data science. I will use the tidyverse over it on any day","link":"/about/index.html"}],"posts":[{"title":"#AdventGate—The shocking secret Big Advent doesn't want you to know","text":"Post Structure To make this post accessible (yet still relevant) to as wide an audience as possible, I have split it up into three sections. The first (~8 minute read) will simply tell the story of my discovery, avoiding any mention of statistics or mathematics in the process. The second is intended for the applied scientist, discussing the statistical methods I used to reach my conclusion, without going into the technical details. The last section is for the theorist, the mathematicians and statisticians who want to see the derivation of the methods I used and proofs of their validatity. As a bonus, anyone interested in the Python code used to perform this analysis and produce the graphics in this post can find the source notebook here. Section One - The WhatIn certain ways, I never really stopped being a kid. So, as December drew closer, I found myself eagerly waiting to tear open the first door of my advent calendar. Perhaps a standard chocolate advent calendar wouldn’t be too out of place at my age, but the Lego Star Wars themed variant that I had in my possession (with ages 6–14 stamped on the front) was perhaps a more unusual choice. Despite this, I was happy; two of my favourite things—Star Wars and Lego—combined together was always going to be a recipe for success. After closer inspection though, this happiness quickly changed to suspicion. After ripping off the plastic packing and folding back the cardboard covering, I was greeted by the standard 4 by 6 grid of numbers you would expect to find on any normal advent calendar. Except something wasn’t quite right. Against All OddsAlthough many of the original advent calendars had their doors arranged in a regular order, most advent calendars you can buy today have a (seemingly—as we shall soon see) random layout. The belief in this randomness has proliferated through society, with Wikipedia saying that the “doors are distributed across the calendar in no particular order”—and when have they ever been wrong? As a mathematician, I am always on the look out for patterns; and as a statistician, I often find myself thinking about the odds of a particular event occurring. Combining these two tendencies together, I noticed that there was a distinct lack of pattern in the layout of these numbers, so much so that it seemed against all odds that such a distribution would have come out by random chance. In particular, I noticed that not a single pair of consecutive numbers (e.g. 4 and 5, 20 and 21) were directly adjacent to each other on the grid. To anyone that hasn’t given the notion of randomness much deep thought (I accept that I’m the outlier here and that this accounts for most people) there may seem nothing wrong with this. After all, if these layouts are truly random, why would we expect to obtain any patterns such as the adjacent consecutive numbers I so desired? This argument seems convincing but is actually flawed. The problem is that humans (even statisticians at times) are quite bad at judging what randomness really looks like. It is not hard to find numerous case studies demonstrating this: Apple making the iTunes shuffle function less random, so it feel more random Humans struggling to generate ‘random’ sequences of coin tosses Seven appearing 14% more often than by random chance when people are asked to pick a random number between one and ten. There is an entire field of psychological research dedicated to this phenomenon but the sticky-note-summary is that we are simply not very good at judging what randomness should look like. With this in mind (and unsure of my own ability to judge randomness), I decided to perform a back-of-the-envelope calculation to determine roughly how likely it was to have no adjacent consecutive numbers in a completely random layout. The exact details of this calculation I will save for the next section but the headline figure is that this event would only occur 3.3% of the time if the layouts were truely random. Colour me dubious. Just to CheckThat was only a rough calculation though. I had made some assumptions that may or may not have held in the real world. By this point though, the statistics bug had bit me and I wasn’t going to stop until I had an answer. Calculating the exact odds of a random layout having no adjacent consecutive doors is actually quite a difficult task. I decided a better approach would be use sheer computational power to simulate several million random layouts and see how many of them had no adjacent consecutive doors. Unsurprisingly, generating such a large number of layouts took some time but the incredible amount of heat my laptop pumped out as it whirred away performing these computations, keeping me warm on a vastly under-heated London Northwestern train, was much appreciated. The result of these simulations is as follows. The rough estimate wasn’t so bad after all! With the same (or at least similar) odds of 3.3%, the chance of having so few adjacent consecutive doors is certainly low enough to cast doubt. Yet, coincidences do happen. How can we be confident that I didn’t just have an unusually distributed layout. To move towards a concrete answer, we’ll need some data… Branching OutBefore I launched myself down the rabbit hole, I thought it would be a good idea to check whether someone had looked into this topic before. Perhaps I could save myself some time. A quick Google of “Are advent calendars actually random?”, however, returned no results of relevance. There are clearly two possibilities. 1) I am showcasing a level of statistical-obsession towards advent calendars that the world is yet to have witnessed 2) Google is in on the Big Advent conspiracy and is censoring other people’s research on the topic; fearful that the truth might get out Using Occam’s Razor it is safe to conclude that the latter must the true reason. Big Advent is a powerful entity though, and one lone example will not bring it down. I therefore decided that the next step in this journey would be to go out and collect data. That is, I needed the layout of other advent calendars. The idea being that the more suspiciously laid-out calendars I could find, the more confident I would become that the disparity wasn’t down to chance. Data Restrictions To keep things simple, I limited myself to only searching for 24 day advent calendars with their doors arranged in a 6 by 4 grid (or 4 by 6, in which case I read them off at a 90 degree angle). Many advent calendars nowadays come with 25 or even 31 doors. This is likely to mess up the grid layout by including doors of different sizes making the mathematics of this project unnecessarily complicated. Besides, isn’t Christmas day already filled with plenty of treats?—an extra chocolate on that day seems a touch overkill to me. Anyone that is close to me will certainly be aware of this search for data. For the last few weeks it has been close to impossible to have a conversation with me without me asking about the ownership and details of one’s advent calendar. The security team at my local Tesco may also be aware, as my multiple trips into the store solely to take pictures of advent calendars and then leave without a purchase may have caught their attention—I only hope I’ve not been added to any lists. The fruit of my labour makes it worth the while; 25 unique layouts of advent calendars, hopefully representative of the general population. Clarification To explain this last point, I didn’t specifically choose these calendars because of their layout—that would invalidate my statistics—but instead recorded the layout of every calendar I saw whether it helped me or not. Instead of using the counts of adjacent consecutive doors as before, I wanted to consider a more all-encompassing metric for this final push. I decided that the best thing to look at would be the total sum of the distances between each consecutive pair of numbers across the calendar. I arbitrarily defined the distance between two adjacent doors to be 1 and so, for example, the distance between the bottom left and bottom right door of a vertical calender would be 5 and the distance of the diagonally opposite doors would be $\\sqrt{3^2 + 5 ^2} = \\sqrt{34}$ using our trusted Pythagorean theorem from high school. With this in mind, I simulated another several million random layouts to find what distribution this metric should have if the layouts were actually random. I then plotted the likelihood curve for this distribution with the values for my observed, real-world calendars added as red lines. Clarification The vertical axis of this plot may be a bit confusing to some. Essentially, it is measuring how likely a certain total distance is to occur when the layouts are randomly generated. From this, we can see that the most common distance is around 62. It is not hard to see that the real-life advent calendars tend to have a higher total distance between consecutive doors than expected. In other words, it seems that the real advent calendars have their numbers much more spaced out than would be likely if they were actually random. Can we quantify this discrepancy? Suppose that the real-life advent calendars did in fact have randomly distributed doors. What would the probability be that we saw a result as significant as this? The statistical details of finding this probability are a bit messy but when we work through the maths, we come out with the following number. p-value using Fisher&apos;s Method: 0.001059Put in simple English, the chance that we would have obtained the distribution of total distances that we saw in our real-life examples if they were in fact random is around 0.11%. In other words, the chance of that occurring is around 1 in 1000—tossing 10 heads in a row with a fair coin is about as likely. That’s certainly enough evidence for me to throw away the belief that these calendars’ layouts could indeed by random. Your browser does not support the video tag. I’m not sure what to do with this information now. I think I’m in shock. I’ve been lied to about the true nature of advent calendars for my entire life and it’s going to take me a while to get over that. I’m sure WikiLeaks would be interested in this result although I’m afraid of what the consequences of revealing this information might be—Big Advent likely have their fingers in many pies after all. Perhaps it’s best if we just keep this as our little secret? Or, well, I wouldn’t exactly mind if you were to share this post with a few trusted others… Section Two - The HowWith the bulk of this post out of the way, let’s start tying together any loose ends, though without going too heavy into the mathematical details. Rough WorkingFirst, how did I come up with that initial estimate that a random layout would have no adjacent consecutive doors only 3.3% of the time? In order to make this calculation simple to compute by hand (with the support of a calculator, of course), I had to make the assumption that the events that any two consecutive doors are adjacent are independent of one another. Essentially (though not exactly—see below), I assumed that one consecutive pair being adjacent or not had no influence on whether the others were. Funnily enough, I’ve struggled to show (though I didn’t give it much thought) that they aren’t independent. Despite this, my instinct says that they shouldn’t be and it’s best to play it safe so this is indeed an assumption. If anyone can offer a simple example showing that these events are not independent, feel free to share it in this post’s comments. Note for Pedants That explanation above is actually closer to pairwise independence, than mutual independence. The latter is what I assumed but the difference is minor enough that I hope it’s okay to let it slide. The beauty of this assumption is that if we know that probability that any two consecutive doors are adjacent (we’ll call this $p_\\textrm{adj}$), our independence assumption gives us the probability that there are no consecutive adjacent doors to be $\\left(1-p_\\textrm{adj}\\right)^{23}$. This is due to the product rule for independent events combined with the fact that there are 23 such pairs. Finding $p_\\textrm{adj}$ is not too hard either. We do this by breaking the doors of the advent calendar up into 3 types depending on how many neighbours they have. We then consider a pair of doors chosen uniformly at random from all possible choices. We ask which type of door the first is. It is not hard to see that there is $\\frac{8}{24} = \\frac{1}{3}$ chance of having a door with four neighbours, $\\frac{12}{24} = \\frac{1}{2}$ for three neighbours, and $\\frac{4}{24} = \\frac{1}{6}$ for a lonely two neighbours. In each of these cases the chance that the other door is adjacent to the first is $\\frac{4}{23}$, $\\frac{3}{23}$ and $\\frac{2}{23}$, respectively. This covers all possible cases and so we have, $$p_\\textrm{adj} = \\frac{1}{3}\\cdot\\frac{4}{23} + \\frac{1}{2}\\cdot\\frac{3}{23} + \\frac{1}{6}\\cdot\\frac{2}{23} = \\frac{19}{138} \\approx 0.1377$$ Substituting this into the expression we had above, the probability (under the assumption of independence) that we have no adjacent consecutive doors is $$\\left(1 - \\frac{19}{138}\\right)^{23} \\approx 0.0331$$ as we had before. There is Only One Test!I love the statistics of this problem because it reminds of a cherished blog post that I came across around a year ago titled, “There is Only One Test!“. The post is definitely worth a read (the same could be said for Allen’s entire blog) but I will briefly summarise. Statistics is often taught in an unhelpful way, treating the process of hypothesis testing as a game of hide-and-seek in which we aim to find an appropriate test from our arsenal of normals, t’s, chi-squareds and F’s, and blindly apply it with little thought to the underlying reasons for doing this. In the end, we’re always trying to answer the same question: “Suppose we are in a system where our apparent effect was purely due to chance (the null hypothesis); what is the probability that the effect we saw occurs. If this probability is very small, then we should probably reevaluate whether the null hypothesis is correct”. Focusing too heavily on which test is best is often unhelpful. A better way to think is to focus on what the distribution of our test statistic is under the null hypothesis. If that distribution so happens to be of the various forms we mentioned above, then great, we have a shortcut to compute the answer. The problem would still be tractable if the distribution didn’t have such a nice form, we’d just be forced to use a more complicated method such as simulation, approximation, bootstrapping, or Monte Carlo methods to find the probability of interest. That’s the key idea—there is nothing special about the common statistical tests that we learn in an introductory statistics class. They are just computational tools to make our lives easier; no matter which we use, the underlying test is the same. This is exactly the train of thought I employed to tackle this problem. I had a test statistic of interest, namely the total distance between consecutive doors, and I wanted to know the likelihood that we would have seen the observed real-world values of this statistic given that the layouts were truly random. Did I know what distribution this statistic followed? Well, kind of. The Central Limit Theorem suggests that it should be reasonably normal (as it is the sum of 23 identically distributed distances), but requires an assumption of independence. Even though the individual distances of pairs are only weakly correlated, they are definitely not independent and so I didn’t want to rely too heavily on this approach. Example If we already had two pairs each separated by distance $\\sqrt{34}$ (diagonally opposite corners), another pair of the same distance would be impossible. Hence the events are not independent. Instead, I decided to simply simulate this statistic a few million times. This gave me a strong idea of what the true distribution of the statistic under the null hypothesis should look like. Yes, it did turn out to be reasonably normally distributed (though perhaps with a longer negative tail than would be desired) but the maximum likelihood estimators of the mean and standard deviation were far from what I would have expected using the central limit theorem so it looks like this was a good call. With this simulated distribution, it is then trivial to calculate p-values for any particular observation; we simply count how many of the simulated statistics lie above the observed value and divide this by the total number of simulations. This is exactly what a p-value is: the proportion of observations that lie above (in this case at least) a certain value when we assume the null hypothesis. Something’s FishyAs discussed above, simulation gives us a great way to calculate p-values for our observations without having to resort to approximations, fallacious assumptions, or attempting to forcibly transform our statistic to match a common distribution. These p-values alone are not enough to come to a conclusion though. If we look at the data for the Lego Star Wars advent calendar shown above, it turns out that the total distance between consecutive doors only comes to $61.3$. That’s actually less than the mean of the simulated distribution. Yes, there may be no adjacent consecutive doors, but overall, there isn’t anything too special about the total distance between the pairs, leading us to a pathetic p-value of $0.54$. This is a long way from the $&lt;0.05$ we typically set as a threshold for rejecting the null hypothesis. Don’t fear though, we have 24 other observations, remember? What do the p-values look like for them. The plot below shows this. Our Lego Star Wars calendar may not have had an extreme enough layout to achieve the standard $0.05$ significance threshold but there are several that do. We can therefore reject the null hypothesis, right? Not so fast. If we were to stop our analysis here, we would have fallen for a statistical fallacy known a p-hacking. Yes, there may be some observations with p-values below 0.05 but it’s worth remembering what that means. A p-value of 0.05 is equivalent to saying that, under the null hypothesis, we would have only expected to see such a result 5% of the time. That is, one in every twenty-or-so observations. But wait, we have 25 observations here so we shouldn’t be at all surprised that at least one of them achieves statistical significance when we look at it by itself. If we want to do things properly, we will have to use a data fusion method. The most common of these is called Fisher’s method, named after the legendary statistician Ronald Fisher. This involves looking at a second meta test statistic, $X^2$, given by $$X^2 = -2\\sum_{i=1}^k \\ln p_i$$ where $p_i$ is the p-value of the $i$th observation and $k$ is the total number of observations. This statistic, in a sense, combines or fuses the 25 statistics we obtained from our analysis and corrects p-values so that we don’t fall victim to p-hacking. Provided that the observations are independent (which they certainly are in our case), it is not hard to show (and we will in the next section) that this follows a chi-squared distribution with $2k$ degrees of freedom. We can therefore calculate this meta statistic using the p-values obtained from our 25 regular statistics and compare it to a chi-squared distribution with 50 degrees of freedom. In our case, $X^2$ comes out to be $86.415$ which a distribution table or online calculator will show to be equivalent to the p-value of $0.00106$ that we saw in the first section. Section Three - The WhyTo wrap this post up, I will offer a complete proof of the validity of Fisher’s method so that the die-hard mathematicians/statisticians can sleep easy. Our starting point is noticing that under the null-hypothesis, p-values are uniformly distributed on the interval $(0,1)$. From my experience, there are two types of people; those who see this as an obvious statement, and those who are rather dubious of the claim. For some reason there is rarely any in-between. If you fall into the latter camp, I will provide a short explanation. This result follows directly from the definition of a p-value. Remember, a p-value less than $x \\in (0, 1)$ under the null hypothesis occurs with probability $x$. In other words, if we treat $p$ as a random variable $$F_p(x) = \\mathbb{P}_0\\left(p \\leq x\\right) = x, \\quad x \\in (0,1)$$ But that is exactly the definition of a uniform random variable on $(0, 1)$, just as we were after. Taking LogsAs a reminder, Fisher’s method concerned the statistic $$X^2 = -2\\sum_{i=1}^k \\ln p_i$$ claiming that it was distributed according to $\\chi^2_{2k}$ under $H_0$. Although this is a clearer form for computing the statistic, for the derivation of the method it is easier to write $$X^2 = \\sum_{i=1}^k 2\\left( -\\ln p_i\\right)$$ It will become evident as to why this is by the end of this section. From above, and noting that our observations are both independent and have identically distributed statistics we have that $$p_i \\stackrel{\\textrm{iid}}\\sim \\textrm{Unif}(0, 1), \\quad i \\in [k]$$ What about $-\\ln p_i$? It turns out that it has an standard (rate 1) exponential distribution. The proof of this statement is below. $$\\begin{align}\\mathbb{P}(-\\ln p_i \\leq x) &amp; = \\mathbb{P}(\\ln p_i \\geq -x)\\\\&amp; = \\mathbb{P}(p_i \\geq e^{-x}) \\\\&amp; = 1 - \\mathbb{P}(p_i \\leq e^{-x}) \\\\&amp; = 1 - \\begin{cases}0, &amp; e^{-x} &lt; 0\\\\e^{-x}, &amp; 0 \\leq e^{-x} \\leq 1\\\\1, &amp; e^{-x} &gt; 1\\end{cases} \\\\&amp; = 1 - \\begin{cases}e^{-x}, &amp; 0 \\leq x\\\\1, &amp; x &lt; 0\\end{cases} \\\\&amp; = \\begin{cases}1 - e^{-x}, &amp; 0 \\leq x\\\\0, &amp; x &lt; 0\\end{cases}\\end{align}$$ In which the last expression is the cumulative density function of a standard exponential distribution. Note that to jump from line 3 to 4, we used the fact that the $p_i$’s are uniformly distributed on $(0, 1)$. Clarification There is nothing specifically relating to p-values in this result. A general statement would be that the negative logarithm of a standard uniform random variable has a standard exponential distribution, which holds in any context. Size Doesn’t MatterThat’s great and all, but we don’t just have a $-\\ln p_i$ in our expression for $X^2$ but rather two lots of this quantity. Thankfully, there is a useful result which states that the exponential distribution is closed under scaling by a positive constant. More specifically, if $X \\sim \\textrm{Exp}(\\lambda)$ then $kX\\sim \\textrm{Exp}\\left(\\frac{\\lambda}{k}\\right)$. It follows that the distribution of $2\\left(-\\ln p_i\\right)$ is exponential with rate parameter $\\frac{1}{2}$. The proof of this general result is short and sweet. If we let $Y = kX$ above then $$F_Y(y) = \\mathbb{P}(Y \\leq y) = \\mathbb{P}\\left(X \\leq \\frac{y}{k}\\right) = F_X\\left(\\frac{y}{k}\\right)$$ By looking at the cumulative density function of an exponential random variable, it is then not hard to see that $Y$ is exponential with rate $\\frac{\\lambda}{k}$. Clarification For those that would like a more explicit solution, there is an alternative approach we can use. If we set $x = \\frac{y}{k}$, it follows that $$\\begin{align}f_Y(y) &amp;= \\frac{dF_Y(y)}{dy} = \\frac{dF_X\\left(\\frac{y}{k}\\right)}{dy} = \\frac{dx}{dy}\\frac{dF_X\\left(x\\right)}{dx} \\\\&amp; = \\frac{1}{k} f_X(x) = \\frac{\\lambda}{k} e^{-\\lambda x} = \\frac{\\lambda}{k} e^{-\\frac{\\lambda}{k}y}\\end{align}$$ and so $Y$ is exponential with rate $\\frac{\\lambda}{k}$ as we had before. A Distribution by Many NamesAt this point we have that $$2\\left(-\\ln p_i\\right) \\stackrel{\\textrm{iid}}\\sim \\textrm{Exp}\\left(\\frac{1}{2}\\right), \\quad i \\in [k]$$ so where does this Chi-squared distribution come from? As a matter of fact, we have it already. Recall (or take my word for it), that the probability density function of a chi-squared random variable with $\\nu$ degrees of freedom is given by $$f_{\\chi^2_\\nu}(x) = \\frac{1}{2 ^ {\\frac{\\nu}{2}}\\Gamma\\left(\\frac{\\nu}{2}\\right)}x^{\\frac{\\nu}{2}-1}\\exp^{-\\frac{x}{2}}$$ In the special case where $\\nu = 2$ (note that $\\Gamma(1) = 0! = 1$ by convention) this becomes $$f_{\\chi^2_\\nu}(x) = \\frac{1}{2}e^{-\\frac{x}{2}}$$ which is simply the density function of an exponential random variable with rate $\\frac{1}{2}$. It all comes together! To Sum it UpThe equivalence above means that we can instead write $$2\\left(-\\ln p_i\\right) \\stackrel{\\textrm{iid}}\\sim \\chi^2_2, \\quad i \\in [k]$$ The finish line is in sight. All we have to do is sum up these $k$ independent chi-squared random variables and show that the resultant random variable follows a chi-squared distribution with $2k$ degrees of freedom. This additivity property follows directly from the definition of the chi-squared distribution. This states that a random variable $X$ has a chi-squared distribution with $\\nu$ degrees of freedom if and only if it can be written as $$X = \\sum_{i=1}^{\\nu}Z_i^2$$ where the $Z_i$s are independent random variables, all following a standard normal distribution. That is $Z_i \\stackrel{\\textrm{iid}}\\sim \\textrm{N}(0, 1), i \\in [\\nu]$. We can therefore write $$X_2 = \\sum_{i=1}^k 2\\left( -\\ln p_i\\right) = \\sum_{i=1}^k \\left(Z^2_{i, 1} + Z^2_{i, 2}\\right)$$ where $Z_{i, j} \\stackrel{\\textrm{iid}}\\sim \\textrm{N}(0, 1), i \\in [k], j \\in {1, 2}$. Since each $2\\left(- \\ln p_i\\right)$ are independent, so must all of the $Z_{i, j}$. There are $2k$ of these in total and so $X^2 \\sim \\chi^2_{2k}$, completing this proof and in turn, this post. I hope you found this supplementary dive into the theory to be beneficial.","link":"/adventgate/"},{"title":"AI is Not as Smart as You May Think (but That's No Reason Not to Be Worried)","text":"This post is one of a two-part narrative discussing the need to focus on the current, pressing concerns with AI, rather than obsessing with the distracting dangers that lie decades away. This post will discuss how AI is perhaps not as intelligent as the mass media or film industry would lead you to believe by focusing on examples of its shortcomings and limitations. The other half of this pair is written by Orlanda Gill—a National Security Studies master’s student, at King’s College London—in which she discusses the inherent risks that stem from the current implementation of sub-par AI systems in military applications. IntroductionPartly due to the influence of many recent AI-centred films gaining mainstream popularity, the technological zeitgeist of this last decade has slowly morphed to become generally fearful and expectant of a looming AI apocalypse. Movies such as Ex Machina and Blade Runner 2049, or the hit television series Black Mirror’s Metalhead all depict a world in which the every-growing power of AI systems has led to an existential threat to humanity at large. Tracing back even further, we can see more examples of this train of thought; The Terminator, 2001: A Space Odyssey, and I, Robot all warn of the same dark fate. This depressing forecast has greatly penetrated the public consciousness: A 2018 survey conducted by the Center for the Governance of AI of 2,000 US adults found that the average respondent ranked AI concerns of the future such as critical AI safety failures and near-extinction events as nearly as important as present day concerns with AI. A similar survey conducted by the Royal Society, likewise showed that the general public is far more concerned by the notion that a robot—deriving its decisions from misaligned AI—may one day cause them physical harm than the often subtle ways that AI systems can currently cause damage. That is not to say that we shouldn’t worry at all about the long-term risks of AI. They certainly do exist and are worthy of attention, just not at the detriment of focusing on more pressing issues that we will definitely be faced with over the coming decade. I think that Andrew Ng, infamous AI and machine learning researcher, summarises this view best. Worrying about AI evil superintelligence today is like worrying about overpopulation on the planet Mars. We haven’t even landed on the planet yet! Andrew Ng The key message is that although AI has a large potential for existential threat, we are still a long, long way from such a world. The rate of development of AI is often exaggerated by a headline-hungry news media. In reality, despite the occasional small breakthrough, the current progress of AI is built far more on small steps than large leaps. On top of this, modern AI approaches (especially those taking advantage of neural networks) have severe limitations that researchers are struggling to overcome. In fact, many of the “cutting-edge” AI algorithms used by leading tech firms this decade were popular statistical literature of the 1980s but now with more data and faster processors thrown at them. This is the point that I want to demonstrate with this post. Modern AI systems are still lacking maturity and there are serious obstacles that we must overcome before any threat from AI could possibly present itself. Despite these limitations, however, military implementations of AI systems are starting to become mainstream. This is a real threat of AI: biased, unreliable, and opaque algorithms being used to determine who lives and who dies. This is not the imaginings of a sci-fi film writer’s creative mind but a real threat that the world is facing right now. The current state of the military’s relationship with AI is discussed in-depth in this post’s counterpart. I would highly recommend giving this sister-piece a read to gain a rounded picture of the current state of the threats from AI, both from a technological (me) and humanitarian (Orlanda) perspective. For now, let’s begin our exploration of the shortcomings of modern AI systems. What challenges do they face and how does this limit their utility? Part I: GeneralisationThe first of two key challenges we will look at is that of generalisation. That is, how do modern AI systems react to new, unseen scenarios. We will focus on a particular subset of AI, known as neural networks, which are commonly used in computer vision and strategic decision-making. These attempt to mimic the structure of the human brain, in order to capture complexity in the data they are trained with, that conventional methods would struggle to. The flip side of this is that, in trying to explain the intricacies of the data they have seen before, they can become incapable of handling any new inputs that deviate from what they’re used to. Aside from training these models on literally every possible scenario that could arise, it becomes extremely difficult to manage this uncertainty or even to predict when things might go wrong. The (Not So) Beautiful GameLuckily for us, when things do go wrong, they can be rather amusing. Out of all of the examples of AI systems failing I have come across (and boy, are there a lot), the following is easily my favourite. It is taken from this 2019 paper and concerns the use of neural networks to teach two AI agents how to take part in a penalty shootout. The red agent plays the goalkeeper whilst the blue agent plays the shooter. The two agents were trained by using a technique known as Generative Adversial Networks, in which the computer ‘plays itself’ a vast number of times, improving the skill levels of both agents in the process. After a billion or so ‘practice sessions’ the agents look like this: Your browser does not support the video tag. They may not be ready for the Premier League just yet (or perhaps even the school playground) but at least we can recognise their behaviour as playing a game of penalty shootout. The shooter knows where to aim and can kick the ball with reasonable accuracy, and the goalkeeper makes valiant attempts at diving for the ball. The final scoreboard (7-11) would suggest that the shooter is doing a slightly better job than the goalkeeper. The aim of the research team behind the paper was to turn this around. They attempted to adjust the goalkeeper’s defence strategy so that the shooter would find it more difficult to score. What makes this process interesting though, is that they didn’t attempt to do it by making the goalkeeper dive better, move faster, or more accurately prediction the ball’s path. Instead, they just told it to lie down: Your browser does not support the video tag. You would think that this strategy would be a recipe for defeat and yet it completely stumps the shooter AI, leaving the keeper to finish with a comfortable 7-point lead. Forget scoring a goal, the shooter can barely even walk anymore! It is worth emphasing that the shooter agent in this example is the exact same as in the previous one; it is only the changing behaviour of the keeper that causes the shooter to struggle in response. This is because the input the shooter is now recieving is a long way from what it is used to seeing. The complexity of the network behind its motion has been trained to respond to the fine details of the ball’s position and goalkeeper’s stance and in doing so has made it incapable of handling any input that deviates significantly from this norm. The need to have generalisable models is extremely important. Take self-driving cars for example. If a human were to drive down a residential street and hear an ice-cream van’s melody echo around the houses, they would likely be extra cautious as a child could run out at any moment desperate to grab a snack before the van gets away. Most humans have never been exposed to a scenario in which they needed to urgently brake to avoid a ice-cream-obsessed child, and yet we know that it is a possibility anyway. Would an AI system necessarily understand how to respond to this or the thousands of other similar edge-cases without first being exposed to it and risking finding out the hard way? Linking in with Orlanda’s post about the use of this type of AI in military applications, should we really be using systems that can be tricked so easily into performly completely erratically when the consequences are so significant? Cheaters Never ProsperWhereas the example above failed to generalise to significantly different input data, it is not hard to come across examples in which only seemingly small changes to the input can lead to wildly different behaviour by an AI. This is commonly caused by overfitting, in which a neural network gives too much weight to certain inputs and so becomes highly susceptible to changes in their values. This is most severe when the input data contains a feature that is not present in typical real-world testing data. In this case, the neural network can have an easy time training, outputting high confidence in its own abilities, even though its predictions are largely based on features of the training data that do not generalise. Then when it comes time to test the model on unseen data, catastrophe can be unleashed. This can almost be thought of as the neural network ‘cheating’. Like a school-child using notes written up their sleeves to ace a test, training a model on unrepresentative data can lead to it performing better than it naturally should. Then, when exposed to real world data, the true errors of the model can become evident, just as the school-child might discover when they are forced to resit the test under the watchful eye of an invigilator. Furthermore, the school-child may now do worse since their confidence in their ability to always be able to cheat led them to never bothering to revise. Likewise, a model that was trained on biased data may perform worse on new data than otherwise would be the case. A classic example of this is the story of the self-driving car, ALVINN. Perhaps not as slick-looking as a Tesla, but considering it was first let loose on UK roads in 1989, arguably more adventurous. ALVINN was powered by the same neural network machinary used in almost all modern self-driving systems. Admittedly, the networks were of a much smaller scale to match the limited computational power of the era but the idea was there. Despite its primitivity, ALVINN was surprisingly successful; capable of independently navigating a simple road with ease. Things took a turn for the worst, however, when ALVINN found itself approaching a bridge. It’s previously calm and controlled driving gave way to erratic swerving, dangerously twisting out of control until the human monitoring the vehicle was forced to take hold of the wheel to avoid a crash. What went wrong? After some time digging through the data, the researchers behind the experiment found that ALVINN had (in a sense) been cheating. ALVINN’s training had mainly consisted of driving down roads with a grass verge along the side. It had therefore learned that if it could just keep this green strip on its left parallel to where it was heading, everything would be alright. That is, alright until that grassy guideline gives way to the stone walls of a bridge. Since ALVINN had weighted the position of this green patch so highly in its decision making, it struggled to know what to do without it, resorting to flailing around wildly, hoping to come across it again and restore order. Another story from AI folklore tells of a US Army research venture in which a team of AI experts attempted to build a neural network that could detect camouflaged tanks given a candidate image. After working through the standard machine learning techniques of the era, the researchers ended up with a model that was remarkably accurate. The team then allegedly handed their final product into the Pentagon for review, which promptly handed it back, complaining that their own tests found the neural network to perform no better than a coin-toss. The story goes that the researcher’s dataset had an inherent bias—the photos of camouflaged tanks had been taken on cloudy days, whilst the photos of tank-less forests had been taken on sunny days. Since the overall colour-temperature of the image correlated so well with the presence of a tank, the model never bothered to actually learn how to detect a tank but rather how to read the weather! Although this exact tale may well be apocryphal, there are many well-documented cases in which similar (though often more subtle) scenarios have arose. A common example of this is a neural network developed by Google for labeling images. One challenge it had to tackle was differentiating between images of Huskies and Wolves. It turns out the network’s decision was based heavily on the presence of grass or snow. This assumption wasn’t awful in most cases and resulted in high internal prediction accuracy, but present the model with a picture of a wolf on grass and it may be a little too confident in telling you to go and give it a pet. These examples are obviously extremes in which it is clear how the neural network overfitted to the input data (or at least in hindsight it is - it took the original researchers some time to piece together what was going wrong with both examples). In most cases though, the differences between the training and testing data may be subtle and remain unknown until too late. Part II: RobustnessThe second (and final) challenge we will discuss regards that lack of robustness that modern AI systems can be subject too. Although robustness has a rigourous statistical definition, in this post I want to talk about it in a more general sense, relating to how difficult it is to break a model and how easily unwanted predictive factors can ‘leak’ into our predictions. We will again mainly focus on the use of neural networks for computer vision as this is such a critical area of modern AI research, though we will also take a detour to discuss the use of AI for predictive crime models and the difficult ethical discussions this can lead to. Invisible AttacksAs is the way of the technological world, as soon as someone develops a new algorithm or tool, another will crack open their PC to start breaking it. We will start by looking at a technique known as one pixel attacks (or its slightly sexier name, pixel-hacking). This is a method used to trick neural networks designed to label images by making minimal changes to the input data. In particular, we take an image that such a neural network can confidently label and, by changing just one lone pixel, completely alter the network’s prediction. A selection of such attacks is shown below, the blue label giving the networks prediction after pixel-hacking has occured alongside its confidence in the choice.For such low resolution images, the editing of one pixel is certainly noticable. For a high resolution image, however, only the most eager-eyed people might be able to notice anything wrong, and even then they may need to be prompted to bother looking for inconsistencies in the first place. These attacks work by exploiting the underlying structure of the neural network. In trying to capture all the detail of the training data the model is presented with, it may unknowingly assign an inappropriately heavy weighting to particular pixels in determining the overall label of the image. By tweaking the value of such a pixel, the output of the model can be altered significantly. With enough attention though, it is likely that these attacks could be foiled by even a non-technologist. We’re not done yet though. If changing one pixel can affect a model’s predictions so significantly, what can be done by controlling all of them? This leads us into the discussion of random-noise attacks. In this, an image is combined with carefully choosen random noise in such a way that the resulting image is virtually indistinguishable from the original yet the neural network being attacked drastically changes its prediction. One example of such an attack is below.Apart from the image on the right being slightly more noisy than that on the left, there is really nothing to distinguish the two by eye. Given only the second image, it is likely that even with the aid of computation, you wouldn’t be able to check whether it was a carefully crafted adversial example without first letting the neural network loose on the image. At least with the generalisation issue above, there was a clear reason as to why predictions were going awry under new inputs. With this example however, that is not the case—a far more worrying outcome. Pedal to the MetalThe above attacks worked by altering the image in its digital form. Adversarial attacks against neural networks are not limited to this approach, however. This is demonstrated in the following paper in which a research team attempt to fool a self-driving car’s street sign detection model by applying small black and white pieces of tape to a physical stop sign. The team aimed to mimic the look of graffiti naturally found on stop signs to make the perturbations less suspicious to a human driver. The chosen tape layout is shown on the right-hand side of the image below. With the addition of only four small pieces of monochrome tape, the car’s interpretation of the sign went straight from ‘stop’ to a confident prediction that this was a 45 km/h speed limit sign. For obvious reasons, this system was never tested outside of the lab, but who is to know how a self-driving car would react if it came across such an altered sign in the real world. Worse still, if a stop sign can be made to read as a 45 km/h speed limit sign, who’s to say that the same isn’t possible for higher speed limits. With Great Power Comes Great FragilityThe final example I wish to discuss differs from the previous two but still relates to the issue of robustness discussed above (or at least the loose, non-statistical definition). This involves the use of neural networks for predicting crime. Although not quite on the Minority Report level of predicting the exact location, offender, and circumstance of a crime, these algorithms are designed to indicate which areas might be a hotspot for crime over the next few hours/days/weeks so that more police officers can be allocated to patrol there. For example, it is known that burglars are likely to visit the same house—or at least the same neighbourhood—in the weeks following an initial break-in. Some research even suggests that the repeat burglary rate within six months of the first incident could be as high as 51%. Taking into account information such as this and many other predictors such as weather and time of year, it is possible to build a model that can reasonably predict where crime is more likely to take place. In a particular example of such an algorithm discussed in Cathy O’Neil’s Weapons of Math Destruction, the team behind the model made it clear that they did not include any demographic predictors of each area’s race in an attempt to remove any racial bias that their model could develop. Despite this, a later analysis of the model’s behaviour found that it did have racial biases. Where did these come from? The answer comes in the form of ZIP codes. Although the race wasn’t included directly in the input data, the model had managed to pick up patterns from the available predictors that allowed it to somewhat piece together the racial demographic of each area, influencing its predictions in the process. There are many examples in data science in which a variable—deliberately left out of the training data—did in fact influence the model’s predictions through another correlated proxy variable that was included. Avoiding this can often be difficult and involves strict ethical vigilance. ConclusionThe future has a lot in store for the field of AI. There is no doubt that over the coming decades there will be great innovations, important discoveries, and—as a consequence—serious dilemmas to discuss. We will be faced with great challenges and will have to work hard to overcome them. It is important, however, that we do not let a fear of the future prevent us from seeing the real risks that exist today. I hope that this post has convinced you that the current state of AI is perhaps not as powerful as a bystander would be led to believe. Yes, great work with great outcomes is being undertaken, but often when you pull back the curtain, you can find AI systems that are still immature and struggle to adapt. The challenges discussed in this post are only the tip of the iceberg for what AI must overcome in the following decades before near human-level intelligence could be reached and any threat could begin to manifest itself. However, this long road to success has proved not to be an obstacle for certain parties interested in AI applications. Robot Wars Judge and Professor of AI and Robotics Noel Sharkley has been talking since the mid-noughties about the steady adoption of AI system in military contexts. Systems similar to those mentioned above that are so easily fooled or can behave so erratically are being implemented right now and yet relatively little uproar is being made in response. The first step in making a difference is learning the lay of the land. To do this, I would recommend reading the other post in this series in which my colleague Orlanda explores the current use of AI in military settings and the risks this presents. I hope that these two posts combined will aid your considerations regarding the importance of both current and future risks of AI.","link":"/ai-not-as-smart/"},{"title":"Beauty From Chaos","text":"The Nightmare That Is a New BlogStarting a blog should not be a difficult feat; and yet that is just what it has proven to be for me. There are far too many questions that I still don’t have the answer to: What should I blog about? Who should I intend for the blog to be read by? How should I produce and publish it? When should I post my first piece of work? On top of this I found myself on a never-ceasing chase for the perfect first post. Every time I felt like I was getting close to completion on a project I became faced with one of many dilemmas: There’s some bug in my work that I don’t know how to fix I don’t know enough to properly solve the problem I’m working on The project seems too simple to share with the world I have let these issues envelope me and this has resulted in an ‘R’ directory clogged with the corpses of uncompleted works. Almost all forgotten in the chase for a new exciting idea doomed for the same fate. This has gone on for some time. But now I have decided that it’s time to put my foot down and take the plinge. Release anything. More exciting, difficult, and long-form work can always follow but the first step needed to be taken else that point would remain unreachable. I finally have the answers to the four questions that have been haunting me for the past few months: whatever, whoever, however, now. The rest will follow soon as I learn more about what I want to achieve and how I can go about doing so. But for now, I hope you enjoy a small interactive visualisation I put together. One small step for man, one small step for mankindI have always been interested in the patterns and order you can derive from random noise. This is what this first project was about. I decided to generate a matrix of random integers in some range, colour them in, and then connect matching adjacent points with a line of the same colour. This gave rise to some interesting designs, two examples of which are shown below. This was simple enough to do so I decided to make the visualisation interactive using the Shiny package. The result of this can be found here. Why not have a play with it and see what interesting designs it will generate for?","link":"/beauty-from-chaos/"},{"title":"Binomial Recursion","text":"An Infinite Coin ProblemImagine you have a coin, biased in such a way that it lands heads with probability $p \\in [0,1)$ (we will require $p \\neq 1$ later in the post) and tails with probability $1-p$. You start by flipping this coin $n \\in \\mathbb{N}^+$ times. Then, count how many heads you obtained in this series of flips and call this $x_1$. Begin flipping again, but this time doing a total of $x_1$ tosses. You count the heads and call the resulting value $x_2$. You can probably see where this is going. In general, on the $i$th set of coin flips, you perform $x_{i-1}$ tosses and record the numberof heads as $x_i$. You repeat this until eventually you get no heads in a series of flips. If this occurred on the $i$th turn, we’d then have $0 = x_k = x_{x+1} = \\ldots$. Formalising this, if we let $X_i$ be the random variable representing the number of heads obtained on the $i$th series of tosses, then these random variables are defined by $$ X_1 \\sim B(n,p) \\qquad X_{i+1} \\sim B(X_{i}, p) \\ \\forall i \\in \\mathbb{N} $$ There are many questions we could ask about these random variables, which in turn relate to questions about the original scenario. A sensible question might be regarding the distribution of each $X_i$, which will correspond to asking what the likelihood is of getting a particular number of heads on a certain series of tosses. We may also wish to know how many heads we expect to see in total for a given $n$ and $p$. This is equivalent to asking what the value of $\\mathbb{E}\\left( \\sum_{k=1}^\\infty X_k \\right)$ is. Handling a specific caseTo keep the focus on the important parts of this problem, I have omitted defining any pmfs outside of their support. Therefore, if any value of the pmf is undefined, it can be assumed to be zero. We will start simple by finding the distribution of the random variable $X_2$ and from there we can go about generalising this for any $X_i$. To begin, we note that since $X_1$ is binomially distributed with parameters $n$ and $p$, it will have a probability mass function given by $$ \\mathbb{P}(X_1=k) = {n \\choose k} p^k (1-p)^{n-k} \\qquad k \\in [n] $$ We also know, given that $X_1$ takes a particular value $k \\in [n]$, that $X_2$ will be distributed binomially with parameters $k$ and $p$ and so it will have the conditional probability mass function $$ \\mathbb{P}(X_2=l|X_1=k) = {k \\choose l} p^l(1-p)^{k-l} \\qquad l \\in [k] $$ Combining these and using the definition of conditional probability we can construct the joint pdf $$\\begin{align}\\mathbb{P}(X_1 = k, X_2=l) &amp; = \\mathbb{P}(X_1=k)\\cdot\\mathbb{P}(X_2=l|X_1=k) \\\\&amp; = {n \\choose k}p^k(1-p)^{n-k} \\cdot {k \\choose l} p^l(1-p)^{k-l} \\\\&amp; = {n \\choose k} {k \\choose l} p^{k+l} (1-p)^{n-l} \\qquad k,l \\in [n],\\ l\\leq k\\end{align}$$ This is a great position to be in as this function describes the entire combined behaviour of $X_1$ and $X_2$. To get the distribution of $X_2$, all we have to do is fix $X_2$ at a certain value and sum across all corresponding possible values for $X_1$ in the support of the joint random variable. If we fix $X_2$ to be $l \\in [n]$ then we must have $l&lt;X_1$, and so we are required to sum across the values $l$ to $n$ as follows $$\\begin{align}\\mathbb{P}(X_2 = l) &amp; = \\sum_{k=l}^n \\mathbb{P}(X_1 = k, X_2=l) \\\\&amp; = \\sum_{k=l}^n {n \\choose k} {k \\choose l} p^{k+l} (1-p)^{n-l} \\qquad l \\in [n]\\end{align}$$ We can now go about simplifying this sum. First we extract any parts of the summand that do not depend on the index, $k$ $$\\begin{align}\\mathbb{P}(X_2 = l) &amp; = p^l (1-p)^{n-l} \\sum_{k=l}^n {n \\choose k} {k \\choose l} p^{k} \\qquad l \\in [n]\\end{align}$$ Next we use the change of variable: $m=k-l$. This gives us a sum starting from zero. We follow this with some algebraic manipulation $$\\begin{align}\\mathbb{P}(X_2 = l) &amp; = p^l (1-p)^{n-l} \\sum_{m=0}^{n-l} {n \\choose m+l} {m+l \\choose l} p^{m+l} \\\\ &amp; = (p^2)^l (1-p)^{n-l} \\sum_{m=0}^{n-l} {n \\choose m+l} {m+l \\choose l} p^{m} \\\\ &amp; = (p^2)^l (1-p)^{n-l} \\sum_{m=0}^{n-l} \\frac{n!}{(m+l)!(n-m-l)!} \\frac{(m+l)!}{m!l!} p^{m} \\\\&amp; = (p^2)^l (1-p)^{n-l} \\sum_{m=0}^{n-l} \\frac{n!}{(m+l)!(n-m-l)!} \\frac{(m+l)!}{m!l!} p^{m} \\\\&amp; = (p^2)^l (1-p)^{n-l} \\sum_{m=0}^{n-l} \\frac{n!}{(n-m-l))!m!l!} p^{m} \\\\&amp; = \\frac{n!}{l!(n-l)!} (p^2)^l (1-p)^{n-l} \\sum_{m=0}^{n-l} \\frac{(n-l)!}{m!(n-m-l)!} p^{m} \\\\&amp; = {n \\choose l} (p^2)^l (1-p)^{n-l} \\sum_{m=0}^{n-l} {n-l\\choose m} p^{m} \\cdot 1^{(n-l)-m} \\qquad l \\in [n]\\end{align}$$ Lastly, observe that the summand is actually just the binomial expansion of $(1+p)^{n-l}$ and so we get $$\\begin{align}\\mathbb{P}(X_2 = l) &amp; = {n \\choose l} (p^2)^l (1-p)^{n-l} (1+p)^{n-l} \\\\\\mathbb{P}(X_2 = l) &amp; = {n \\choose l} (p^2)^l (1-p^2)^{n-l} \\qquad l \\in [n]\\end{align}$$ by the difference of two squares. The eager-eyed reader may notice that this is in fact the probability mass function of a binomial random variable with parameters $n$ and $p^2$ and hence $X_2 \\sim B(n,p^2)$. This leads us to conjecture that $$X_i \\sim B(n,p^i) \\quad \\forall i \\in \\mathbb{N}$$ This turns out to be completely true. What follows is a formal proof of this using induction and then a combinatorial argument which aims to reason why this is true in a less formal way. A Formal Proof of the General CaseWe will proceed inductively on $i$. We see that the statement holds for $i=1,2$ so we have our base case covered comfortably. For our inductive hypothesis we suppose that $X_{i-1} \\sim B(n,p^i)$ and proceed from here to try and show that $X_{i} \\sim B(n,p^{i+1})$. We do this using the same method as for $X_2$; first we calculate the joint probability mass function of $X_i$ and $X_{i}$ and then sum over all valid values of $X_{i-1}$ for fixed $X_{i} = l$ to get the marginal distribution of $X_{i}$ which will hopefully correspond to a binomial random variable with parameters $n$ and $p^{i}$. First note that for all $k,l \\in [n],\\ l\\leq k$ we have $$\\begin{align}\\mathbb{P} \\left(X_{i-1} = k, X_i = l \\right) &amp; =\\mathbb{P} \\left( X_{i-1} = k \\right) \\cdot \\mathbb{P} \\left( X_i = l| X_{i-1} = k \\right) \\\\&amp; = {k \\choose l} p^l (1-p)^{k-l} \\cdot {n \\choose k} \\left( p^{i-1} \\right)^k \\left( 1 - p^{i-1} \\right)^{n - k}\\end{align}$$ Which implies that $$\\begin{align}\\mathbb{P} \\left( X_n = i \\right) &amp; = \\sum_{k=l}^n {k \\choose l} {n \\choose k} p^l (1-p)^{k-l} \\left( p^{i-1} \\right)^k \\left( 1 - p^{i-1} \\right)^{n - k} \\\\ &amp; = p^l \\sum_{k=l}^n {k \\choose l} {n \\choose k} (1-p)^{k-l} \\left( p^{i-1} \\right)^k \\left( 1 - p^{i-1} \\right)^{n - k} \\\\\\end{align}$$ We then use the substitution $m=k-l$ to get $$\\begin{align}\\mathbb{P} \\left( X_n = i \\right) &amp; = p^l \\sum_{m=0}^{n-l} {m + l \\choose l} {n \\choose m + l} \\left(1-p \\right)^{m} \\left( p^{i-1} \\right)^{m+l} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\ &amp; = \\left( p^i \\right)^l \\sum_{m=0}^{n-l} {m + l \\choose l} {n \\choose m + l} \\left(1-p \\right)^{m} \\left( p^{i-1} \\right)^{m} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\&amp; = \\left( p^i \\right)^l \\sum_{m=0}^{n-l} \\frac{(m+l)! n!}{l! m! (m+l)! ((n-l)-m)!} \\left(1-p \\right)^{m} \\left( p^{i-1} \\right)^{m} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\&amp; = \\left( p^i \\right)^l \\sum_{m=0}^{n-l} \\frac{n!}{l! m! ((n-l)-m)!} \\left(1-p \\right)^{m} \\left( p^{i-1} \\right)^{m} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\&amp; = \\left( p^i \\right)^l \\frac{n!}{l! (n-l)!} \\sum_{m=0}^{n-l} \\frac{(n-l)!}{m! ((n-l)-m)!} \\left(1-p \\right)^{m} \\left( p^{i-1} \\right)^{m} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\&amp; = \\left( p^i \\right)^l {n \\choose l} \\sum_{m=0}^{n-l} {n-l \\choose m} \\left(1-p \\right)^{m} \\left( p^{i-1} \\right)^{m} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\&amp; = \\left( p^i \\right)^l {n \\choose l} \\sum_{m=0}^{n-l} {n-l \\choose m} \\left(p^{i-1}-p^i \\right)^{m} \\left( 1 - p^{i-1} \\right)^{(n - l) - m} \\\\&amp; = \\left( p^i \\right)^l {n \\choose l} \\left( p^{i-1} - p^i + 1 - p^{i-1} \\right)^{n-l} \\\\&amp; = {n \\choose l} \\left( p^i \\right)^l \\left(1 - p^i \\right)^{n-l} \\\\\\end{align}$$ Since this last line is the probability mass function for a binomially distributed random variable with parameters $n$ and $p^i$, our induction is complete and so we can rest assured that our conjecture does hold for all $i \\in \\mathbb{N}$. The jump from the third-to-last to penultimate step is probably the toughest and involves spotting that the summand is the binomial expansion of $((p^{i-1}-p^i)+(1-p^{i-1}))^{n-l}$. This may be somewhat difficult to notice in the forward direction but checking it in the reverse is a trivial matter of substituting the two expressions forming the binomial into the expansion formula. In fact, the whole proof may feel like it has an air of aimless wandering with the right results just popping out with little explanation to the logic behind the proof’s direction. I would not argue at all with the analysis! The best way to understand the structure of this proof is to compare it to the specific case of finding the distribution of $X_2$. The proofs are very similar but it is much clearer to follow the thought process behind the simpler case. A Combinatorial JustificationFor those readers who prefer an intuitive explanation of why a result is true more than a formal proof, I also offer a more combinatorial justification of proposition. To keep things simple, I will only focus on the first two random variables $X_1$ and $X_2$, though this result can easily be generalised inductively. The key insight to this explanation is noticing that the order of events involved in the problem can be changed without actually affecting the distributions of $X_1$ and $X_2$. A binomial random variable with parameters $n$ and $p$ is simply the sum of $n$ independent and identically distributed Bernoulli random variables with parameter $p$. We can treat Bernoulli random variables as drawing a ticket from a box in which all tickets either have the number 0 or 1 written on them, the proportion of which having 1 being $p$. In the original statement of the problem if we have $X_1 = x_1$ take on a specific value, this amounts to use drawing $n$ tickets from such a box mentioned above and counting that $x_1$ have a 1 written on them. Then, if we have $X_2 = x_2$ take on a specific value too, this equates to us performing a secondary follow-up experiment in which we draw $x_1$ tickets from the box again and seeing that $x_2$ of them have a 1 written on them. We now frame this problem in a different way. We draw the $n$ tickets for $X_1$ as before but now, rather than waiting till the end to see how many 1’s we have, as soon as we draw a 1, we follow this up by drawing another ticket but for $X_2$ instead. We can think of this procedure as actually just drawing one ticket from the box but with three possibilities for what can be written on it: $X_1=0$. This value models drawing a zero for $X_1$ and hence not drawing for $X_2$. $X_1=1, X_2 = 0$. This value models drawing a one for $X_1$ but then getting a zero for $X_2$. $X_1=1, X_2 = 1$. This value models drawing a one for both $X_1$ and $X_2$. We observe that these events occur with probability $(1-p)$, $p(1-p)$ and $p^2$ respectively. Since to have $X_2=x_2$ all we care about is having $x_2$ tickets out of the $n$ total be of the last type which occurs with probability $p^2$ each time, independent of any other ticket combination, we must have $X_2 \\sim B(n, p^2)$ as we expected. Finding The ExpectationNow that we know the distributions of each $X_i$, computing the expectation of their sum isn’t too tricky. We do have to be careful of convergence issues but since we restricted $p$ to be less than 1, we don’t run into any issues. If $p$ is 1, it’s clear that the coin-tossing goes on forever and so obviously don’t have an expectation for the total number of heads. We will calculate this value my first using the linearity of expectation, then recalling that the expected value of a $B(n,p)$ random variable is $np$ and then using the formula for the closed for of an infinite geometric series, which will converge since $p&lt;1$. $$\\begin{align}\\mathbb{E}\\left( \\sum_{k=1}^\\infty X_k \\right) &amp; = \\sum_{k=1}^\\infty \\mathbb{E}(X_k) \\\\&amp; = \\sum_{k=1}^\\infty np^k \\\\&amp; = np \\sum_{k=1}^\\infty p^{k-1} \\\\&amp; = np \\sum_{j=0}^\\infty p^{j} \\quad [j=k-l]\\\\&amp; = \\frac{np}{1-p} \\qquad p \\in [0,1)\\end{align}$$ Interestingly, this is exactly the number of tosses you perform on the first set of flips multiplied by the odds of getting a head. Final CommentsIt is worth mentioning that, in fact, this proposition can be generalised further. We do this by letting $X_1 \\sim B(n, p_1)$ and defining $X_i \\sim B(X_{i-1}, p_i) \\ \\forall i \\in \\mathbb{N}$. Then it is true, similar to above that$$X_i \\sim B \\left( n, \\prod_{k=1}^{i} p_k \\right) \\ \\forall i \\in \\mathbb{N} $$The proof is almost identical to the one given above just with some more notational complexity. The combinatorial justification can also easily be adapted to accommodate this change. I leave it to the eager reader to confirm the validity of this statement for themselves.","link":"/binomial-recursion/"},{"title":"Maths Matters: Bouncy Numbers","text":"This blog post will contain a solution to problem #113 (non-bouncy numbers) of ProjectEuler+ which will in turn provide a substantial hint for problem #112 (bouncy numbers). Almost a year ago now I wrote a blog post titled Efficiently Solving a Google Coding Interview Question Using Pure Mathematics. In it, I discuss how I had used techniques from pure mathematical fields such as combinatorics and linear algebra, to find a solution to a typical Google interview problem that was simple, efficient, and impressively short. Today’s post could easily be described as the spiritual successor to the former. We will tackle a new problem, this time from ProjectEuler+ (HackerRank‘s take on the original ProjectEuler). The exact techniques we use will certainly differ but the overall style and structure will be the same. More importantly, so will the take-away message: maths matters! Bouncy NumbersThe problem that we will be looking at involves a definition of what makes a number bouncy, or more importantly not bouncy. We will be concerned with counting how many non-bouncy numbers there are less than a given power of ten. The best place to start is by defining what exactly a bouncy number is. The Problem StatementThe specific problem that we will be looking at is #113 - non-bouncy numbers. The original problem statement can be found here but I will summarise below. We say that a number is increasing if no digit is exceeded by the digit to its left. For example, 123 and 445 are both increasing numbers but 687 isn’t. We have a similar definition for decreasing numbers in which no digit is exceed by the digit to its right. We call any positive integer that is neither increasing nor decreasing a bouncy number. The crux of the question is simply-stated: how many positive integers below $10^k$ are not bouncy? These answers grow large with $k$ and so the original problem adds that the answer should be printed modulo $(10^9 + 7)$ but we will ignore this for simplicity. A Typical SolutionThis problem stood out to me as it seemed to have a surprisingly low success rate of around 50% for what to me seemed like a straight-forward problem. The reason for this disparity is that my initial approach to the problem took a vastly different line of attack to most, judging by the problem discussion board. It appears that most competitors tried to tackle the problem using either brute force (good luck) or dynamic programming. The issue is, that due to the large number of test cases and range of $k$ (up to $10^5$) any brute force approach, and all but the most efficient dynamic programming solutions, will ending up using more than the allowable code execution time. My solution differs in that there is no recursion, no iteration, and no enumeration; in fact there’s not really any code at all. Why? Because my experience with combinatorics led me to know a closed form for the solution. A Mathematical ApproachBefore we get into the nitty-gritty mathematics, lets take a look at the code for my solution (written in Python). The input format for the problem is first an integer $t$ giving the number of test cases, followed by $t$ lines each containing a different value of $k$. The output format is then $t$ lines, each containing the number of non-bouncy numbers less than $10^k$. The Final Code, First123456789101112131415from functools import reduce # define combinatorial choose functiondef nCr(n,k): numer = reduce(lambda x, y: x * y, list(range(n, n - k, -1))) denom = reduce(lambda x, y: x * y, list(range(1, k + 1))) return numer // denomfor _ in range(int(input())): k = int(input()) num_increasing = nCr(k + 9, 9) - 1 num_decreasing = nCr(k + 10, 10) - (k + 1) num_both = 9 * k num_not_bouncy = num_inc + num_dec - num_bth print(num_not_bouncy) This code passes all visible and hidden test cases for the problem in barely any time. The first section of the code—importing functools.reduce and defining nCr(n,k)—isn’t actually part of the general problem-solving logic. They simply make up for the lack of base mathematical functionality in Python by manually adding definitions. In all, only the four lines starting num_{something} = are essential for solving the problem, the rest is just boilerplate code. These four lines carry a lot of power. Before we piece them together though, we will have to have a quick review of elementary combinatorics. My Thought ProcessElementary CombinatoricsIf you feel like you have a good grasp of basic combinatorics including the definition of $n \\choose k$ and knowledge of the principle of bijections, this section will not teach you anything new so I advise that you skip over it. Combinatorics is a vast field of mathematics but we will only need to take a short glance at a specific area. In particular, we will be looking at the function denoted by $$n\\choose k$$ which we pronounce as n choose k. This function gives the number of ways of selecting $k$ objects out of $n$ distinguishable choices. For example if we to form a team of 5 out of a class of 30 pupils, we would be able to do this in ${30 \\choose 5}$ unique ways. That’s nice and all, but just defining a notation for a concept doesn’t get us very far. The important point is that we have a formula for $n\\choose k$ which lets us calculate it simply and efficiently for any reasonable $n$ and $k$. The formula is as follows: $${n\\choose k} = \\frac{n!}{k!(n-k)!} = \\frac{n \\cdot (n-1) \\cdot \\ldots \\cdot (n - k + 2) \\cdot (n - k + 1)}{k \\cdot (k-1) \\cdot \\ldots \\cdot 2 \\cdot 1}$$ This is exactly what that first block of code in our solution is for. It sets up the function to compute $n \\choose k$ for any positive integers $0 \\leq k \\leq n$. This doesn’t contribute to the overall logic of the solution but is necessary component to the required computations. Before we get on to exactly into how my solution works, we will need to take a quick detour to learn about the principle of bijections. This is best illustrated with a motivating example. Suppose that you have a handful of apples and oranges and you want to know whether you have the same quantity of each fruit. The obvious way to check this is to count how many of each fruit you have and then compare these two numbers to check for equality. Let’s consider this problem again but now with two large crates, each full of apples and oranges respectively. Counting is now a risky endeavour; how can we be sure that we don’t lose count or accidentally miss a piece of fruit? A more robust solution would be to pair the apples and oranges up until we either run out of both fruits at the same time - in which case the original quantities of each fruit were the same - or there is one type of fruit left over - and so the original numbers were not equal. This notation is abstracted in the field of combinatorics (and maths at large) to what is known as the principle of bijections. This states that whenever we can form a one-to-one correspondence (also known as a bijection, hence the name) between two sets, they are of the same size. This principle will come in handy in just a few moments. Application to the ProblemWith preliminaries out of the way, we can proceed to solve to the problem. We split our approach into four parts: 1) Calculate the number of increasing numbers less than $10^k$ 2) Calculate the number of decreasing numbers less than $10^k$ 3) Calculate the number of numbers less than $10^k$ that are both increasing and decreasing 4) Compute the number of bouncy numbers less than $10^k$ using the above three values It turns out that each of these can each be completed in just a single line of code involving the nCr(n, k) function we defined earlier. We will break down each step one at a time. Step 1We start by counting the number of increasing numbers less than $10^k$. As a reminder, we include both strictly increasing (e.g. $123$) and weakly increasing (e.g. $455$) in our general definition of increasing numbers. Counting these directly is rather difficult but we can introduce a clever one-to-one correspondence to make this task much easier. We will define the mapping as follows: Given an increasing positive integer less than $10^k$, write it in the form $d_1d_2\\ldots d_k$ where $d_1 \\ldots d_k$ are digits between 0 and 9 to be concatenated Since the given number was increasing we have $0 \\leq d_1 \\leq d_2 \\leq \\ldots \\leq d_k \\leq 9$ We map this integer to a sequence of stars (*) and bars (|) such that each star corresponds to the digits $d_1\\ldots d_k$ Further we set the number of bars between the $i$th and $(i+1)$th star to be equal $d_{i+1} - d_{i}$ Lastly we place a number of bars equal to $d_1$ at the start of the sequence and $9 - d_k$ at the end They may seem like a highly opaque definition but hopefully the inclusion of an example will add clarity. First we look at the number $457$ when $k=5$. Since we are considering five digits, we write this as 00457. We start with our 5 stars: $$*****$$ Our first digit is $0$ so we don’t place any bars at the start. Likewise our second digit is $0$ and so as there was no increase, we do not insert a bar. Between the second and third digit however we have an increase of $4$ so we insert that many bars there to get: $$**||||***$$ Carrying on, we have a jump of $1$ between the third and fourth digit and $2$ between the fourth and last so we add the corresponding number of bars to the diagram: $$**||||*|*||*$$ Finally, we add two bars to the end of the sequence since $7$ is two short of $9$. This leaves us with: $$**||||*|*||*||$$ Notice that we can convert these sequence representations back by reversing our rules. For example the sequence, $$||**|**|*|||||$$ corresponds to the increasing number $22334$. Notice that in general we will have $k$ stars and 9 bars. The reason for the latter is perhaps less apparent. It follows from the way we defined the number of bars between each of the digits and at the end. The number at the start was $d_1$. Then we had $d_2-d_1$ between the first pair, and $d_3-d_2$ between the next. We continue this until there are $d_k - d_{k-1}$ between the last pair. Lastly, we add $9-d_k$ bars to the end. So in total we have $$d_1 + (d_2 - d_1) + (d_3 - d_2) + \\ldots (d_k - d_{k-1}) + (9-d_k) = 9$$ bars. Since mappings are unique in both the forwards and backwards direction (try working backwards from the stars and bars above to the original number), we have a one-to-one correspondence. It follows from the principle of bijections that the number of increasing numbers less than $10^k$ is exactly equal to the number of ways of creating a sequence of $k$ stars and $9$ bars. The end is now in sight. The last step is to notice that the number of ways of arranging $k$ stars and $9$ bars is the same as the number of ways of picking $9$ objects from a choice of $k+9$. Why is this? Well suppose that we have $k+9$ spaces which can each be filled with either a star or a bar: $$\\underbrace{\\_\\,\\_\\,\\_\\ldots\\_\\,\\_\\,\\_}_{k+9}$$ We then pick $9$ of these to fill with the 9 bars we have available. Then the remaining $k$ spaces are forced to be stars. Since the choice of spaces to fill with bars uniquely determines the sequence of stars and bars, and vice-versa, these represent the same process. Finally, remember from our definition of $n \\choose k$ is the number of ways of choosing k elements from a selection of n objects. Well that is exactly what we have done here, but choosing $9$ objects from $k+9$ instead. There are therefore $${k+9} \\choose 9$$ ways to create such sequences of stars and bars, and so using the principle of bijections, this is exactly how many increasing numbers less than $10^k$ we have. A small tweak remaining is just to subtract $1$ from the result since the problem specifically concerns positive integers. With this addition made we are left with the first ‘proper’ line of code in my solution: 1num_increasing = nCr(k + 9, 9) - 1 Step 2This next step is very similar to the last but with a minor change. This is since, with decreasing numbers, we are allowed to have zeros both at the start and end of the number (the leading zeros being used to pad the number to have $k$ digits). We use the same numbers of stars as before but add an extra bar. We also decide on the number of bars between each star in a slightly altered way. This is harder to describe algebraically so instead we use the rule that for each additional bar, our next star progresses one place in the following sequence $${0,9,8,\\ldots,1,0}$$ This is highly analogous to the previous method so I won’t go into additional examples though I encourage the reader to try one out for themselves. The important point is that we are now arranging $k$ stars and $10$ bars. In a similar way to before we find that this can be done in $${k+10} \\choose 10$$ ways. We must make a similar correction to before. This time, however, we subtract $k+1$ rather $1$. This is because we have $k+1$ unique representations of zero, show below. $$\\underbrace{***\\ldots***}_{k}||||||||||$$ $$\\underbrace{***\\ldots***}_{k-1}||||||||||*$$ $$\\vdots$$ $$*||||||||||\\underbrace{***\\ldots***}_{k-1}$$ $$||||||||||\\underbrace{***\\ldots***}_{k}$$ This leaves us with the second essential line of code: 1num_increasing = nCr(k + 10, 10) - (k + 1) Step 3Thankfully this step is much simpler than the last two. We introduce this to prevent double-counting numbers that are both increasing and decreasing. The only numbers satisfying both these properties are those made of a single digit. It is not hard to show that there are $9 \\cdot k$ ways to do this. Indeed, there are $9$ digits that can choose from and then we can repeat it one of $1, 2, \\ldots, k$ times (i.e. $k$ different ways). This leads us to the third line of the main body of the code: 1num_both = 9 * k Step 4To round everything up will simply note that the number of non-bouncy integers less than $10^k$, which by definition is the number of integers less than $10^k$ that are neither increasing nor decreasing, is equal the number of integers less than $10^k$ that are increasing plus the number that are decreasing subtract the number that are both increasing and decreasing. This gives rise to the final important line of code: 1num_not_bouncy = num_inc + num_dec - num_bth With this, we can draw a close to this post. Before we do though, I would like to emphasise the importance of pure mathematics in this solution. Without a reasonable knowledge of combinatorics, coming up with this solution would have been highly unlikely and so one may have been forced to resort to more complicated or inefficient methods. Some would argue that this approach seems far more confusing than a dynamic programming approach and I understand that train of thought. At first glance, this solution seems incredibly involved and time-consuming to produce. I argue though, that with practice of pure mathematics—and, in particular, combinatorics—this sort of problem-solving becomes second nature, allowing you to tackle challenges of this sort after only a few seconds of contemplation.","link":"/bouncy-numbers/"},{"title":"Bank Holiday Bodge: Coding up Christmas","text":"For the previous bank holiday bodge, I took a look at using the Spotify top songs datasets from Kaggle and, with the help of UMAP and RasterFairy, built a wall of cover art in which similar songs were positioned close to one another. The format of the post involved me walking through my every step, mistakes and all. The result of this was a ridiculous 5000+ word read. I do think there is a utility to such posts that go into complete detail, but I think the idea works better as a standalone concept rather than combining it with ‘the bodge’. Therefore, this post will be short and sweet; first showing off the finished product and then taking a few paragraphs to talk about some of the challenging elements of the project. It’s Beginning to Look a Lot Like ChristmasIn my extended family, Christmas Day is one of those occasions that takes some time to get going. That’s understandable; people have worked hard for the entire year and so a lie-in and lazy morning is well-deserved. Habits die hard though, and after several months of waking up at 6am to beat traffic (a post analysing my commute will be out in early March) Christmas Day wasn’t going to be any different. This gave me a window of a few hours before the festivities began to bash out a Christmas-themed bodge. The result of this whirlwind coding session was the following animation. Your browser does not support the video tag. The resolution of this version of the animation is kept low to improve page loading times. By using the source code linked a few paragraphs down, you are free to produce a high-quality version at the expense of file size. This animation is based on a classic reddit thread titled t * sin (t) ≈ Christmas tree. In this, many users discuss the use of parametric equations to generate Christmas trees. The coding in this thread is almost entirely in Mathematica and JavaScript. I therefore decided to port this idea to Python using the matplotlib.animation module and jazz up the tree with some ornaments in the process. The source code for this animation can be found here. There are many tweakable parameters to change the look of the tree so definitely have a play around if you have time. For those interested in the use of matplotlib.animation for creating such 3D parametric animations the you may want to read further. The documentation for the module is rather poor, resulting in me performing a lot of experimentation. Therefore in the next section I will discuss some of the hacks and tricks I used to get such a plot working so that I can save others time when they tackle similar projects. For any one else, Merry Christmas and a Happy New Year! The (Nervous) BreakdownAlthough I still managed to complete this project in one morning, it was a lot more fuss than I thought it would be when I first got going. The matplotlib.animation module has notable short-comings which I needed to work around to produce this visualisation, the frustration from which almost bringing me to breakdown in the process. Thankfully, I made it through with a working solution. Here are two key tricks I learnt in the process. Collaboration Opportunity The trouble that I faced with producing this animation has led me to consider producing a small Python package for quickly and easily generating 2D and 3D animations using parametric equations. I have a lot of projects in the pipeline at the moment but I might eventually prioritise this. If there is anyone who would be interested in co-developing this, please do get in touch. Private MethodsPrivate methods aren’t really a thing in Python. I have mixed feelings about this decision but can concede that when classes are designed in a sensible way, this rarely causes any issues and allows for great flexibility in extending functionality. matplotlib.animation is not designed in a sensible way. As much as I appreciate the developers’ hard work, the module structure has a lot to be desired. Whereas private methods/attributes are often used in Python to avoid exposing obfuscating elements of a class, in matplotlib.animation they instead seem to be used to avoid having to document key features. For example, the setting of positions and point colours for 3D scatter plots seems like quite a vital feature of an animation module. Despite this, the attributes used to control this behaviour are private and there is no public-facing method to interact with them. This means that the package has absolutely zero documentation on how to change 3D points’ sizes and colours. In the end, I was forced to work my way through the source code of the mpl_toolkits.mplot3d.art3d.Path3DCollection class to find out how to manually implement the features that I needed. This led me to discovering the private methods _offsets3d, _facecolor3d, _edgecolor3d which can be manually overwritten in order to update the animation. This can be combined with the set_sizes() method which is inherited from a generic plot to change point size in each frame. Note however that set_color() is not inherited—I wish I knew why but frankly I’m at a loss. ZorderingZordering (or Z-ordering as literally everyone but the matplotlib team call it) is the process of ordering the rendering of objects in a plot so that those that are meant to appear closer to the ‘camera’ are ‘on top’ of others. In 3D plotting, this technique is essential as an absence of it will break any notion of 3D perspective. Matplotlib generally has the ability to implement this feature. Does this apply to 3D plots though—of course not, that would make things too easy. As mentioned in this StackOverflow discussion, Axes3D ignores the zorder parameter used in general plots and instead renders objects in the order it thinks they should appear in. Annoyingly, its thinking is almost always wrong. After a considerable amount of time bashing my head against the metaphorical wall that is matplotlib, I came up with a solution. It appears that matplotlib often just renders objects in the order that they appear in the inputted coordinate array. Therefore if we were to sort this array manually to be inline with the zorders we required, matplotlib would finally give us the plot we sought. It would be possible to implement this for any camera location but to keep things simple I fixed the camera to have an azimuth of zero (i.e. looking down the positive x-axis) so that sorting the coordinate (and colour/point size) array(s) y the negative of their x-coordinates gave the correct zorder. Generalising this is not too hard, you simply need to find the normal vector to the plane generated by the ‘camera’ and sort along that.","link":"/coding-up-christmas/"},{"title":"Solving Complex Coverage Problems Using Microsoft Excel","text":"The Excel spreadsheet used in this post can be found on the GitHub repository for this blog Formulating the Coverage ProblemSuppose that you work at a telecommunications company and one day you are sent an Excel spreadsheet with the following data.As you can see, the data concerns the location of 6 towns and the number of customers the company has at each one. You are informed that the company wishes to build new communication towers in a selection of these towns so that every customer is covered by at least one tower (we assume that currently no town has any existing coverage). Your objective is to determine the cheapest way in which this can be achieved. The data goes on to detail that there are three sizes of tower available to be built, each with its own associated range, coverage (that is the number of customers it can sustain), and construction costs (we assume that running costs are negligible). There are also limitations on the number of some of the types of satellite you can build. A further constraint is that due to planning permission issues, you can only build one tower in any given town. Lastly it is added that if two towers cover the same customer then only one of them needs to use its coverage to serve that customer. This feels very close to the sort of problem a real life telecommunications company may be trying to solve albeit with reduced scale and complexity. The expanse of this example is not limited by the methods we will be using but rather the tool - Microsoft Excel. Using a more powerful method such as a programming language like Python or Matlab would facilitate the expansion of this problem further. Despite this, I believe that this example captures the essence of the problem with the added bonus that it can be formulated in a visual way in a spreadsheet. We will solve this problem using linear programming; in particular, the branch and bound algorithm. To do this we will need to build a model to describe the problem and then use the Solver add-in in Excel to find an optimal solution. Before we get to this, let’s take a look at this problem through a mathematical lens. A Mathematical DescriptionWe begin by formulating the problem mathematically. If you are only interested in the practical side of this problem then feel free to skip this section but I would argue that any good Excel model will have a well-thought-out mathematical model behind it. Decision variablesTo begin, we will need to define a collection of decision variables. For shorthand, we denote the towers sizes small, medium, and large by the integers 1, 2, and 3 respectively. $$c_{i,j} :=\\begin{cases} 1 &amp; \\textrm{if town } j \\text{ is covered by a tower in town } i \\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\qquadi \\in[6], j \\in [6]$$ $$n_{i,j} :=\\begin{matrix}\\textrm{the number of customers in town } \\\\j \\textrm{ covered by a tower in town }\\end{matrix}\\qquadi \\in[6], j \\in [6]$$ $$t_{i,k} :=\\begin{cases} 1 &amp; \\textrm{if a tower of size } k \\text{ is built in town } i \\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\qquadi \\in[6], k \\in [3]$$ The first two sets of variables may seem redundant when put together but we do in fact need both of these. The use of the second is clear but the first is more subtle. This is used when checking whether the tower range criterion has been met. We want town $i$ to be in the range of town $j$ whenever at least one customer in town $j$ is being covered by a tower in town $i$, otherwise we have no restriction. This condition can be described using an if statement on the expression $n_{i,j} \\geq 1$ but such conditional expressions are non-linear so instead we have to use a new decision variable to represent this and force it to behave in the desired way using additional constraints. This will be explained in more detail shortly. ConstantsWe will also need a way of describing the data we are given at the beginning of the post. We denote the latitude and longitude of town $i$ by $\\textrm{lat}_i$ and $\\textrm{long}_i$ although these are not very useful to us so we immediately replace them with the 6x6 matrix $D = (d_{i,j})$ with the $(i,j)$th entry representing the Euclidean distance between towns $i$ and $j$. That is $$d_{i,j} := \\sqrt{(\\textrm{lat}_i-\\textrm{lat}_j)^2 + (\\textrm{long}_i-\\textrm{long}_j)^2}\\qquadi \\in[6], j \\in [6]$$We also define a 6-element column vector $\\hat{\\nu} = (\\nu_i)$ where $\\nu_i$ is the number of customers in town $i$, as well as 3-element column vectors $\\hat{\\rho} = (\\rho_k)$, $\\hat{\\gamma} = (\\gamma_k)$, $\\hat{\\pi} = (\\pi_k)$, and $\\hat{\\mu} = (\\mu_k)$ where $\\rho_k$, $\\gamma_k$, $\\pi_k$, and $\\mu_k$ represent the range ($+\\infty$ if not specified), coverage, cost (price), and maximum number allowed for each tower respectively. Objective function and constraintsOur objective function is simple; we just wish to minimise costs. In mathematical language, this is $$\\textrm{min } w = \\sum_{k=1}^3 \\left(\\pi_k \\sum_{i=1}^6 t_{i,j}\\right)$$We can now begin to formulate our constraints. We start with the simplest; that is the limit on the number of towers in each town and towers of each type. These are formulated as $$\\sum_{i=1}^6 t_{i,k} \\leq \\mu_j \\qquad \\forall k \\in [3] \\tag{1}$$ and $$\\sum_{k=1}^3 t_{i,k} \\leq 1 \\qquad \\forall i \\in [6] \\tag{2}$$ We can now add the constraint that every customer must be covered by at least one tower. Since the customers are indistinguishable, we only require that the potential coverage for each town is larger than the size of the customer base there. That is $$\\sum_{i=1}^6 n_{i,j} \\geq \\nu_j \\qquad \\forall j \\in [6] \\tag{3}$$ Similarly, we also need that the total coverage provided by any town is at most the maximum coverage that its tower can supply. Since every town can have at most one tower we have that the maximum coverage is just $\\sum_{k=1}^3 \\gamma_k t_{i, k}$ (which is zero if no tower is built in that town, as expected) and so we can write $$\\sum_{j=1}^6 n_{i,j} \\leq \\sum_{k=1}^3 \\gamma_k t_{i, k} \\qquad \\forall i \\in [6] \\tag{4}$$ Now is an appropriate moment to add a constraint to link the behaviour of each $c_{i,j}$ and $n_{i,j}$. We require that $n_{i,j}$ is zero whenever $c_{i,j}$ is zero, and have no restriction when $c_{i,j} = 1$. Some might say that we should force $n_{i,j}$ to be strictly positive if $c_{i,j} = 1$ but this is redundant since the model will force this behaviour naturally - it would be sub-optimal to cover a new town, possibly worsening the range constraint and then not serve any customers there. We first define $M = \\sum_{i=1}^6 \\nu_i$ to be a large constant representing the total number of customers across all towns. Then, we use the following constraint to link the variables $$n_{i,j}\\leq Mc_{i,j} \\qquad \\forall i \\in [6], j \\in [6] \\tag{5}$$ Lastly, we need a constraint to control the ranges of the satellites. This is the most complicated to implement but using the variables $c_{i,j}$, it becomes reasonably simple $$d_{i,j}c_{i,j} \\leq \\sum_{k=1}^3 t_{i,k} \\rho_ \\qquad \\forall i \\in [6], j \\in [6] \\tag{6}$$ Note that since $j$ does not appear on the left-hand side, this can be rephrased as $$\\max\\limits_{j\\in[6]}( d_{i,j}c_{i,j}) \\leq \\sum_{k=1}^3 t_{i,k} \\rho_ \\qquad \\forall i \\in [6]$$Though this is no longer linear. This concludes the constraints for the problem and so the entire linear programming formulation too. In total we have 90 decision variables and 93 constraints, plus the implicit constraints that 54 decision variables and binary and the other 36 are non-negative. If we generalised this problem to have $n$ towns and $m$ towers, a similar calculation gives $n(m+2n)$ decision variables and $m+3n+2n^2$, with the implicit constraints that we have $n(m+n)$ binary variables and $n^2$ non-negative real variables. It is clear that this problem will become computationally infeasible very quickly as both variables are increased. We are now in a strong position to begin translating these mathematical relations into an Excel model. Modelling the problem with ExcelThe distance matrixFor clarity in Excel formulae, I began by assigning names to the two tables in the data sheet. I named the left table TownData and the right table TowerData. Then, the first step in building our Excel model is to convert the Cartesian locations of the towns into a distance matrix. I did this by adding a new sheet in the workbook named ‘Distance Matrix’ and creating a table to contain the distances. This is the final resultThe values in the table are generated by several VLOOKUP formulae. In particular, for cell D4 I used 12=SQRT((VLOOKUP(D$3, TownData, 2) - VLOOKUP($C4, TownData, 2))^2 + (VLOOKUP(D$3, TownData, 3) - VLOOKUP($C4, TownData, 3))^2) Breaking this down, the first VLOOKUP statement, VLOOKUP(D$2, TownData, 2), looks in the TownData table and finds the row matching matching the value in D3, the label of the column we are in, and then selects the second column (latitude) from that row of the table. The other 3 VLOOKUPs are similar. C4 is used to select the label of the row that we are in and we replace the third argument with 3 to select the third column of TownData, longitude. I then used the standard formula for the Euclidean distance between two points to give the final value. Note, that I anchor the row for D3 and the column for C4 using the dollar symbol so that when this formula is copied to the other cells in the table, the row and column labels are referenced correctly. I then applied some formatting to the table. Firstly, I changed the data type for the inner cells to have no decimal places displayed to improve clarity. I then applied a series of conditional formatting rules to the table to colour the cells green, yellow, orange, or red depending on whether the distance between any two towns could be covered by every tower, the two largest towers, just the largest tower, or none. This makes the assumption that the ranges of the towers are increasing with size which will almost certainly be true in reality and so if our input data is valid, we will not have any issues here. In this example we have no red cells but if we change the range of the largest tower to be 80 we get the following The full modelWe are now ready to begin building the full model. I decided to do this on another separate worksheet using the following layout. As mentioned at the start of the post, the final Excel spreadsheet can be found on the GitHub repository for this blog.This may seem daunting at first glance but when it has been broken down to its constituent parts, it is much more palitable. As can be seen in the key, the green cells represent decision variables. For those that read the mathematical formulation, the upper set correspond to $t_{i,k}$, the middle set to $n_{i,j}$, and the lower set $c_{i,j}$. For the time being these are filled with zeros, acting as place-holders. We also have a small grey table at the top and a lone grey cell at the bottom. Their purposes will be discussed later but all that needs to be known for now is that they are constructed by using VLOOKUP statements to copy values across from the data worksheet. Let us now look in more detail at the top-left table (cells B2:K9). The blue cells on the right are calculated by summing the rows of the inner table and so represent the total number of each tower type built. This is compared to the maximum number of each tower allowed in the red cells next to them. These values are taken directly from the data worksheet using a VLOOKUP statement, although observe that since our lookup value is a string we need to set the optional range_lookup parameter to be FALSE else we get erratic behaviour. These values are then passed into IF statements to handle the case when they are a blank string. In this case we are allowed an unlimited amount of that type of tower. We cannot represent infinity in Excel so instead we set it to the ceiling of total number of customers divided by the maximum coverage of that tower type since we would never need more towers than that for a reaistic problem. An alternative approach would be to set this to 6 since we only have that many towns, each of which with one possible tower but using the former method generally produces a tighter formulation of the problem and so improve runtime. For a specific example, the formula for cell K4 is 123=IF(ISBLANK(VLOOKUP($C4, TowerData, 5, FALSE)), CEILING(), VLOOKUP($C4, TowerData, 5, FALSE)) The purple row below the inner table simply uses SUMPRODUCT with the column above and the costs of each tower to calculate the build cost for each town. These are then summed up in the gold cell to give the total cost (our objective function). The blue row below this is simply a sum of the inner columns above and so represents the number of towers in each town. This is compared to the fixed value of 1 in each case. The setup for the table below this (B11:K20) is very similar We sum the rows and columns as before to get the blue cells. The row sums represent the number of customers that can be covered in each town and the column sums represent how many customer a tower in a given town is covering. On the right we then compare these to the size of the customer base in each town taken from the data worksheet using another VLOOKUP statement. On the bottom we compare these to the range of the chosen tower (or lack of) by computing the SUMPRODUCT of the corresponding column of the inner upper table and the coverage data for each tower. For example, the formula in D20 is =SUMPRODUCT(D$4:D$6, $M$3:$M$5). The model now gets slightly more convoluted due to the limitations of the two dimensions of a spreadsheet. Since Excel Solver constraints cannot handle additional constants, we must manually multiple our coverage decision variables ($c_{i,j}$) by our large constant ($M$) We do this using the array formula {=$D$40 * $D$24:$I$29} applied to the cells D33:I38. We will later use these new scaled variables to control the behaviour to control the behaviour of the middle table of decision variables ($n_{i,j}$). Lastly I add two tables on the right to apply the range contraints. The lower green table is simply the product of the distance matrix and the coverage decision variables ($c_{i,j}$) in the lower green table using an array formula. This represents the range required for any particular tower covering some town. These will later be compared to the red table above. Each column in the inner table contains the exact same formula which computes the SUMPRODUCT of the corrsponding column of the top-right inner table and the range data. For example, all of the cells O13:O18 contain =SUMPRODUCT(D$4:D$6, $O$3:$O$5). With some additional static formatting, I achieved the result shown above. In particular, I set the data type for each numeric cell to be integral. This is because our problem is complex enough that Excel Solver will only return an answer to a given tolerance rather than an exact solution. However, we are only interested in the exact solution and these will be close enough to this that it will be okay to round. The next step is to setup the Solver add-on and find an optimal solution to the problem. Finding an optimal solutionIf you wish to reproduce this example, it is import to make sure that you do in fact have the Excel Solver add-in enabled. If this is not the case, you can learn how to do this through this support document. I setup Solver as followsSome of the parameters are standard. Our objective cell is J7 and we wish to minimise this. Our decision variables are all of the green cells, that is D13:I18, D4:I6, and D24:I29. We also wish to make D4:I6 and D24:I29 binary variables and so two of our parameters control this. The other variables we need to be non-negative and so we select ‘Make Unconstrained Variables Non-Negative’. Our problem is linear so we use the Simplex engine. We can now take a look at the other constraints we added. The first is that D13:I18 should be element-wise less than or equal to D33:I38. This corresponds to constraint 5 in the mathematical formulation and is used to force the number of customers in town $j$ covered by a tower in town $i$ to be zero whenever the tower in town $i$ is not covering town $j$ and unrestricted otherwise. The next constraint says that D19:I19 must be at most D20:I20 for each element. This corresponds to the constraint that the number of customers covered by a tower in town $i$ can be at most the available coverage (or lack of) of the tower in that town. This relates to constraint 4 above. We then have the two binary constraints which we have already discussed. The next constraint limits the elements of D8:I8 by the corresponding elements of D9:I9 which take a constant value of 1. This refers to the constraint that only one tower can be built in each town. This is constraint 2 in the mathematical formulation. We then have a constraint that the values of J13:J18 must be element-wise greater than or equal to K13:K18. This relates to the requirement that every customer is covered by at least one satellite. This is constraint 3 from the last section. The penultemate constraint says that J4:J6 can be at most the value of K4:K6 for each element. This simply represents that you cannot build more towers of one type than the given maximum, the first constraint from above. The final constraint says that each element of O22:T27 must be less than or equal to O13:T18. This is used to ensure that no tower covers a town outside of its range. This is constaint 6 in the mathematical formulation. This concludes the setup for this problem. Clicking ‘Solve’ begins the branch-and-bound Simplex algorithm and after a few seconds (and around 800 sub-problems) we get the optimal solution shown below (within a negligable tolerance).Reading of the decision variable, we see that the (although there may be more) optimal solution is to build a small tower in town 2, a medium tower in town 1, and a large tower in town 4. We use the medium tower in town 1 to cover 571 customers in itself and, interestingly, all 29 customers in town 2. The small tower in town 2 is then used to solely cover town 6 with spare coverage for 20 customers. The large tower in town 4 is then used to covered all other customers exactly in towns 1, 3, 4, and 5. The cost of this coverage is £33,000. Due to the nature of this problem, there will obviously be many different equally-optimal solutions with different exact coverages for each town pair. This blog post is already very long but if I were to continue, I would next fix the optimal price and choose a second objective to function to optimise whilst maintaining the total cost to find a more practical optimal solution. For example, I may try to optimise the amount of overall spare coverage to future proof the network, or attempt to spread the spare coverage around more evenly by adding some padding to the coverage constraints. Another idea might be to add bias to make coverage of closer towns more favourable, although this would require some careful formulation. I encourage any reader to download the spreadsheet form the GitHub repository for the blog and try using this model with your own data. The model can be customised by adding or removing rows and columns to change the number of towers and towers. It is worth noting however that the scale and complexity of this problem is right on the edge of the default Excel Solver’s limitations. The add-in allows for up to 200 decision variables of which we are using only 90, however our real issue is the constraint limitation of 100, leaving us with only 7 spare. Even with the current setup, for unusual inputted data, the model can take several minutes to return and optimal solution, running through thousands of sub-problems in the process. In some pathologiacal cases I have discovered, this can be up to tens of minutes. These examples are often not very realistic as they involve towns that are no longer homogenously distributed but instead put deliberately far away form each other. Furthermore, without any limits on the number of each type of tower, the problem can take a long time to solve as the solution space is much larger. To overcome these problems there are two solutions. The first is to use a propriety Solver add-in for Excel such Frontline Solvers. This product is made for large coparations some comes with a a heafty price-tag in the thousands of dollars however it is much faster than the standard add-in and can support up to 2000 decision varibales and 8000 constraints for linear problems. My much more prefered solution however is to use a linear programming library for an open-source programming language such as scipy.optimize for Python or JuliaOpt for Julia. I would avoid using R, although some good linear programming packages do exist, due to its reduced numerical computation speed. If you have access to propriety mathematical languages such as MatLab and Mathematica, these also have very powerful optimisation libraries capable of solving linear programming problems like this in much faster times and with greater scale than Excel Solver. Despite this, for a simple, explainable, and visual model of modest size, Excel really can’t be beat.","link":"/coverage-problems/"},{"title":"Data Roaming: A Portable Linux Environment for Data Science","text":"This post will assume familiarity with Linux and access to an installation of any Linux distribution. IntroductionIf you are a student interested in data science, you might often find yourself working on library or university computers. Depending on the lenience of your university’s IT policy, you may run into one of two issues: You are unable to install any new software (such as languages, IDEs, or packages) You can install software but this has to be repeated every time your log in The first scenario makes data science work completely infeasible whereas the second is simply (though still significantly) a headache and waste of time. When trying to think of a solution to these problems, my mind first turned to the Ubuntu Live USB image. This is an installer image for Ubuntu that can be burnt to a CD or USB stick. Not only can this be used to install Ubuntu, but it also allows you to run Ubuntu on your hardware, directly from the USB stick (hence the “Live”). This is a step in the right direction. One could use this live USB stick alongside a Bash script used to install all necessary packages and software. Still, although this no longer requires user attention, it may take a while to download and install all requirements. Instead, it would be nice if we could perform this installation work upfront, creating a custom Ubuntu image with our requirements preinstalled. This is exactly the purpose of Cubic, a not-so-well-known tool for building custom Ubuntu images. It is supported by Ubuntu 18.04 and later and can be easily installed using the Aptitude package manager. 123sudo apt-add-repository ppa:cubic-wizard/releasesudo apt updatesudo apt install --no-install-recommends cubic Cubic is fairly straight forward to use and is incredibly flexible. Below I will detail how I used it to create a simple data science environment containing the following: An installation of R, Python, Julia An installation of RStudio, VSCode, Jupyter An installation of Git and Solaar Tools needed to update the image Creating the EnvironmentStep 1: PrerequisitesYou will first need to download an Ubuntu image (as an ISO file). I went for Lubuntu due to its small footprint. Alongside this, you will need a Bash script used to install additional functionality to the operating system. The script I used can be found here. You can download this and modify it to meet your needs, or start from scratch. Make sure the resulting file is called recipe.sh. A few points to note: The script will be ran as root, so there is no need to use sudo. You will likely need to enable the Ubuntu Universe/Multiverse using add-apt-repository universe/add-apt-repository multiverse. Make sure to install Cubic and UNetbootin (bottom of the script) if you want to update the environment directly from the live boot. With these two files, you can start creating your custom image. Step 2: Creating the ImageStart by opening Cubic and selecting a directory to store the project files (you’ll need to remember this for later). Next, load the Ubuntu ISO as the original disk image. You may wish to change the details of the custom disk (these are aesthetic only). Once the disk has been analysed, a virtual environment will launch. From here cd into /etc/skel. This is the skeleton folder, used to populate new users’ home folders. Create a new directory with mkdir -p Desktop/src and cd into this. Use the copy dialogue located in the top-left of the window to copy your ISO and recipe.sh into the working directory of the virtual environment. Add execute permissions to the recipe file with chmod +x recipe.sh and run it with ./recipe.sh. This will install all requirements specified in the recipe file. Once this is complete, press next to create the image, optionally selecting any additional packages to be installed from the Ubuntu defaults. Leave all other parameters as their defaults. Step 3: Burning the ImageOnce the disk image is created you can burn it to a USB stick. First, you will need to format your USB stick appropriately. This is best done using GParted. You will need to create 3 partitions in either EXT4 or FAT-32 format called OS, DEV and DATA respectively. I would advise using FAT-32 so the data partition is readable by Windows. Now you can use UNetBootin to burn the ISO file created by Cubic onto the OS partition of the USB stick. Step 4: Running the EnvironmentYou are now good to go. Plug the USB stick into university computer and go to the boot menu (usually F2, F10 or F12). From here, you can select the OS partition to boot from. Updating the EnvironmentYou cannot make permanent changes to the environment whilst running it (although you can still install new software as you normally would on Ubuntu, just with no persistence between boots). This is why we have created a secondary DEV partition. Using the OS partition, you can modify the recipe file and use the included Cubic and UNetBootin installations to create an updated image to burn to the DEV partition. Once you are happy with the changes, use the DEV partition to burn the new image to the main OS partition.","link":"/data-roaming/"},{"title":"Is it Time to Ditch the MNIST Dataset?","text":"The MNIST dataset—shown in the banner image for this post—is the bread and butter of deep learning. Just as many programmers begin their journey by learning to output the iconic “Hello, World!“, the majority of deep learning enthusiasts likely spent their formative days working with the MNIST dataset. This prevalence is evident through its frequent use in introductory tutorials on deep learning, the benchmarking of new neural network architectures, and company PR showcases attempting to demonstrate (often misguidedly) their data-science-savviness. A Case Against MNISTFor those uninitiated, the typical goal associated with this data set is to create a machine learning model that can be trained on the first 60,000 images along with their corresponding labels, and then use the knowledge gleaned from this process to predict the labels of the remaining 10,000 unseen images. This can be achieved using a wide range of machine learning algorithms, but deep learning is a tool particularly well-equipped for the task, and so MNIST’s dominance in the deep learning landscape naturally arose. The problem is, the world of deep learning has changed substantially since the original inception of the dataset; we have more efficient algorithms and more computational power to leverage. This leaves us in an awkward position where it now takes little effort to obtain a seemingly impressive score. In fact, with just a few dozen lines of code and a few minutes to train a model, it is possible to train a neural network classifier with up to 98% accuracy on the dataset. In my opinion, this tendency towards using such a simple data set for teaching and evaluation might be a minor detriment to the learning and development of new deep learning practitioners. It’s as if you tried to teach IT skills in the modern world by focusing on how to install programs. In the early days of computer science, the act of setting up software was arduous—sourcing dependencies, defining compilation options, and building binaries—and, due to the limited scope of these devices, occupied more of what would be considered standard usage. Yet, the modern day equivalent is as simple as clicking download, install, and blindly clicking ‘I agree’. Instead, IT tuition has adapted to the improved simplicity of modern operating systems to now focus on tasks that would have been deemed too challenging a few decades ago. I propose the same transformation needs to occur in deep learning, with a movement away form the MNIST dataset towards challenges more appropriate for our modern toolset and computational power. Impacts of MNISTBut what actual harm could come from reliance on the MNIST dataset for initial explorations into deep learning? I would argue that there are two key dangers. The first is that the MNIST dataset paints an unrealistic image of how simplistic the field of deep learning can be. Within reason, no matter how you design your neural network architecture, it is likely that will obtain impressive accuracy metrics after just a short round of training. When moving on from the MNIST dataset, however, this can induce a shock to the system, potentially leading to feelings of disappointment and spurring a withdrawal from the field. I would argue that closing this gap between the introductory step and more involved work might make it less likely for people to be put off deep learning without moving beyond their initial foray into the field. The second characteristic of the MNIST dataset that I find disagreeable is that it leaves very little room for thought or experimentation. When a simple, introductory model can obtain an accuracy of up to 98% on a dataset, it offers little motivation to dig deeper and explore possible improvements. Even if you did have the drive to search for these small performance gains, you’ll find that the major limiting factor in that final 2% is illegible handwriting or misclassified examples, two issues that cannot be fixed by model improvement. On the other hand, a deep learning dataset that only lends itself to modest accuracies when paired with introductory models is more likely to inspire innovation or act as an anchor for the discussion of more advanced deep learning techniques—data augmentation, semi-supervised learning, transfer learning, and the likes. I will rest my case at this point. Perhaps I haven’t convinced you that the MNIST dataset is all that bad, but at the very least, I hope we can agree that some alternatives wouldn’t go amiss. I wish to spend the rest of this post discussing two ideas in more depth. The first regards placing emphasis on the excessive simplicity of the dataset by showing how much predictive power you can leverage from just one image pixel. The second will consist of some alternatives to the MNIST data, offering fundamentally equivalent learning problems, but with some added complexity and scope for exploration. The Power of a PixelThe 70,000 images in the MNIST dataset are each composed of $28\\times28$ pixels. To really drive home the simplicity of the predictive task associated with the MNIST dataset, I will throw the majority of these away and instead ask, “for any pair of digits, how accurately can we distinguish between the two classes using only one of the 784 pixels”. That is, for each pair of digits, I will find the single pixel that has the most predictive power in distinguishing between the two classes. This only takes a few lines of code and a handful of minutes to run. The results can be seen in the following graphic. The source code for this visualisation can be found at this link. As an example, if you wished to distinguish the images of the digits 2 and 4, by using just the pixel at coordinates $(21, 10)$, you could achieve an accuracy of $88.8%$. The pair of classes that are hardest to separate is 3 and 5 with an optimal accuracy of $77.1%$ and the easiest pair is 0 and 1 with $99.8%$. The fact that even the toughest of these pairs can still be distinguished with reasonable accuracy (the rest being even easier to classify) is rather shocking. This highlights my point that the MNIST dataset is not fit for purpose when paired with deep learning algorithms. For applications involving KNN, SVMs, RFs, and in this case, a brute-force pixel model, the dataset is more appropriate, but for models as complex and powerful as neural network, I believe we should consider other possibilities. Looking ElsewhereThis post naturally leads us to question what alternatives there are available for the MNIST dataset. Luckily, there are a few, all of which offer a fundamentally equivalent problem to solve, but with the added complexity we so desire. Here is collection of my favourites (although a well-filtered search on the UCI Machine Learning Repository will bring up many more examples): Fashion MNIST—the more stylish cousin of the standard MNIST dataset. Composed of the same quantity of images split over the same number of classes, this dataset differs from the MNIST dataset only in that the images consist of items of clothing rather than handwritten. The difficulty of this dataset is only a mild step up from the traditional MNIST, yet it offers far more room for exploration. Kizushiji MNIST—in a similar vein, the Kuzushiji MNIST dataset offers another experience parallel to the original MNIST dataset, focused instead on the identification of a selection of Japanese characters. If this isn’t enough of a challenge for you though, extensions of the dataset featuring 49 and 3832 classes respectively are also available, the latter of which at a higher $64\\times64$ pixel resolution. EMNIST—an extension of the existing MNIST dataset to included the majority of alphabetic characters. Quickdraw—if you really want to push yourself, Google have open-sourced the doodling-data they collect from their website Quickdraw. The breadth of the dataset means that obtaining impressive accuracies may be a challenge, but there’s no harm in trying (after all, all it takes is a few adaptations to an existing MNIST model and some more training). With these datasets ready and waiting, go out and explore. Further, the next time you come across a benchmark or tutorial based on the MNIST dataset, give it a whirl on these alternatives too. Who knows, the added challenge might just be the birth of new inspiration.","link":"/ditch-mnist/"},{"title":"Creating a Dynamic 8-Bit Wallpaper for Linux with Python","text":"Recently, whilst browsing Reddit, I came across an old thread on r/wallpapers showcasing a collection of 8-bit desktop wallpapers each of which displaying a beautiful landscape at a different time of day. The thread linked to the following Imgur gallery containing all of the images.These images by themselves have little use but over the next few years several contributors had grouped together, using these wallpapers, to produce a live wallpaper for android, an integration with Windows and OSX, and a web implementation. Yet the Linux community were not getting much love from the project. There are a few scripts kicking around on Github allowing users to manually set up a live version of the wallpaper but these all seem to use fixed time intervals to determine when to change the wallpaper and so in the summer and winter when the time the sun is up is far from the average value, the wallpaper is completely asynchronous to real life. What I wished to do was to set up an implementation for Linux which would use local sunrise and sunset times to adjust when the wallpaper changes to result in an experience that perfectly matches up with the real world. I used data from the UK Hydrographic Office, a website offering free astronomical data for anybody to use. Using this site you can generate a text file containing the sunrise and sunset times for any location in the United Kingdom (with daylight savings already accounted for). I retrieved this data for the entirety of 2018. If you wanted complete accuracy this would need to be updated each year but with the annual difference of the sunrise/set times being in the handfuls of minutes, I don’t think this is necessary. This data comes in a very messy format and so I had to perform some cleaning with R: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# required librarieslibrary(dplyr)library(magrittr)library(tidyr)# extract data on months January to Junefirst_months &lt;- read_table(\"rise_set_times.txt\", skip = 7, n_max = 37, col_names = FALSE) %&gt;% # remove any row that is just NAs filter_all(any_vars(!is.na(.))) %&gt;% # replace blank values with NAs mutate_all(funs(replace(., . == \"\", NA)))# extract data on months July to Decemberlast_months &lt;- read_table(\"rise_set_times.txt\", skip = 65, n_max = 37, col_names = FALSE) %&gt;% filter_all(any_vars(!is.na(.))) %&gt;% # remove duplicate day column select(-1) %&gt;% mutate_all(funs(replace(., . == \"\", NA)))# combine all monthsall_months &lt;- bind_cols(first_months, last_months) %&gt;% # convert all variables to integers after removing leading whitespace mutate_all(funs(as.integer(str_remove(., \"$\\\\s\"))))# combine sunset and sunrise times (and hours and minutes) for gatheringfor (i in 1:12) { all_months %&lt;&gt;% unite(col = !! i, !! {i + 1:4}, sep = \" | \")}all_months %&lt;&gt;% rename(Day = X1)# gather months into its own columnall_months %&lt;&gt;% gather(\"Month\", \"Set_Rise_Times\", -Day)# reorder columnsall_months %&lt;&gt;% .[c(2,1,3)]# split sunset and sunrise times into two variables each (hours and minutes)all_months %&lt;&gt;% separate(Set_Rise_Times, into = c(\"Rise_Hour\", \"Rise_Minute\", \"Set_Hour\", \"Set_Minute\"), convert = TRUE)# output cleaned data as CSVwrite_csv(all_months, \"processed_times.csv\") Cleaning this data is much easier in R than Python, yet to keep this blog post self-contained for anyone familiar with just the basic use of Python, I have packaged this code up into a Shiny web app which can be viewed here. Providing you collect the data from the source above using the same settings, you will be able to upload the returned text file to this web app and it will automatically clean the data for you and provide you with a CSV file to download.Once your data is clean you can start setting up the wallpaper. You will first need Python 3 installed on your computer. You can then create a folder somewhere in your file system to contain the files relevant to this project. In this folder, you will need to download the images from the Imgur link and add the cleaned CSV file containing the sunset and sunrise times. You then need to copy any one of the wallpapers and rename it to current.png. The choice of wallpaper to copy makes no difference as it will be overridden when the wallpaper is updated. Next you need to create a Python script in this folder containing the following code with the blanks filled in: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#!/usr/bin/env pythonimport datetimeimport osimport shutilimport pandas as pdimport numpy as np# set the PATH to the location of the folder containing# the wallpapers and sunrise/set timesPATH = \"______\"# the filename of the current wallpaprCURRENT = \"current.png\"# the filename of the cleaned sunrise/set CSV filetime_data = \"______\"# the filenames of the wallpapers (e.g \"early_morning.png\")early_morning = \"______\"mid_morning = \"______\"late_morning = \"______\"early_afternoon = \"______\"mid_afternoon = \"______\"late_afternoon = \"______\"early_evening = \"______\"mid_evening = \"______\"late_evening = \"______\"early_night = \"______\"mid_night = \"______\"late_night = \"______\"# load sunrise/set times datarise_set_times = pd.read_csv(PATH + \"processed_times.csv\")# current datetimemonth = datetime.datetime.now().monthday = datetime.datetime.now().dayhour = float(datetime.datetime.now().hour)min = float(datetime.datetime.now().minute)# time as a fraction of the daytime_frac = (hour + min / 60) / 24# extract sunrise and sunset times for the particular dayday_times = rise_set_times.loc[(rise_set_times['Month'] == month) &amp; (rise_set_times['Day'] == 5)]# extract sunrise timerise_hour = day_times.values[0, 2]rise_min = day_times.values[0, 3]# convert to fraction of the dayrise_frac = (rise_hour + rise_min / 60) / 24# extract sunset timeset_hour = day_times.values[0, 4]set_min = day_times.values[0, 5]# convert to fraction of the dayset_frac = (set_hour + set_min / 60) / 24# split day into intervals based on sunrise/set timesday_change_times = np.linspace(rise_frac, set_frac, 10)# split night into intervals based on sunrise/set timesnight_change_times = np.linspace(set_frac, rise_frac + 1, 4) % 1# combine day and night timeschange_times = np.append(day_change_times, night_change_times[1])change_times = np.insert(change_times, 0, night_change_times[2])# decide on the new wallpaper based on the current time of the dayif time_frac &lt;= change_times[0]: shutil.copy(PATH + mid_night, PATH + CURRENT)elif time_frac &lt;= change_times[1]: shutil.copy(PATH + late_night, PATH + CURRENT)elif time_frac &lt;= change_times[2]: shutil.copy(PATH + early_morning, PATH + CURRENT)elif time_frac &lt;= change_times[3]: shutil.copy(PATH + mid_morning, PATH + CURRENT)elif time_frac &lt;= change_times[4]: shutil.copy(PATH + late_morning, PATH + CURRENT)elif time_frac &lt;= change_times[5]: shutil.copy(PATH + early_afternoon, PATH + CURRENT)elif time_frac &lt;= change_times[6]: shutil.copy(PATH + mid_afternoon, PATH + CURRENT)elif time_frac &lt;= change_times[7]: shutil.copy(PATH + late_afternoon, PATH + CURRENT)elif time_frac &lt;= change_times[8]: shutil.copy(PATH + early_evening, PATH + CURRENT)elif time_frac &lt;= change_times[9]: shutil.copy(PATH + mid_evening, PATH + CURRENT)elif time_frac &lt;= change_times[10]: shutil.copy(PATH + late_evening, PATH + CURRENT)elif time_frac &lt;= change_times[11]: shutil.copy(PATH + early_night, PATH + CURRENT)else : shutil.copy(PATH + mid_night, PATH + CURRENT)# set the background imageos.system(\"/usr/bin/gsettings set org.gnome.desktop.background picture-uri \\\"file:///\" + PATH + CURRENT + \"\\\"\")# alert the user that the background has been changedprint(\"Wallpaper changed.\") The last step is to to use cron, a Linux utility for scheduling jobs to run. To edit the cron tab(le) for your user simply open up the Unix terminal and enter 1crontab -e This will open the cron tab(le) and you can enter the following two lines, replacing the place-holders with the relevant paths, at the bottom of the editor 12*/5 * * * * PATH_TO_PYTHON_INSTALLATION PATH_TO_WALLPAPER_CHANGING_SCRIPT@reboot PATH_TO_WALLPAPER_CHANGING_SCRIPT Don’t forget to leave a new line after these new entries else they will fail to interpretted. If you are unsure of the location of your Python installation, running which python in the terminal should give you the required path. The first line is used to rerun the wallpaper-updating script every 5 minutes and the second, unsurprisingly, runs it whenever the computer reboots. If all has gone to plan, you should now have your own sun-tracking, live, 8-bit desktop wallpaper.","link":"/dynamic-wallpaper/"},{"title":"Efficient Calculation of Efficient Frontiers","text":"The SetupSuppose we have a fixed amount of money that we have set aside for investment. We have narrowed down the equities we to a list of length $n$. From analysing historical data we know the vector of expected returns for each equity $v$ and the covariance matrix of returns $\\Sigma$..With that in mind, our goal is to choose the simplex vector $x$ (i.e. $0\\leq x_i \\leq 1$, $\\sum x_i = 1$) that maximises our expected return for any given risk tolerance $r=x^T \\Sigma x$. If we were to find this $w$ for each $r&gt;0$ and use this to calculate our expected return $e = v^T x$, we would end up with a curve known as the efficient frontier, which maximises our return for any given risk tolerance. I became aware of the efficient frontier when talking to a collegue about a project he was working on. As an intermediate step, this involved calculating efficient frontiers, and I was surprised to see the approach that he was taking. It involved randomly generating a large number of simplex vectors, calculating their associated risk and expected return, before plotting these all on a graph as shown below.With enough simulations, you become able to see the outline of the efficient frontier, the curve marking the top boundary of the data points. This could then be turned into a curve by calculating the convex hull of the points, perhaps with some added smoothing. This approach stood out to me for two reasons: There was no way it would scale; as the number of equities increases, the space of portfolios to explore grows roughly exponentially. This means that the solution is not appropriate for real-world use where you would wish to consider a large number of equities. The efficient frontiers generated looked an awful lot like a parabola—if this was the case then surely there was an analytic way of finding the equation of this curve. Another popular approach for computing the effient frontier is using gradient descent. Under the right assumptions (the same as we will introduce later), the problem we are looking at is convex, and so we are guaranteed to find a local optimum, with convergence that is quadratically fast. This is still slower than the method we’ll look at below so we won’t discuss it further, but it is worth noting that it is also much more efficient than the random approach above. An Analytic ApproachAs mentioned above, I was immediately suspicious that an analytic solution existed to the efficient frontier problem. It turned out that this was the case provided we introduced one key assumption: shorting is valid. I will not get into the financial definiton of short-selling now (though you can read about here). All we need to care about is that it corresponds to use removing the bounds on our portfolio weights $x_i$. We still however enforce that $\\sum x_i = 1$. Under this condition, our problem becauses a quadratic programming problem which can be solved using Lagrangian multipliers. Note, it is algebraically more elegant to solve the dual form of our problem (minimise risk for a given return), though the same solution is arrived at. It is worth noting that although the introduction of shorting might make this analytic solution inappropriate for markets that do not allow it (more discussion on this later), the opposite flaw exists for the randomised approach we saw earlier in that it only works when shorting is forbidden, else the space of solutions to explore grows to intractable proportions. Our problem can be stated as $$\\begin{align}\\text{minimise} \\quad &amp; w = x^T \\Sigma x \\\\\\text{s.t.} \\quad &amp; v^T x = e \\\\&amp; 1^T x = 1\\end{align}$$ The corresponding Lagrangian is $$\\mathcal{L}(x, \\lambda, \\mu) = x^T \\Sigma x - \\lambda (v^T - e) - \\mu (1^Tx - 1)$$ The derivative is therefore $$\\Delta_{x, \\lambda, \\mu}(x, \\lambda, \\mu) = \\begin{pmatrix} 2x^T\\Sigma - \\lambda v^T - \\mu 1^T \\ e - v^T x \\ 1 - 1^Tx\\end{pmatrix}$$ Note, that this is a vector of length $n+2$. The usual way to proceed would be to set this equal to zero and solve the resulting system of equations through substitution. I tried this for a while and stuggled greatly. This is when I noticed a clever cheat which draws the straight out. This was to write the above system as a matrix equation in $x$, $\\lambda$ and $\\mu$. $$\\begin{pmatrix}2\\Sigma &amp; v &amp; 1 \\\\v^T &amp; 0 &amp; 0 \\\\1^T &amp; 0 &amp; 0\\end{pmatrix}\\begin{pmatrix}x \\ \\lambda \\ \\mu\\end{pmatrix}=\\begin{pmatrix}0 \\ e \\ 1\\end{pmatrix}$$ It is easy to verify that the matrix on the left is invertible whenever $\\Sigma$ is postive definite (a reasonable assumption). To do this we use the following expression for the determinant of a block matrix. $$\\text{det}\\begin{pmatrix}A &amp; B \\\\C &amp; D\\end{pmatrix}= \\text{det}(A) \\cdot \\text{det}(D - CA^{-1}B)$$ Alternatively, you can solve for $x$ directly, to avoid the extra cost of solving the entire system. Try It For YourselfSince the matrix above is symmetric, postive definite, we can use efficient methods for solving the system. This gives us the optimal value of $x$ with incredible efficiency. To illustrate this difference, I have developed a Shiny dashboard to demo the initial and efficient approaches, benchmarking there runtimes to show how vastly superior this approach is. You can find the dashboard here. Wrapping UpTwo final thoughts… How much does the optimal portfolio under shorting differ from that without? Could we simply project the shorted solution onto the space of non-shorted solutions to obtain an approximately optimum solution. I hope to return to this problem at a later point, hoping to obtain either exact or probabilistic bounds. After discussing this project with another collegue, it turns out that this solution is not completely original (though I believe my method of proof is novel and more elegant than other approaches). That said, it was original to me at the time of discovering this approach, which I think is good enough for an undergraduate.","link":"/efficient-frontiers/"},{"title":"Excel Abuse: Peaceable Queens","text":"I understand why people use Excel. I really do. As a user of Python and R, amongst several other programming languages, I often find it frustrating that so many people gravitate towards Excel regardless of the nature of the problem they are trying to solve. For simple accounting, visualisation, and data entry I think that Excel is the perfect tool. As the scale and complexity of applications begin to grow, however, it starts to feel that the comfort people find in Excel ends up being more of a burden than a blessing. In my work, I have seen many complicated and expansive software tools productionised using Excel. On top of this, the lack of portablity between Excel versions, the difficulty of maintaining and documenting such applications, and the inivitable performance limits that a user runs into, have repeatedly made themselves clear. All of this hassle makes me wonder whether it would just be easier for the developers of these products to learn a new, more suitable toolset. It may take a while to learn, say Python, to a competent level, but after seeing the frustration Excel can cause, maybe this is a worthy investment for the long-run. I say this and yet, I’ve never truely given Excel a full evaluation; I’ve never attempted to throw some of the more complicated projects I come across at the program and see how it copes. This is going to change, however. So that I can feel entirely justified in nudging developers away from Excel towards more formal toolsets, I have decided to start a new series in which I aim to push Excel to its limits to see if (more likely, when) it breaks and how the development of such complicated applications manifests itself. In this first installment, we will look at a mathematical puzzle known as the peaceable queens problem. Can Excel solve this problem? If so, how monsterous will the solution become? Read on to find out. The Peaceable Queens ProblemThe peaceable queens problem is relatively obscure in the world of mathematics, but presents an interesting challenge just about suitable for solving in Excel. It concerns a chessboard of dimensions $m\\times n$ where $m,n$ are arbritary positive integers. The goal of the problem is to find the maximum integer $z$ such that $z$ black queens and $z$ white queens can coexist on the chessboard without attacking each other. For example, in the case where $m,n=4$ we have a $4\\times4$ chessboard. The diagram below first shows an invalid (non-peaceable) solution. This is followed by a valid solution (all queens coexist) that is not optimal (at least one queen of each colour can be added whilst maintaining peace). The final board shows one of many optimal solutions featuring two queens of each colour. You can try as hard as you’d like to find an arrangement with three queens of each colour, but trust me, you’ll be searching for a while! Note that we require any solution to have the same number of black and white queens. Hence, even if we can add more queens of just one colour to an existing solution, this has no value to us. The above gives a solution (though admittedly, not a proven one) for a specific case. What about general $m,n$? Before we create an Excel spreadsheet to solve this problem, let’s see what mathematicians have found out using general methods. The State of the ArtI came across this problem when looking at the Online Encyclopedia of Integer Sequences (OEIS). This site consists of an immense collection of sequences of integers (e.g. the Fibonacci sequence, the prime numbers). The 250,000th of which just so happens to be the sequence of optimal values of $z$ in the peaceable queens problem for square chessboards of increasing sizes. For example, the fourth entry in the sequence corresponds to the chessboard of size $4\\times4$ and so has a value of $2$ as we saw above. The earliest known reference to this problem was by Stephen Ainley in his 1977 book of mathematical puzzles. Ainley found layouts for square chessboards up to size $30\\times30$, only one of which ($27\\times27$) has since been improved upon. There is reason to believe that many of these solutions are indeed optimal, yet we have only proved this for results up to $15\\times15$. Therefore if we can solve the problem for chessboards of size anywhere near $15\\times15$ we will have achieved a lot. Although we do not currently have optimality proofs for $n&gt;16$, reasonable lower and upper bounds do exist. You can read more about these in the [sequence]’s(https://oeis.org/A250000) comments section. A Mathematical FormulationBefore we jump into our Excel-based solution, we will have to reframe the problem in a mathematical setting. Specifically, we can describe the problem as an Integer Programming Problem. It’s not worth sweating over the details of this and so if you don’t care for the maths, you can skip the rest of this section. The important takeaway is that the problem can be thought of as this special integer programming formulation, the likes of which Excel is equipped to solve. Specifically, for $i\\in[m]$, $j\\in[n]$ we let $b_{ij}$, $w_{ij}$ represent be the binary indictors of whether there is a black or white queen in the square $(i,j)$, respectively. Our goal then becomes to maximise $$z := \\sum_{i=1}^m\\sum_{j=1}^n b_{ij}$$ subject to $$\\sum_{i=1}^m\\sum_{j=1}^n b_{ij} = \\sum_{i=1}^m\\sum_{j=1}^n w_{ij}$$ and $$b_{i_1j_1}+w_{i_2j_2}\\leq 1 \\quad \\forall ((i_1, j_1), (i_2, j_2))\\in M$$ where $M$ is the set of all ordered pairs of squares that share a line (row, column or diagonal) of the board. It is clear to see that this formulation involves $2mn$ variables. Counting the number of constraints is more challenging. We will have $2mn$ binary constraints plus one more to enforce an equal number of black and white queens. Then we are left with constraints for peacability, of which there will be $|M|$. For each $(i_1, j_1)\\in[m]\\times[n]$ there are $(m-1) + (n-1) + 2(\\text{min}(m, n) - 1)$ squares $(i_2, j_2)\\in[m]\\times[n]$ that share a line with $(i_1, j_1)$ (you can just count them; don’t overthink it) and so |M| = $mn(m-1 + n-1 + 2(\\text{min}(m, n) - 1))$. It is clear that this dominates the number of constraints and so overall we have $\\mathcal{O}(mn\\text{max}(m, n))$ constraints. For $m=n$ we see that the number of constraints is $\\mathcal{O}(n^3)$—not great! This formulation turns out to be sub-optimal, but for our purposes it will do. See here for a more detailed analysis of the problem. Excel AbuseYou can download the final spreadsheet from this link. Note that you will need to enable content as the worksheet depends on macros. With the maths out of the way, we can move onto solving the problem using Excel. As suggested above, Excel has a built-in add-on for solving many types of optimisation problems such as integer programming problems. Since we can formulate the peaceable queens problem in this way, we’re in a good position to solve it. The only issue is that Excel requires a lot of structural and formula overhead to start the solver (i.e. we must populate the worksheet with a lot of relevant information). In my first draft I did this by hand for the $3\\times3$ case, but this takes a long time so is not suitable for a general solution. Instead we can leverage VBA, a scripting language for controlling Excel, to automatically generate such content. Creating a SolutionI began by creating a user interface consisting of two dimension inputs, two buttons, and a static colour key. The dimension inputs have data validation restricting values to be integers in the range 2–3 (more on this later). Once these are set to the desired values, clicking the ‘Solve Problem’ button will populate the sheet with the information Excel’s solver requires and then begin solving the problem. After a few seconds a dialog box will appear to notify the user that a solution has been obtained. Note that when a solution is found it has also been proven to be optimal. There may be other valid solutions but they will have at most the same number of queens. A solution can then be read off from the green boxes, the orange cells showing the optimal value of $z$ found. The box titled ‘Blacks’ corresponds to the positions of the black queens (a square has value one if and only if there is a black queen there) and likewise for the ‘Whites’. We have a solution…for the $3\\times3$ case that is. Unfortunately, the built-in Excel solver does not scale well. Due to this, Microsoft have put limits on the number of variables (100) and constraints (200) you are able to use for an optimisation problem. For the $3 \\times 3$ and smaller cases, we do not surpass these limits yet for $3 \\times 4$ and larger we do. Therefore, although our solution works in theory, it has little us in practice. Going FurtherThe limitations of the built-in solver are rather disappointing. Thankfully, a workaround comes in the form of FrontlineSolvers‘ suite of 3rd-party Excel solver plugins. Unfortunately, this software is equipped hefty price tag of a few thousand dollars. Luckily for us, there is a free 15-day trial. I signed up for this (no card required), installed the software, and let it loose on some larger problems. This raised the size of the largest possible board to around $10\\times10$. This is still not limited by the implemented approach but rather that the trial version has its own (though admittedly, large) variable and constraint limits too. My back-of-the-envelope calculation suggests that the full version of this solver (with no limits and access to solving algorithms optimised for sparse systems) could result in solutions for boards up to $12\\times12$ and perhaps $13\\times13$ with some patience. This is incredible considering that no approach to the problem is yet to surpass $15\\times15$. Adapting the workbook to use FrontlineSolvers’ solver is simple. First install the trial version of the solver. Then open the existing workbook, disable protection and remove data validation from the dimension cells. Click the ‘Solve Problem’ button as before. This populate the spreadsheet but will not find a solution (since this surpasses the default solver’s limits). The sheet will now be protected so disable this again. Navigate to the ‘AnalyticSolver’ tab in the top ribbon and select ‘Model’. This will open a sidebar, and selecting the green play button will solve the problem. The largest solution I found was for the $10\\times10$ board. This took a few hours of my computer’s intense labour to discover and subsequently prove optimality. Thankfully there is no NSPCMP (National Society for the Prevention of Cruelty to Microsoft Products) so I got away with this abuse. The resulting layout is as follows. ConclusionI don’t want this post to drag on so I will refrain from offering a solution using a more appropriate tool such as Python or R for comparison. Note though, that even though this solution does technically work, it is clunkly, ugly, and would require several thousands of dollars worth of spare cash to implement asl a permanent solution (though why you’d have a regular need for find peaceable arrangements of queens, I do not know). An equivalent tool using a formal language could easily be much faster, portable, and—most importantly—completely free for any scale your computer can handle. Solving this problem was fun but the conclusion only confirms my initial beliefs—if the problem is this complex, save Excel the abuse.","link":"/excel-abuse-peaceable-queens/"},{"title":"Integration Tricks using the Exponential Distribution","text":"Tricky IntegralsUnless you are a Physicist, the sight of the following integral will most likely send a shiver down your spine. $$\\int_0^\\infty 6x^2e^{-3x}dx$$ This is not a fun integral to evaluate. The standard approach involves integrating by parts twice. If you remember to divide by the 3 when required, keep an eye on that tricky negative exponent, and correcting substitute your limits both times, you might end up with the correct answer of $4/9$ (quite possibly with a headache to accompany it). This method is cumbersome and mistake-prone. But thankfully this isn’t the only way. I set aside Physicists in the introduction as they tend to be rather proficient with their integrals and have probably just memorised the fact that $$\\int_0^\\infty x^ne^{-x}dx = n! \\qquad n\\in\\mathbb{N}$$ This is a really useful identity to know. A quick substitution in the original problem and you have exactly that, up to a constant that is. If this method works for you then go ahead. It is undeniably fast and you are far less likely to make a mistake than with the original method. Yet this still doesn’t sit well with me. I’ve never been a fan of learning formulae. You can forget them or confuse them with others. And even if you remember it perfectly, the substitution can still give you trouble. Lastly, even if you get all of these steps right, how can you justify to yourself that you did in fact remember the correct method. The result’s appearance is similar to the appearance of that of a rabbit from a hat - magic. I offer one more approach. The Exponential DistributionBefore we get to the brunt of the problem, we need to take a slight detour to introduce the exponential distribution to those unacquainted with it. This is is a very prominent distribution in statistics and so I shall only give a quick run-through of its uses and properties as I hope most readers will be familiar with them already. For a more detailed look at it, the corresponding Wikipedia page does an excellent job. The exponential distribution is a probability distribution that describes the time between the occurrence of events in a Poisson process, a process in which events occur at a constant rate, independent of when the last event occurred. It has a single parameter, $\\lambda &gt; 0$, which controls the rate of the Poisson process, a small $\\lambda$ meaning that events occur more frequently. This can be used to model processes such as the time between customers arriving at a checkout, the time until a radioactive particle decays, or distance between mutations on a DNA strand. The key property of this distribution is that it is ‘memoryless’. In fact, it is the only continuous distribution with this property. This means that if you continue to wait for an event to occur, the probability of the event occurring in any instant neither increases or decreases with time. These characteristics are very interesting but the properties we are interested in for solving integrals are the probability density function (PDF) and the moments of an exponential random variable. We start with the PDF of an exponentially distributed random variable $X$. It is given by $$f_X(x) =\\begin{cases} \\lambda e^{-\\lambda x} &amp; x\\geq 0 \\\\ 0 &amp; x &lt; 0 \\end{cases}$$ This gives us an insight into a simple case of the problem. Since, the support of an exponential random variable is $(0,\\infty)$ we must have that the integral of its density over that region is 1 and so $$\\int_0^{\\infty}\\lambda e^{-\\lambda x}dx = 1 \\qquad \\lambda \\in \\mathbb{R} \\tag{1}$$ We can use this for computing integrals of this form with the most minimal work. For example $$\\int_0^{\\infty}8 e^{-2 x}dx \\stackrel{(1)}{=} 4\\int_0^{\\infty} 2e^{-2 x}dx = 4$$ Adding an $x$ or Two“So what?”, you may ask, “how does this help us with our original problem?”. And at the moment we still can’t solve the more complex case The factor of $x^2$ makes this a bit trickier but with the introduction of two more properties of the exponential distribution this becomes trivial. In particular, we need the mean and variance of the exponential random variable $X$. These are $$\\mathbb{E}(X) = \\frac{1}{\\lambda} \\\\\\textrm{Var}(X) = \\frac{1}{\\lambda^2}$$ and using the formula relating variance and the first two moments of a random variable we get $$\\mathbb{E}(X^2) = \\textrm{Var}(X) + \\mathbb{E} (X)^2= \\frac{1}{\\lambda^2} + \\frac{1}{\\lambda^2} = \\frac{2}{\\lambda^2}$$ These expected values are defined in terms of the integrals $$\\mathbb{E}(X) = \\int_0^{\\infty}\\lambda x e^{-\\lambda x}dx$$ and $$\\mathbb{E}(X^2) = \\int_0^{\\infty}\\lambda x^2 e^{-\\lambda x} dx$$ and so we get $$\\int_0^{\\infty}\\lambda xe^{-\\lambda x} dx= \\frac{1}{\\lambda} \\qquad \\lambda \\in \\mathbb{R}\\tag{2}$$and$$\\int_0^{\\infty}\\lambda x^2e^{-\\lambda x} dx= \\frac{2}{\\lambda^2} \\qquad \\lambda \\in \\mathbb{R}\\tag{3}$$ This lets us quickly solve problems featuring a single power of $x$ $$\\int_0^\\infty 2xe^{-5x}dx = \\frac{2}{5}\\int_0^\\infty 5xe^{-5x}dx \\stackrel{(2)}{=} \\frac{2}{5} \\cdot \\frac{1}{5}=\\frac{2}{25}$$ or a factor of $x^2$. Returning to our original problem, this gives a solution in no time $$\\int_0^\\infty 6x^2e^{-3x} dx= 2\\int_0^\\infty 3x^2e^{-3x} dx\\stackrel{(3)}{=} 2 \\cdot \\frac{2}{3^2}=\\frac{4}{9}$$ When you compare that working to the amount needed when using repeated integration by parts, this method seems like a God-send. If you are not familiar with the exponential distribution, this may feel no better than the Physics approach of learning to rattle off a relevant formula. Surely we’re just learning some new formulae instead. Well…yes. That’s true. But, the difference is that the exponential distribution already has a great deal of importance in statistics anyway and so any proficient statistician will be required to know the PDF of the exponential anyway; its mean and variance too. The value of $\\mathbb{E}(X^2)$ may not be something you commit to memory but using the relation that it has with $\\mathbb{E}(X)$ and $\\textrm{Var}(X)$, it can be derived in a moment with little thought. In fact, a confident statistician will even know the moment-generating function (MGF) of the exponential distribution which, due to its simple derivatives, allows you to generalise these methods to any power of $x$. Generalisation to All PowersIn order to handle the general case in which we have any power of $x$ we need to look at the MGF of an exponential random variable $X$. This is given by $$M_X(t) = \\frac{\\lambda}{\\lambda-t} \\qquad |t| &lt; \\lambda$$ This is worth memorising but is not difficult to derive if forgotten. It also has very simple derivatives $$M’_X(t) = \\frac{\\lambda}{(\\lambda-t)^2} \\\\M’’_X(t) = \\frac{2\\lambda}{(\\lambda-t)^3} \\\\M^{(3)}_X(t) = \\frac{6\\lambda}{(\\lambda-t)^4} $$ In general we have $$M^{(n)}_X(t) = \\frac{n!\\lambda}{(\\lambda-t)^{n+1}} \\qquad n\\in \\mathbb{N}$$ All of which converge for $|t| &lt; \\lambda$. These are useful because in general we have the property that $$M^{(n)}_X(0) = \\mathbb{E}(X^n) \\qquad n\\in \\mathbb{N}$$ Which gives for the exponential distribution that $$\\int_0^{\\infty}\\lambda x^ne^{-\\lambda x}dx = \\mathbb{E}(X^n) = M^{(n)}_X(0) = \\frac{n!}{\\lambda^{n}} \\qquad n\\in \\mathbb{N} \\tag4$$ In fact, after using a substitution to remove $\\lambda$ from the exponent, this is exactly the same formula as we gave for the Physics approach above. We are now equipped to handle any integral of this form. To close off this post off, let’s tackle one last Goliath of an integral. With integration by parts, this would have taken several minutes, five repeats, and most likely resulted in an incorrect answer. But using the exponential distribution, it becomes a cake-walk. $$\\int_0^\\infty (x^5+4x^3+2x^2) e^{-2x}dx \\\\= \\frac{1}{2}\\int_0^\\infty 2x^5e^{-2x}dx + 2\\int_0^\\infty 2x^3e^{-2x}dx + \\int_0^\\infty 2x^2e^{-2x}dx \\\\\\stackrel{(4)}{=} \\frac{1}{2} \\cdot \\frac{120}{32} + 2\\cdot \\frac{6}{8} + \\frac{2}{4} \\\\= \\frac{15}{8} + \\frac{3}{2} + \\frac{1}{2} \\\\= \\frac{31}{8}$$ And we barely even broke a sweat…","link":"/exponential-integration-tricks/"},{"title":"Generating Normal Random Variables - Part 1: Inverse Transform Sampling","text":"The Importance of the Normal DistributionThe normal (or Gaussian) distribution is arguably the most important and widely used probability distribution in the field of statistics. First studied in the early 18th century by Abraham de Moivre, a French mathematician (though later explored further by Karl Friedrich Gauss, hence the name), the distribution is so prevalent in probability theory due to its unique statistical properties which lead to it featuring heavily in many natural contexts. One of the most important properties of the normal distribution is the central limit theorem. This states that if you take any sequence of independent, identically distributed random variables, calculate their partial sums and scale these so they are centred around zero and have unit variance, then the resulting random variable will have a distribution that is approximately normal. For example, if you were to flip a coin again and again until boredom forced you to stop, the resulting number of heads obtained (after appropriate scaling) would have an approximately normal distribution. Other examples of real life scenarios in which the normal distribution appears either approximately or exactly are: The distribution of heights and weights The time it takes to travel to work The volatility of stocks The position of a diffusing particle relative to its starting point Since the normal distribution is so common in our statistical work and nature itself, it makes sense that we would want a method of generating our own normally distributed random variables. This is exactly what this series of blog posts is about. Generating Normal Random VariablesAnyone familiar with R may be wondering why this is a topic even worth discussing. After all, calling the built-in rnorm() function will give you as many normal random variables as you could possibly need with no effort required. And that is true; but how exactly does rnorm() give you these values? As smart as your computer may be, it turns out that it does not have a way to directly generate normal random variables. In fact, all it can do is generate standard uniform variables - that is picking a number randomly from the interval (0,1). Even then, technically, this is not a random process, rather pseudo-random - the algorithms designed to do this, however, are good enough that we’ll consider them random. It is then the job of cleverly designed statistical algorithms to take these standard uniform random variables and mutate them into the normal random variables that we require. This is exactly what rnorm() does, and by the end of this post you’ll understand roughly how. Before we begin generating random variables which follow the normal distribution, it is worth noting that really there is no such thing as the normal distribution. In fact there are infinitely many such distributions (as shown above). Each of these is categorised uniquely by their mean and variance. However, due to another useful property of the normal distribution, if we wish to generate any normal random variable, the only difficult part is first calculating a standard normal random variable - that is, belonging to a normal distribution with mean zero and unit variance. Once we have this, we can use the property that if $Z$ is a normal random variable, then $X = \\sigma Z + \\mu$ is a normal random variable with mean $\\mu$ and variance $\\sigma^2$. Inverse Transform SamplingA Theoretical BackgroundThere are many different methods of generating normal random variables from a source of standard uniform variables many of which we will discuss in the following posts. The first of which and the subject of this post, is known as inverse transform sampling. The reason I have decided to start with this method, is that it is one of the simplest to introduce and its shortcomings lead on nicely to introducing alternative approaches. Inverse transform sampling is a very versatile technique. Its use is not just limited to generating normal random variables but it can be used (in theory at least - we’ll see more about this later) to generate a random variable with any given distribution. All we need to know is the cumulative distribution function (CDF) of the random variable we are trying to generate. This is the function given by $F_X(x) = \\mathbb{P}(X\\leq x)$. In order to utilise this CDF, along with our source of standard uniform random variables, to perform inverse transform sampling we first require its inverse. By definition of a CDF, we know that $F_X(x)$ is non-decreasing. Unfortunately, this alone cannot guarantee that $F_X(x)$ has a true inverse function; to be sure of this we require that $F_X(x)$ is strictly increasing. What we can do however is find a left-inverse, that is a function such that for any $u \\in (0,1)$, $F_X\\left(F_X^{-1}(u)\\right) = u$. Specifically we can take $F_X^{-1}(u) = \\textrm{inf}{x : F_X(x) \\geq u }$. Although a true inverse is often easy to handle Mathematically, a left-inverse is sufficient for the requirements of inverse transform sampling. This fact is particularly important when the distribution we are trying to generate is discrete, in which case we are certain to not have a true inverse. We now make the following observation: $$\\begin{align}&amp; \\mathbb{P}(F_X^{-1}(U) \\leq x) &amp; \\\\ &amp;= \\mathbb{P}(U \\leq F_X(x)) &amp; \\textrm{[applying } F_X \\textrm{ to both sides]} \\\\ &amp; = F_X(x) &amp; \\textrm{[since } \\mathbb{P}(U\\leq y) = y \\textrm{ when } U \\textrm{ is a standard uniform r.v.]}\\end{align}$$ That is to say that $F_X^{-1}(U)$ and $X$ have the same distribution. Therefore, by taking our standard uniform samples and passing them through our inverse CDF, we can obtain new samples which will follow the target distribution that we were aiming to generate. The IntuitionIf the last section seemed to go a bit too deep, don’t worry – understanding the theoretical underpinnings of inverse transform sampling is not necessary for its algorithmic implementation. The main take away is that if we can find an inverse (or near-inverse) function for our target CDF $F_X$ which we will denote $F_X^{-1}$, we can then generate our standard uniform random variables, pop them into our inverse CDF, and as if by magic, the resulting values will follow the distribution we were aiming to generate. There is one problem though. This is all dependent on if we can find an inverse CDF. Furthermore, what if we can’t even find the CDF in the first place? It turns out that, although most of the distributions we care about have nice CDFs with computable inverses, the vast majority of random variables do not. When we do have a manageable CDF, we’re all set; inverse transform sampling will work perfectly and if our inverse CDF is easy to compute, will be highly efficient. If we can’t find such a CDF or its inverse, then we will have to consider other approaches. A Concrete ExampleTo demonstrate inverse transform sampling, we will apply it on a distribution which has a CDF suited to the method: the exponential distribution. The probability density function of an exponential random variable $X$ with rate parameter $\\lambda$ is given by $$f_X(x)=\\begin{cases}e^{-\\lambda x} &amp; x &gt; 0\\\\0 &amp; \\textrm{otherwise}\\end{cases}$$ And so integrating gives a CDF of $$F_X(x)=\\begin{cases}1-e^{-\\lambda x} &amp; x &gt; 0\\\\0 &amp; \\textrm{otherwise}\\end{cases}$$ We then set $u = F_X(x)$ and solve for $x$ to get $F_X^{-1}(u) = -\\frac{1}{\\lambda}\\ln(1-u)$. From the result above, this tells us that the random variable $-\\frac{1}{\\lambda}\\ln(1-U)$ has the same distribution as $X$. Furthermore, since $U$ and $1-U$ have the same distribution, we can generate $X$ by using $-\\frac{1}{\\lambda}\\ln(U)$. We can see this in practice here. 123456789101112lambda &lt;- 2U &lt;- runif(10000)X_simulated &lt;- -1 / lambda * log(U)X_expected &lt;- rexp(10000, lambda)par(mfrow = c(1, 2))hist(X_simulated, xlab = \"X\", main = \"Histrogram of simulated values\", breaks = 10, xlim = c(0, 5))hist(X_expected, xlab = \"X\", main = \"Histrogram of expected values\", breaks = 10, xlim = c(0, 5)) The exponential distribution isn’t the only distribution that works well for inverse transform sampling. Here is a table of other distributions along with their PDFs, CDFs, and inverse CDFs, that can be effectively generated using inverse transform sampling. table { width: 100% !important; } th { text-align: center !important; } Distribution Name Probability Density Function Cumulative Density Function Inverse CDF Pareto $\\frac{\\alpha \\beta^\\alpha}{x^{\\alpha+1}}$ $1 - \\left(\\frac{\\beta}{x}\\right)^\\alpha$ $\\frac{\\beta}{\\sqrt[\\leftroot{-3}\\uproot{3}\\alpha]{1-u}}$ Cauchy $\\frac{1}{\\pi\\gamma\\left[1+\\left(\\frac{x-x_0}{\\gamma}\\right)^2\\right]}$ $\\frac{1}{\\pi}\\arctan\\left(\\frac{x-x_0}{\\gamma}\\right) + \\frac{1}{2}$ $\\gamma\\tan\\left(\\pi\\left(u-\\frac{1}{2}\\right)\\right) + x_0$ Logistic $\\frac{1}{4}\\textrm{sech}^2\\left(\\frac{x-\\mu}{2s}\\right)$ $\\frac{1}{s\\left(1+e^{-\\frac{x-\\mu}{s}}\\right)}$ $\\mu - s\\log\\left(\\frac{1-u}{u}\\right)$ Rayleigh $\\frac{x}{\\sigma^2}e^\\frac{-x^2}{2\\sigma^2}$ $1 - e^\\frac{-x^2}{2\\sigma^2}$ $\\sqrt{2\\sigma^2\\log\\left(\\frac{1}{1-u}\\right)}$ Application to the Normal DistributionWith a simple example of inverse transform sampling under our belt, we can now turn our attention to the normal distribution. As mentioned in the introduction, it will suffice to generate random variables with a standard normal distribution and then scale them appropriately to obtain the distribution we were targeting. The cumulative density function of the standard normal distribution is given by $$f_X(x)=\\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-x^2}{2}}$$ This leads us into a problem. It can be shown that the integral $\\int e^{-x^2} dx$ has no closed form using the standard elementary functions. That means that we have no way of finding a simple expression for the normal CDF. The normal distribution is simply just one of those many troublesome distributions for which inverse transform sampling is difficult. Since we can’t find a closed form the CDF we have very little chance of finding a nice way of expressing the inverse CDF that we need to implement the algorithm. Thankfully though, we have a workaround. Although we are unable to find a closed form for the inverse CDF, it is not too hard to approximate it using numerical analysis. The exact details of the process are far beyond the scope of this post, revolving around Taylor series, convergence, and difference equations. All you need to know is that it is not too hard to approximate the inverse CDF we require using some carefully crafted polynomials. Using this trick, we can carry on just as if we have an explicit form for the CDf. Implementing the methodThe AlgorithmAs mentioned in the previous section, we are forced to use an approximation for the inverse CDF of the normal distribution in order to implement this method. We could code this up ourselves but, luckily for us, R already has a built-in function which can do this for us. Specifically we will use the qnorm() function. This, given a vector of values between zero and one, will return for each element $u$, the value of the constant $x$ such that $\\mathbb{P}(X\\leq x) = u$. After a little thought, it should be clear that this is exactly the same process that our inverse CDF follows. Under the hood R uses the method we described above to generate these values - an approximation using carefully chosen polynomials. The raw source code for the qnorm() function can be found here if you want to take a look at the internal workings. Using this function to approximate our inverse CDF, the inverse transform sampling algorithm can be coded up in just a few lines. 12345678inverseTransform &lt;- function(n = 1) { # generate 'n' standard uniform samples u &lt;- runif(n) # pass these samples through our inverse CDF x &lt;- qnorm(u) # return the new, normally-distributed values return(x)} VerificationWe should now verify that the samples generated by this function are in fact normal. To do this we will begin by generating a thousand such samples. 12# generate 1000 samples using our inverse transform sampling algorithmsamples &lt;- inverseTransform(10^3) The first check we will perform is to plot the histogram for these values. We also overlay the expected density curve for the standard normal distribution. 12345# plot a histogram of our sampleshist(samples, freq = FALSE)# add a standard normal density curvedensitySamps &lt;- seq(-4, 4, by = 0.01)lines(densitySamps, dnorm(densitySamps), lwd = 2, col = \"red\") Looking good! Another visualisation we can use to verify normality is the normal Q-Q plot. This compares the quantiles of our sample to the expected quantiles of a normal distribution. The closer to the diagonal line the points lie, the more likely it is that our samples are normally distributed. 12qqnorm(samples)qqline(samples) That is a pretty much perfect Q-Q plot. There is some deviation from the diagonal at the tails of the distribution but that is to be expected since there are far fewer samples generated in those regions. Lastly we can apply the Shapiro-Wilk test. This is a hypothesis test which evaluates how likely it is that a collection of samples were drawn from a normal distribution. 1shapiro.test(samples) Shapiro-Wilk normality test data: samples W = 0.99854, p-value = 0.5789The test statistic generated ranges from zero to one, a higher value indicating that the samples are more likely to be from a normal distribution. Here we obtained a value of greater than 0.99 which should fill us with confidence. If we so desired we could also use the Kolmogorov-Smirnov test - a hypothesis test similar to Shapiro-Wilk but that is non-parametric and generalises to comparing any two distributions - but by this point we already have a substantial amount of evidence that our generated samples are normally distributed. Success! BenchmarkingNow that we know that our algorithm works as expected, we should also test its performance. We will do this by generating a large number of samples repeatedly and timing how long this takes. We will do this using the rbenchmark package. I have already written a blog post about this package detailing why I find it to be the best benchmarking package available in R, so I will not go into much depth on how it should be used. We will test our function by generating one million samples and repeating this one hundred times for a total of one hundred million samples. 12345library(rbenchmark)bm &lt;- benchmark(\"Inverse Transform\" = inverseTransform(10^6), columns = c(\"test\", \"replications\", \"elapsed\"), replications = 100)bm $(document).ready( function () {$('#table0').DataTable();} ); testreplicationselapsed &lt;fct&gt;&lt;int&gt;&lt;dbl&gt; Inverse Transform1005.51 As you can see in the table above, the one hundred million samples were generated in just a few seconds, showing incredible efficiency. Next Steps: The Box-Muller TransformInverse transform sampling is an incredibly powerful tool. Even when dealing with troublesome distributions such as the normal distribution, by using appropriate approximations, we can still produce a working algorithm that generates samples with incredible speed. This method is so effective that, although implemented slightly differently, it is the exact procedure used by rnorm() to generate normally distributed samples in R. Really, we could end this series of posts here; we have already derived one of the most effective methods for generating normal random variables. But it feels like a bit of a hollow victory. For a start, the algorithm we produced isn’t really ours - it is heavily dependant on the qnorm() function. And even if we were to code up our own function to approximate the inverse normal CDF, it will always be just that, an approximation. There are many other methods of generating normal random variables. Many of these are exact methods, using no approximations. Others are capable of generating multi-dimensional normal random variables. Furthemore, a rare few can even perform better than the inverse transform sampling method that R uses. More importantly, by learning how these alternative methods work, we can discover new statistical concepts and see how they can be applied to solve real-world problems such as this. The next method we will look at is the Box-Muller transform, a method for generating two-dimensional standard normal random variables using an ingenious pair of transformations. We will learn how this method works and how to implement it in R in the next blog post. But for now, the approximate method that inverse transform sampling offers us will have to do.","link":"/generating-normal-random-variables-part-1/"},{"title":"Gibrat's Law: The Central Limit Theorem's Forgotten Twin","text":"The post was originally posted on Warwick Data Science Society‘s Research Blog. You can find the source at this link. Testing, testing. One, two, three. Can everyone hear me alright? Wonderful. Hello and welcome to the Warwick Data Science Society Research Blog. It’s so nice to see you. IntroductionTesting and TediosityTesting is boring. Everyone knows that they should test the solutions they produce, yet reality often fails to live up to this ideal. We check a few obvious things, run through the logic in our head, and convince ourselves that nothing could ever go wrong…and then it does just that. Sometimes this is okay. For small, individual projects, it’s not the end of the world if your code base comes collapsing down on you; a few hours of bodging and everything should be stable again (all be it with code now so messy that you’d want to consider cryongenics). For larger projects, and especially those of collaborative nature, this just won’t do. That is a long way to say, this post is a test, but hopefully not a boring one. To ensure that the testing of this site is performed thoroughly, I decided to write this post to ensure that everything is behaving as expected. I hope it can also act as a template for other keen researchers—whether they are currently a member of WDSS or merely interested in getting involved—to contribute their own work. The philosophy of this post is to write about a topic that is actually (well, hopefully) of some interest to people, and do so in a natural way. None of this testing through enumerating corner-cases; let’s use the site for what it’s made for in practice and share some interesting insights along the way. With that in mind, I wish to introduce to you: Gibrat’s law. Don’t be disheartened if you haven’t come across this term before,—most haven’t. This is likely because the rule is overshadowed by it’s far more infamous twin, the central limit theorem. If you’ve not come across this second term either, don’t fret. You may want to watch this short video by Khan Academy as a preliminary, but we’ll introduce both of these ideas either way. As a matter of fact, the Wikipedia article for Gibrat’s Law is only made up of a few paragraphs with the link to the law’s creator, Robert Gibrat, directing you to a yet non-existant page. We seem to be diving head first into the rabbit hole today. BackgroundIf you are already familiar with central limit theorem, you may wish to skip to the next section. The Central Limit TheoremWe’ll start with the central limit theorem. There are many ways of explaining or defining this phenomenon, each striking a subtle balance between statistical rigor and clarity of explanation. We will favor the latter, ensuring to remember that we should keep it too ourselves if we wish to avoid the disapproving gaze of the pure probability theorists. This simplified description goes as follows: Consider a sequence of independent random variables Take the sum of these values Regardless of whether the original variables were normally distributed, the distribution of this sum will tend towards a normal distribution as the number of variables increases To clarify this notion, let’s look an example. One of the simplest would be an experiment involving tossing multiple fair coins. We consider each coin flip to be an independent random variable taking value zero if the result is tails, and likewise one for heads. We can then say that the total number of heads is the sum of these random variables. It would follow from the central limit theorem that the total number of heads should therefore tend to a normal distribution as the number of total coin tosses gets large. Let’s simulate this experiment for different numbers of coins and see this process in action. Just as we expected, the more coins we toss, the closer the resulting distribution of heads resembles the classic bell shape of the normal distribution. Indeed, once we use around one hundred or more tosses, we can almost model the number of heads as a continuous variable. The Importance of the CLTIt’s all well and good that the theory works, but why should we care? The real power of the central limit theorem presents itself when we want to perform statistical analysis or tests on unfriendly distributions. The normal distribution has some incredibly delightful properties and has been studied in great extent, so working with it is often straightforward. On the other hand, there are many distributions which are not so cooperative, and that’s even assuming we know the distribution of whatever we are trying to model. Thankfully, the central limit theorem allows us to circumvent these issues by assuring us that as long as our process can be modeled as the sum of independent random variables (which is a surprisingly common property), we can approximate its distribution as normal and apply all of the standard techniques we know and love. For example, the number of visitors to a website in a given time period is unlikely to have an underlying normal distribution governing the process. This could make it difficult to analyze data related to this, however the central limit theorem offers a workaround. The rough conceptual notion behind this approach is to divide our time period up into smaller and smaller intervals and consider how many people visit the website in each of those. We can assume that these are reasonably independent and so our total visitor count becomes the sum of many independent random variables and so the central limit theorem holds. A small caveat of this specific result is that we require the number of visitors to the site in the time period to be reasonably large (&gt;20 visitors is typically fine), but as long as this holds, we can perform analysis just as if our data was normally distributed. We can even go one step further. Since translating or scaling a normal distribution does not change its normality, it is possible to approximate many situations using the standard normal distrubtion (mean 0, variance 1) by applying appropriate transformations. This makes our lives even simpler and offers some elegance in the process. From Sums to ProductsThis leads us nicely onto Gibrat’s law. In our rough definition of the central limit theorem above, we described taking numerous independent random variables and computing their sum. Gibrat’s law begins in a similar vein but goes on to consider the product of these values instead. Because of this alteration, we can no longer expect the aggregation to approach a normal distribution as the number of variables grows large. Instead we tend towards a related distribution—the log-normal distribution. As the name eludes to, a random variable following log-normal distribution can be defined by its logarithm being normally distributed. The converse of this framing is that whenever we take a normal random variable and take its exponential, the result will follow a log-normal distribution. Because of this, a log-normal random variable only takes strictly positive values with a density curve along the lines of the following. Just as with our coin-tossing experiment for the central limit theorem, we can validate our belief in Gibrat’s law using another simulation. We’ll introduce an explicit example of Gibrat’s law in the final section of the post and for now look an a more esoteric example. In particular, we will consider numerous independent random variables, uniformly-distributed on the interval $(0.5, 1)$. We then take the product of the first $n$ these variables and look at the probability distribution of this value. Using Gibrat’s law we would expect this product to approach a log-normal as $n$ becomes large, with a density tending in shape towards the curves shown above. A few simulations shows exactly that. The example shown here is, admittedly, rather abstract. Despite this, it is still simple to frame it in terms of a real world problem (though perhaps not one of upmost importance). For example, we could imagine the product of independent uniform random variables modeling the process of repeatedly cutting a piece of string at a randomly chosen point along its length and retaining the longer part. It is worth noting that just as there is no one normal distribution, but rather many of various shapes and sizes, parameterized by their mean ($\\mu$) and variance ($\\sigma^2$), the same is true for the log-normal distribution. In fact, we parameterize a log-normal distribution not by its own mean and variance, but by the mean and variance of the resulting normal distribution we obtain when taking logarithms. This relationship between the two distributions offers us much of the same power the central limit theorem displays for Gibrat’s law too. Yes, taking the product of the independent random variables in its definition doesn’t give us the normal distribution we so desire directly, but with a simple logarithmic transformation we can get there. Further, because taking the logarithm of a random variable is a simple transformation with some desirable properties, we are able to utilize many of the tools we have developed for manipulating and analyzing the normal distribution with log-normal distribution. This in turn makes Gibrat’s law an incredibly powerful tool for simplifying statistical analysis. One question remains though: when does it hold? What sort of scenarios can be modeled as a product of independent random variables? This brings us neatly to the final section of this post in which we will look into just that, and confirm Gibrat’s law in action. Verifying the LawSo far we’ve discussed some interesting ideas, but I think it’s time to pull things back from the land of statistical theory into the real world. Where would we expect to find Gibrat’s law in our lives? The key insight needed to answer this question is to understand the underlying mechanism of Gibrat’s law; it’s all above the multiplication of random variables. Specifically, we take the previous product and multiply it by some new random amount. Another way of looking at this is that these random variables represent the growth rate of some quantity. Now, this is not a fixed growth rate as you would have say with interest rates or nuclear decay, but rather a stochastic growth rate. To simplify this notion, all we are looking for is systems that evolve by a growth rate proportional to their current size with some added variation captured by the random nature of the variables. An example of such a system (and incidentally the original inspiration for the law) is that of modeling the growth of a firm. In a simple model, we can ignore the impacts of overarching market changes and catastrophic economic events and instead suggest that the growth of a company is roughly proportional to its current size with some added noise. Obviously, this model is primitive and its assumptions do not hold exactly, but it is close enough to believe that the distribution of the size of firms would indeed be log-normally distributed. Another example could be the distribution of the population sizes for various countries or cities. There are clear troubles with modeling such a system using Gibrat’s law, but overall it is not unreasonable to suggest that the population growth of a city is largely dependent on its current size, with some added stochasticity. Don’t take my word for it though! Instead, let’s quickly gather some data to verify this result for ourselves. Sourcing DataAfter some searching, I decided that the best data source to investigate the validity of Gibrat’s law for use with population data is this Wikipedia article containing the populations of all sovereign states and dependencies. I then scraped relevant columns from the main table of this page, resulting in a dataset for which ten randomly chosen rows are shown below. $(document).ready( function () {$('#table0').DataTable();} ); City Population 5 Manchester 503127 24 Southampton 236882 28 York 198051 31 Chelmsford 168310 32 Dundee 153990 51 Bath 88859 53 Hereford 58896 56 Stirling 34790 57 Lichfield 32219 66 City of London 7375 Visualizing the ResultsWe can now plot a histogram of the populations for these cities. On top of this, we overlay a log-normal distribution with parameters chosen to best fit (using maximum likelihood estimation) to see how well the densities match. Now, before you say anything, I know. It’s not perfect. But, I hope we can agree that there is definitely something there. It’s also quite easy to explain why the match isn’t exact. For a start, we only had data for 69 cities so it’s no surprise that the histogram is so jagged. On top of this, city populations is one of the more difficult applications of Gibrat’s law due to the many factors that influence their size that aim to violate the assumptions of the rule. Despite these shortcomings, it is clear that Gibrat’s law has value to it either as a conceptual or statistical tool. It should certainly be the case that the validity of the law in any particular scenario should be tested thoroughly before resting too heavy on the results, but as a tool to guide you in the right direction, it is is invaluable. As eluded to above, the example of population data is more difficult than most applications of Gibrat’s law due to the numerous and influential externalities. I didn’t want to shy away from this case, especially when it is an example that has clear real-world implications for modeling. It therefore follows that in many more simplistic and controlled cases, Gibrat’s law shines even brighter. For example, it has been of great benefit in my work at AstraZeneca in forming suitable priors for energy distributions in statistical models. Without knowledge of this law it may have taken me more time to discover that the log-normal distribution was a natural and accurate model for these quanta. If anything, this post does not offer anything of immediate practical use. That is not to say however that there isn’t an important message. That is, remember Gibrat’s law—it’s there more than you think, and your awareness of it is vital for optimum efficiency in your statistical work. I hope you found this post of some insight, and I look forward to sharing more ideas and research as this blog developments.","link":"/gibrats-law/"},{"title":"Gloopy Violin Plots","text":"The following packages will be required for this post: 12345library(gganimate) # animationlibrary(ggplot2) # visualisationlibrary(dplyr) # manipulating datalibrary(readr) # importing datalibrary(tidyr) # tidying data The Importance of the Fourth Dimension in Data VisualisationThe fourth dimension is often overlooked in data visualisation applications. There are some very understandable reasons for this. To begin, such plots are of no use in static mediums such as scientific journals, research papers or printed blog posts. Furthermore, they often require more effort to produce than single frame plots, perhaps requiring additional tools and packages or, in the worst, the manual stitching together of individual plots. Despite its drawbacks, the use of time in the production of data visualisations can be very effective in expressing the meaning of a dataset in a natural and distinctly human way. Consider this example: if I were to give you pictures taken from a fixed camera at secondly intervals of a car driving down a road, you would most likely struggle to build a mental image of how far that car was travelling. This is strange because you have all the necessary data. All you are left to do is interpolate for the moments that you missed and then use your understanding of the world to have a guess at the speed of the car. This would be a surprisingly difficult task, yet if I gave you a video of the moving car, although you may not be completely accurate, you’d be able to make a much better judgement of the speed the car was travelling at. Enter: Gloopy Violin PlotI intend to use this principle to produce a natural visualisation of the progress of same-sex marriage laws in the United States using this dataset by the Pew Research Centre. A reduced view of the dataset looks like this, $(document).ready( function () {$('#table0').DataTable();} ); State199819992000 &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt; Alabama Statutory Ban Statutory Ban Statutory Ban Alaska Constitutional BanConstitutional BanConstitutional Ban Arizona Statutory Ban Statutory Ban Statutory Ban Arkansas Statutory Ban Statutory Ban Statutory Ban California Statutory Ban Statutory Ban Statutory Ban Colorado No Law No Law Statutory Ban ConnecticutStatutory Ban Statutory Ban Statutory Ban Delaware Statutory Ban Statutory Ban Statutory Ban Florida Statutory Ban Statutory Ban Statutory Ban Georgia Statutory Ban Statutory Ban Statutory Ban The typical way of representing this data may be a collection of bar plots faceted by year or, if you allow yourself to treat the nominal variable representing the type of law as ordinal, a ridgeline plot (previously known as a joy-plot) would be a good choice. The problem with these two methods though is that it is very hard to conceptualise the speed of change when we are only given single snapshots in time rather than the whole picture just like with the car example above. Instead I wanted to produce an animated violin plot to represent this data in a way that would be easier to interpret the message of. The core of this visualisation is produced as follows. 123456789101112131415161718192021222324252627282930313233343536# load data from a csv on GitHubdl &lt;- \"https://raw.githubusercontent.com/zonination/samesmarriage/master/ssm.csv\"df &lt;- read_csv(dl)# store the four types of law from most opposed to leastlaws &lt;- c(\"Constitutional Ban\", \"Statutory Ban\", \"No Law\", \"Legal\")df %&gt;% # tidy the data frame so it is in a form ggplot can work with gather(key = \"year\", value = \"law\", '1995':'2015') %&gt;% # factor the law types using the order stored in the laws variable mutate(law = factor(law, levels = laws, ordered = TRUE)) %&gt;% # encode the laws as numerical values so they can be used with a violin plot mutate(law.encode = as.numeric(law)) %&gt;% # add dummy years to add pause before looping † rbind(filter(., year == \"2015\") %&gt;% mutate(year = \"2015 \")) %&gt;% rbind(filter(., year == \"2015\") %&gt;% mutate(year = \"2015 \")) %&gt;% # generate static plot ggplot(aes(x = 0, y = law.encode)) + geom_violin(bw = .3, trim = FALSE, scale = \"area\") + scale_x_discrete(\"Proportion of States\") + scale_y_continuous(\"\", breaks = 1:4, labels = laws) + coord_flip(xlim = c(-.5,.5), ylim = c(0,5)) + labs(title = \"Same-Sex Marriage Laws in the USA\", # remove \"dummy\" from dummy year names subtitle = paste0(\"Year: \", \"{closest_state}\"), caption = \"Source: Pew Research Centre\") + theme_minimal() + theme(panel.grid.minor.x = element_blank()) + # animate using gganimate transition_states(year, transition_length = 1, state_length = 0) + ease_aes('sine-in-out') -&gt; p# animate with custom parametersanimate(p, nframes = 300, fps = 15, width = 720, height = 540) † Sadly, at of the time of writing this post, gganimate does not have a simple way to add a pause between loops. This is the best solution I could find to overcome this limitation. UPDATE: gganimate now has the ability to add pauses at both the beginning and end of the loop. See the documentation for the animate() function to learn more. This graphic shows us how much fluctuation there was on same-sex marriage law in the USA throughout the early 2000’s and almost makes clear the sudden impact on national legislation when the Supreme Court legalized same-sex marriage on June 26, 2015.","link":"/gloopy-violin-plots/"},{"title":"Efficiently Solving a Google Coding Interview Question Using Pure Mathematics","text":"1%precision 0 &apos;%.0f&apos;The Knight’s DiallerRecently, I have become more and more interested in the sort of coding problems that are typically used in job interviews for developer and software engineering roles. In particular, whilst browsing Glassdoor, I came across an interview question previously used by Google called ‘The Knight’s Dialler’. The Problem StatementThe problem goes as follows: You place a knight chess piece on one of the 10 digits of a standard mobile phone keypad (shown below). You then move it arbitrarily in the typical ‘L’-shaped patterns to any other number on the keypad. For example, if you were on the 1 key, you would be able to move to either the 6 or 8 key. All keys except the 5 leave you with a possible new key to jump to. Suppose that as you make the knight hop around, every new key it lands on is dialled (i.e. we do not dial the key it starts on), generating a sequence of integers. The question is: given a specific starting point on the keypad and a fixed number of moves, how many possible distinct numbers can be dialled?This is a great interview question! It is simple to explain; it is very open, allowing many different approaches to the problem; and – although the problem requires some thought to solve – the actual coded solutions are rather short. A Typical SolutionAfter looking at many of the submitted solutions to this problem on Glassdoor and various coding-puzzle forums, I noticed that a common approach to this involved some sort of recursion. The fact that most people jump to this method makes complete sense; the problem has a very recursive nature to it. If, for a given starting point, you know the number of possible sequences that you can dial in $n-1$ steps, then all you have to do is enumerate these with the possible choices for the $n$th step and you have your solution. The issue with this sort of method is that recursion can often lead to exponential-time algorithms. That is, algorithms whose runtime varies in-line with the exponential of one of the inputs (in this case, the number of allowed moves). This means that for large inputs, your algorithm will be unusable and so we should avoid this scenario if possible. Furthermore, unless you are careful with the details of your implementation, it is easy for the required amount of storage to grow with input size, another trait which is undesirable in algorithms. There are some approaches that avoid these issues. One of which is a dynamic programming approach, shown here. This runs in linear-time and uses constant memory, making it a strong solution. My only issue with this method is that its validity didn’t immediately jump out to me. The internal logic is obscured by a series of nested loops and confusing temporary variables. These inefficient or confusing solutions are what made the interview question so interesting to me. Especially because, due to my mathematical background, I almost instantly saw how it could be solved using a solution that is both linear and constant in memory, as with the DP approach. What allowed me to quickly leap to this efficient solution was my knowledge of elementary linear algebra and combinatorics. In fact, this problem translates perfectly into these two fields, leading to a very refined and simplistic solution. A Mathematical ApproachI will explain the thought process behind my solution soon, but will begin by revealing the code (written in Python) so it can be used as a reference later. The Final Code, First{% codeblock lang:python %} import numpy as np def count_numbers(start = 0, moves = 0): # adjacency matrix for the implied network adj_mtrx = np.matrix([ [0,0,0,0,1,0,1,0,0,0], [0,0,0,0,0,0,1,0,1,0], [0,0,0,0,0,0,0,1,0,1], [0,0,0,0,1,0,0,0,1,0], [1,0,0,1,0,0,0,0,0,1], [0,0,0,0,0,0,0,0,0,0], [1,1,0,0,0,0,0,1,0,0], [0,0,1,0,0,0,1,0,0,0], [0,1,0,1,0,0,0,0,0,0], [0,0,1,0,1,0,0,0,0,0] ]) # eigen-decomposition e, v = np.linalg.eig(adj_mtrx) # compute powers of diagonals e_power = np.diag(e ** moves) # compute number of full-length paths all_paths = np.matmul(np.matmul(v, e_power), np.transpose(v)) # compute number of paths starting from the initial key num_paths = np.matmul(all_paths, [[1]] * 10)[start] # convert to scalar and return return(np.asscalar(num_paths)) {% endcodeblock %} Short and sweet, but does it actually work? Let’s test it on a few cases. First, we know that if we start the knight on the 5 key then we should never be able to move, so can’t dial any digits. {% codeblock lang:python %} count_numbers(start = 5, moves = 42) {% endcodeblock %} 0Zero! Just as we expected. For one more test, we can look at the number of sequences that we can dial in a small number of moves. 1count_numbers(start = 1, moves = 3) 10This gives 10 possible sequences. Getting out pen and paper will show us that this is in fact the correct amount. The possible sequences are: 1 -&gt; 6 -&gt; 1 -&gt; 6 1 -&gt; 6 -&gt; 1 -&gt; 8 1 -&gt; 6 -&gt; 7 -&gt; 2 1 -&gt; 6 -&gt; 7 -&gt; 6 1 -&gt; 6 -&gt; 0 -&gt; 6 1 -&gt; 6 -&gt; 0 -&gt; 2 1 -&gt; 8 -&gt; 1 -&gt; 8 1 -&gt; 8 -&gt; 1 -&gt; 6 1 -&gt; 8 -&gt; 2 -&gt; 4 1 -&gt; 8 -&gt; 2 -&gt; 8 Lastly, just to satiate curiosity, we can run the function for a large number of moves. {% codeblock lang:python %} print(count_numbers(start = 4, moves = 90)) {% endcodeblock %} 2.6529416969833432e+32As we can see, these numbers grow rapidly with the number of allowed moves. For example, the value computed above is larger than the estimated number of bacterial cells on Earth – so do forgive me if I don’t check this one by hand. To further validate my method, I compared my results to the solution given by the original Google interviewer who introduced the question and we obtained the exact same values. My Thought ProcessSo how does this method work? And, more importantly, how did I come up with this? My initial insight towards the problem was that the movements of the knight can be abstracted away to the connections between the nodes of a network graph. Abstraction is a very common technique in mathematical problem-solving, in which you remove anything but the bare mathematical structures. The keys can be replaced by labelled nodes and the possible moves of the knight can be replaced by arcs. For example, since the knight can move from the 1 key to the the 6 key, we would have an arc between nodes 1 and 6. We would not have an arc between 1 and 2 however, since this is not a valid move for the knight. This gives us the following graph.This graph completely captures all of the information about the problem. The only issue is, this picture is not currently in a computer readable form. There are many ways of storing graphs but the most common way in mathematics is to use an adjacency matrix. This is an $n\\times n$ matrix (where $n$ is the number of vertices of the graph) where the cell in the $i$th row and $j$th column is 1 if nodes $i$ and $j$ are connected by an arc and 0 otherwise. Mathematicians favour this representation since it allows us to use all of the techniques we have developed in linear algebra – as we will see later in this post. The adjacency matrix for the above graph (which we will call $A$) will look like this. $$A =\\begin{pmatrix}0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{pmatrix}$$ As you can see, we have a 1 at the intersection of the 0th row and 4th column since nodes 0 and 4 are connected. Also it is worth noting that since our graph is undirected – that is whenever you can go from one node to another, you can always come back in the opposite direction – our adjacency matrix is symmetric ($A^T=A$ in linear-algebraic terms). If you look at the final code again, you’ll find that the first line is simply just defining this matrix. Now that we have our graph in a form both friendly for computers and mathematicians alike, we can use a clever theorem from combinatorics to extract some useful information from our matrix. In particular, we use the result that the element in the $i$th row and $j$th column of the matrix $A^k$, for some natural number $k$, gives us the number of distinct paths from node $i$ to node $j$ of length $k$. That’s a really interesting result, but what’s its relevance to us? It may not seem clear at first, but in fact, knowing this gives the whole game away, providing us with an instant route to the problem’s solution. This is because every possible sequence of dialled numbers corresponds exactly with one path through the network. And so, if we know how many paths there are from any starting point to any end point – which we obtain from our exponentiated adjacency matrix – we can simply sum up the values of interest to get our result. Specifically, since we want to know the number of paths from a given starting point, we will want to sum over all of the columns of $A^k$ for a fixed row. If we recall how we compute matrix multiplications, it is not too hard to see that in order to get the sum across the $i$th row, we simply multiply $A^k$ by a column vector of length 10 containing only the value 1 and then extract the $i$th component of the resulting column vector. If you are sceptical of this, give it a go with a small arbitrary matrix to convince yourself. So that’s it, we’re done! This is a completely valid solution to the problem. It is incredibly simple; only requiring 3 lines of code - one of which is just defining the matrix. This solution is not perfect however. This approach is limited by the slow computation of $A^k$. Multiplying $10\\times 10$ matrices is not a quick process and doing it $k-1$ times for large $k$ could take a long time. Using some knowledge of linear algebra, however, it is possible to speed up this method up with only a few more lines of code necessary. Turbo-charging Our SolutionThe next idea that came to me was to use a technique known as eigen-decomposition to make computing the powers of $A$ more efficient. Recall that since our graph was undirected, our adjacency matrix $A$ will be symmetric. A well-known result in linear algebra lets us take advantage of this fact. It says that any symmetric matrix, such as $A$, can be decomposed in the following way. $$A = P^TDP$$ where $D$ is a diagonal matrix – it is zero everywhere but the main diagonal – and $P$ is orthogonal – that is that $P^TP=PP^T=I$, where $I$ is the identity matrix. The proof of this is quite long-winded and mundane, using induction on the size of the matrix, but the result is incredibly powerful. One use of this is in computing powers of $A$. We simply have to observe that $$\\begin{align}A^k &amp;= \\underbrace{A\\cdot A\\cdot\\ldots\\cdot A}{k\\text{-times}} \\\\&amp;= \\underbrace{(P^TDP) \\cdot (P^TDP)\\cdot\\ldots\\cdot (P^TDP)}{k\\text{-times}} \\\\&amp;= P^T \\cdot \\underbrace{(DPP^T) \\cdot (DPP^T)\\cdot\\ldots\\cdot (DPP^T)}{(k-1)\\text{-times}}\\cdot DP \\\\&amp;= P^T \\cdot \\underbrace{D \\cdot D\\cdot\\ldots\\cdot D}{(k-1)\\text{-times}}\\cdot DP \\\\&amp;= P^TD^kP\\end{align}$$ since all of the $P$’s and $P^T$’s on the inside of the product cancel out to give $I$ by orthogonality. We have now reduced the problem of exponentiating $A$ to exponentiating a diagonal matrix $D$. This may feel like we’re going in circles, but that is not the case. Exponentiating a diagonal matrix – unlike a general matrix – is incredibly simple. All you have to do is exponent the individual elements, and since the majority of these are zero, this is very easy to do. After computing $D^k$ all that is left to do is compute $A^k=P^TD^kP$ – a computationally easy task – and carry on as described in the last section. With this knowledge in hand, it shouldn’t be too hard to piece together how the lines of the code above correspond to the steps of this method. Even with this new time-saving addition, our code still requires only 6 lines. I am yet to see any purely computer-science approach to this problem that is as short, simplistic, and elegant as this. Because of the way eigen-decomposition behaves in numpy, the v that is returned to us is equivalent to $P^T$ above. Time-complexity AnalysisBefore we wrap up, we should take a quick theoretical look at how quickly our algorithm runs. We first look at the eigen-decomposition. In practice, calculating eigenvalues/vectors of an $n\\times n$ matrix requires $O(n^3)$ time. There are algorithms such as Coppersmith and Winograd which compute these in $O(n^\\omega)$ with $2&lt; \\omega &lt; 3$, but due to the complexity of implementing code for these for little computational gain in most circumstances, they are rarely used. Since our matrix is of fixed size however, this part of the code is simply $O(1)$ time. The only other part of the code worth considering is the computation of $D^k$. This will involve exponentiating 10 numbers $k$ times each and so will run in $O(k)$ time. Hence our algorithm is linear, exactly the same as the dynamic programming approach but, I would argue, far more beautiful.","link":"/google-interview-question/"},{"title":"A Statistican's Guide to Playing Darts","text":"IntroductionDarts is a game of incredible skill and precision. If however - like me - you lack the necessary coordination to shine at the sport, luck will have to suffice. Then, if things happen to go your way, all you are left to do is claim deliberateness. In fact, there is a truth to this for all players, newcomers and seasoned professionals alike. All abilities will have some natural variation between each throw giving rise to a probability distribution reflecting where the dart landed relative to where it was aimed. After all, no one is guaranteed to hit exactly where they wanted. This raises the question, if you want to maximise your score in a given throw, where is it best to aim? We start with the extremes. If you were some sort of superhuman darts-ninja guaranteed to hit exactly where you aim, there is no doubt that you would simply aim for the treble 20 on every throw. Ultimately, it’s the highest possible score you can get on the board and since you’re certain to hit it if you try, it would be foolish to aim anywhere else. On the contrary, suppose you are a complete darts rookie, playing with your off-hand during a magnitude 7.6 earthquake. You’d be quite chuffed if you even managed to get it on the board, never mind what score you get. Since you are so unlikely to do even this, to maximise your score you need to have the highest chance of just getting on the board. This will be achieved by aiming at the centre. We now have our two extremes, the perfect player should always aim for treble 20 whereas a hopeless player should aim for the centre. But this still leaves us to ask what the best strategy is for any player in between these edge-cases. Modelling the ProblemThe DartboardWe will be looking at the regulation dartboard, envisioned by Brian Gramlin in 1896. The numbers are ordered in such a way they they punish inaccuracy; if you aim for the highest score - 20 - and miss, you could easily end up with the lowest - 1. The standard radii for the rings according the British Darts Organisation are 6.4 mm, 16.0 mm, 99.0 mm, 107.0 mm, 162.0 mm and 170.0 mm with the radius of the whole board being 225 mm. These are the dimensions we’ll use for our model. Throw DistributionWe will need to model the random nature of a dart throw by some sort of probability distribution. But what sort of distribution might we expect to see? A promising candidate would be modelling the horizontal and vertical displacement of the dart’s actual target from the intended one by two independent and identically distributed normal random variables. The normal distribution is a bell shaped curve which is often used for modelling errors so is highly suitable for this purpose. If we set the parameter $\\mu$ to zero then the joint density function of the two random variables will be highest at the point where the player aims and then tail off as you get further away, equally so in all directions. This is the exact behaviour we would expect to see in the real world; no matter what your ability is at darts you’ll always be more likely to hit closer to where you were aiming than further. The standard deviation parameter, $\\sigma$, will control how spread out the dart is likely to be from where it is intended. A good player would have a small standard deviation whereas a bad player would have a larger one resulting in a wider and flatter density curve. This is the same as saying that a good player is more likely to hit near to where they were aiming whilst a bad player has a higher likelihood to be further away. This model is not a perfect reflection of reality. However, what we lose in accuracy we gain in simplicity which will come important later when we start using this model for computations. A handful of critiques and improvements are: There is no reason to believe that the standard deviation for the horizontal and vertical errors should be the same. It is reasonable to imagine that they would be similar but since you are having to counteract the affect of gravity when considering the vertical accuracy of your shot, this added layer of complexity may make the standard deviation slightly higher. The normal distribution is perfectly symmetrical but it is most likely not the case that this be true for the distribution of errors of a real player. The left or right-handedness of the player may give the distribution for the horizontal error a skew in a particular direction and the influence of gravity may do the same for the vertical component. Lastly, our assumption of the independence of the vertical and horizontal errors may not be justified. Perhaps if you mess up your horizontal aim, say due to shaky hand, you may be more likely to also mess up the vertical. Despite these possible discrepancies, we continue in the belief that our basic model is as accurate a reflection of real life as we need for any non-pathological cases. Forming a StrategyCalculating ScoresTo begin, we will need a function which can convert a pair of Cartesian coordinates representing the location a dart lands to the respective score at that point. Due to the pattern-less nature of the numbers’ order we will have to do this directly. 1234567891011121314151617181920212223242526272829303132333435363738calcScore &lt;- function(x, y) { # concert to polar coordinates r &lt;- sqrt(x^2 + y^2) if (r == 0) { theta &lt;- 0 } else { theta &lt;- atan2(y, x) } # transform so zero is on the boundary of 20 and 1 # and the angle increases as we go clockwise phi &lt;- pi / 2 - theta - pi / 20 # find what region the angle is in; # e.g. region 1 is 0-18°, region 2 is 18°-36° etc. region &lt;- findInterval(phi %% (2 * pi), seq(0, by = pi/10, length.out = 20)) # find score for that region regionScore &lt;- c(1, 18, 4, 13, 6, 10, 15, 2, 17, 3, 19, 7, 16, 8, 11, 14, 9, 12, 5, 20)[region] # find which ring the dart is in ring &lt;- findInterval(r, c(0, 6.4, 16, 99, 107, 162, 170, Inf)) # calculate final score score &lt;- switch(ring, 50, 25, regionScore, 3 * regionScore, regionScore, 2 * regionScore, 0) return(score)} The best way to visualise this data is using a heat-map. Calculating ExpectationsWe can use this score function alongside the density function for the 2D normal distribution to find the expected score attained by aiming at any location on the board for a given standard deviation. To find this expected value we simply centre our normal distribution at the point that is aimed at, then integrate the product of the density function and the score function over the entire board. We then sweep the distribution over the whole board to get the expected value at each point. In a perfect world we would compute this integral directly and to the highest precision our computer can store. In reality this process is extremely time consuming. In my preparation for this post, I developed such a method and even with the use of time-saving short-cuts and tight coding the program took over an hour to run for some larger standard deviations. Since the end goal is to compute this for many standard deviations, this is not a feasible approach. Instead, I got out my pen and paper and went about finding an approximation method which would give only a negligibly different result to the true value but would be much more computable. I decided in the end to split dartboard up into a 361x361 grid (essentially a 1mm x 1mm grid). There was nothing special mathematically about these exact dimensions - they just worked well with the code - and after some long-winded analysis and a touch of brute force computations, I concluded that for standard deviations of at most 110 this method would be accurate to the integer, which for this use is all we need. This is quite a weak bound so in reality the method will be a lot closer to the true solution but in the worst possible case we know that it is at least that good. In practice, if your standard deviation is any more than 100mm then there is no doubt that you should be aiming for the centre of the board. We can package up this method in a neat little function so we can easily reuse it for any standard deviation. 12345678910111213141516171819202122232425262728293031323334expectationMtrx &lt;- function(sd) { # values used to approximate integral values &lt;- -180:180 # compute values of 2D normal density at approximating points density_mtrx &lt;- outer(values, values, function(x, y){ dnorm(x, 0, sd) * dnorm(y, 0, sd) }) # compute scores at approximating points score_mtrx &lt;- outer(values, values, calcScore) # pad with zeros on all sides zero_mtrx &lt;- matrix(0, nrow = 361, ncol = 361) score_mtrx &lt;- direct.sum(direct.sum(zero_mtrx, score_mtrx), zero_mtrx) # matrix to hold expected values expectationMatrix &lt;- matrix(0, nrow = 361, ncol = 361) # sweep distribution matrix over the board for (i in 1:361) { for (j in 1:361) { # which subest of the score matrix to multiply with the density score_mtrx.subset &lt;- score_mtrx[i + 361 + values, j + 361 + values] # contibution to expected value at approximating point contrib &lt;- sum(score_mtrx.subset * density_mtrx) expectationMatrix[i, j] &lt;- contrib } } return(expectationMatrix)} Visualising the ExpectationsStatic visualisationsWe can now call this function for any standard deviation up to 110 and get back a matrix giving the expected score we’d achieve for aiming at any location on the data board with that level of accuracy. Let’s generate these matrices for a selection of standard deviations and visualise the results. Animated visualisationsThese plots are interesting by themselves but don’t tell the whole story. What we really need is an animation to show how the heat-map changes as the standard deviation increases, and more importantly, how this affects the optimum location to aim and the expected score corresponding to it. We produce such an animation using the gganimate package. As we can see, the optimum location to aim for a low standard deviation is the treble 20, just as we expected. Once the standard deviation is above 16mm though, it becomes more effective to aim at the treble 19. As the standard deviation increases further, the optimum target sweeps inwards towards the bullseye. By a standard deviation of 100mm we are almost at the centre. If we continued increasing the standard deviation, we would find that the optimum target slowly approaches the exact centre of the board. Tracking the optimumLet’s now forget the expected scores themselves and consider just the location of the optimum target for each standard deviation. We can map its path to get the following plot. This follows the behaviour that we described before, with the exact jump from treble 20 to treble 19 occurring at a standard deviation of 17mm. It can also be seen that the approach to the center slows as the standard deviation increases. Tracking the optimum scoreIt is now sensible to ask what the optimum obtainable expected score is for each standard deviation. We can look at this using the following visualisation It it evident from this plot, that the game of darts has a definite ‘skill elbow’. For small standard deviations, the expected score is extremely high but as soon as you start increasing this, the score drops off massively until it begins to stabilise at around 20mm. Then as you increase standard deviation more the expected optimum score only slightly reduces and will eventually tail off to zero. This shows that darts is a game where the difference between a terrible player and a mediocre player is not that severe, but the difference between that same mediocre player and a good player manifests itself savagely, creating a completely one-sided game. Estimating Your Standard DeviationThis analysis is all well and good but without a way to calculate your own standard deviation it has no use in practice. I have therefore developed a test to estimate your standard deviation. This involves throwing 100 darts aimed exactly at the centre of the bullseye and recording how many of them lie within the outer bull ring (either the bullseye itself or the ring around it). It can be shown using an appropriate statistical transformation that the radius squared of two independent normal variables with mean zero and equal standard deviation $\\sigma$ is distributed exponentially with rate parameter $\\frac{1}{2\\sigma^2}$. Using this, we can calculate for any standard deviation the probability that a throw aimed at the centre will land within the outer bull. We can then use this probability as a parameter for a binomial random variable representing the number of times we land inside the outer bull in 100 throws. By reversing this process, first estimating the binomial probability parameter from our sample and then using the cumulative density function of the exponential function, we can estimate the standard deviation for any given number of outer bull hits. If all of that seemed very complicated, don’t worry. Understanding the statistics behind where the standard deviations come from is not necessary for using the outcome of it in practice. The results of the above method can simply be read off from the following table. $(document).ready( function () {$('#table0').DataTable();} ); hitssdxyz &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt;&lt;dbl&gt; 100 0.0 0 10260.0 99 5.3 0 10258.0 98 5.7 0 10252.5 97 6.0 0 10247.2 96 6.3 0 10342.9 95 6.5 0 10339.5 94 6.7 0 10336.6 93 6.9 0 10333.9 92 7.1 -1 10331.6 91 7.3 -1 10329.5 90 7.5 -1 10327.5 89 7.6 -1 10425.8 88 7.8 -1 10424.3 87 7.9 -1 10522.9 86 8.1 -2 10521.7 85 8.2 -2 10620.6 84 8.4-35-10019.8 83 8.5-36-10019.0 82 8.6-36-10018.4 81 8.8-37-10117.8 80 8.9-37-10117.3 79 9.1-38-10116.9 78 9.2-38-10116.5 77 9.3-39-10016.1 76 9.5-39-10015.8 75 9.6-40 -9915.6 74 9.7-41 -9715.3 73 9.9-42 -9615.1 7210.0-43 -9414.9 7110.2-44 -9214.7 ............... 30 18.9-20-811.8 29 19.3-19-811.8 28 19.7-18-711.7 27 20.2-17-711.7 26 20.6-16-611.6 25 21.1-16-611.5 24 21.6-15-611.5 23 22.1-14-511.4 22 22.7-14-511.3 21 23.3-14-511.3 20 24.0-13-511.2 19 24.6-13-511.1 18 25.4-13-411.0 17 26.2-12-411.0 16 27.1-12-410.9 15 28.1-12-410.8 14 29.1-12-410.7 13 30.3-11-410.6 12 31.6-11-410.6 11 33.1-11-410.5 10 34.9-11-310.4 9 36.8-11-310.3 8 39.2-11-310.2 7 42.0-11-310.2 6 45.5-11-310.1 5 50.0-11-310.0 4 56.0-11-3 9.9 3 64.8-11-3 9.8 2 79.6-11-3 9.7 1112.9-11-3 9.6 If 100 throws seems like too many to bother with, you will get a similiar - though less precise - result by taking 25 or 50 throws and then multplying your number of hits by 4 or 2 respectively to simulate 100 throws. Recalling the plot of optimum expected score vs. standard deviation, we saw that you need a standard deviation of less than around 20mm to pass the skill elbow and become a good darts player. From the table above, we see that for this to be the case, you would be expected to hit the outer bull at least 28 times out of the 100 hits. Do this hold for you?","link":"/guide-to-darts/"},{"title":"A Statistican's Guide to Love","text":"The Python code used for simulation and visualisation in this post can be found in the source notebook here. IntroductionNo-one ever said that finding true love was easy. And though for the lucky few, Valentine’s day can be a time filled with love and affection, for many people it can just be a harsh reminder of this fact (such a pessimist, I know). Perhaps, though, statistics can lend us a hand. It may seem that the world of love and romance lies far from the realms of probabilistic reasoning and yet, surprisingly, the field might have some useful ideas to contribute. Specifically, statistics is capable (under a few assumptions—which we’ll get to later) of offering a solution to the question: “How do I find the one?”. What makes this question so difficult to answer is that we are having to handle uncertainty. Until we end a relationship and move on, it’s impossible (at least whilst retaining morals) to see if there could be something better with someone else. In most cases, we don’t stress about this and simply settle for someone that makes us happy; blocking out any thoughts that there could be someone better. In pursuit of ‘the one’ however, this just won’t do. Thankfully, this inherent uncertainty is a challenge that statistics is well-equipped to tackle. Modeling the ProblemIf we want to solve this problem using statistics, we’ll have to make some assumptions. Now, obviously, the more assumptions we implement, the less practical our statistical model becomes for actual match-making. But frankly, if you’re getting your dating advice from a mathematician, I think you’re far beyond the point of caring about real-world practicalities. Our main assumption is that it is possible to estimate how many people you would be able to date in your life if you were to never settle down. We will call this number $n$. In practice, it may be difficult or even impossible to come up with this number. By sacrificing some rigour, however, we can be happy in simply accepting a best guess. We also assume that these $n$ people have an objective ranking so that there is a clear ‘best’ person—the one. We then suppose that we are offered the chance to date each one of these $n$ people one at a time. After we get to know them, we make a choice: settle or move on. If we move on then there is no forgiveness; we can never return to that same person again. If we get to the $n$th person then we have no choice but to spend the rest of lives with them, for better or worse. Although these assumptions may not be indicative of real life—though a world where we never have to think about rejection doesn’t sound too bad—they are necessary to frame the problem in statistical terms. With the model out of the way, we are simply left to answer the question: what strategy will maximise our chances of settling with the best person? How can we find the one? The SolutionIt turns out that the optimal strategy is surprisingly simple. It goes as follows: Take your best guess at $n$, the number of people you could possibly date in your life if you were never to settle down Divide $n$ by $e$ (known as Euler’s number and equal to roughly $2.718$) and round the result to the nearest whole number. Call this number $k$ Date $k$ people, rejecting each one of them, keeping track of the best Continue to date the other $n-k$ people, settling only if you find someone better than everyone you’ve seen so far (or you reach the $n$th person) Admittedly, it’s a bit of a cruel strategy; you have to feel some remorse for the $k$ people rejected without a fighting chance. As the saying goes though, you’ve got to break a few hearts to make an omelette (well, it went something like that at least). Remarkably, it can be proved (and we will do so at the end of this post) that this is the unique optimal strategy that will maximise your chance of finding the one. Perhaps a better hypothetical talking point than an actually strategy for love but at least now you know. Speed DatingBut why should you trust that this is indeed the best strategy? Sure, I’ll provide a statistical proof at the end of this post but I’ve no doubt that it won’t be everyone’s cup of tea. Instead, I plan to prove this fact to you using some simulated speed dating. We look at values of $n$ ranging from 1 to 100, enough to verify my claim, and consider each possible cutoff $k$ from 0 (settling with the first person we meet) to $n-1$ (holding out for the last person). For each pair of $n$ and $k$ we will then simulate one million sets of $n$ people and try out the strategy for that specific $k$. We can then average over these one million simulations to see what proportion of the time we ended up with the one. In all, this equates to around $5.2$ billion dates. It takes my computer an hour or two to run all of these simulations, averaging around $700,000$ dates per second—a pace even the most decisive speed dater would struggle to match. We then produce a heat map showing the likelihood of finding the one for each value of $k$ (rescaled so that the highest probability becomes one). This leaves us with a plot like this. I’ve also included two lines. The purple line line tracks which value of $k$ was the best of the $n$ available based on our simulations. The black meanwhile shows the line $k=\\frac{n}{e}$—the trend we’d expect the purple line to follow if the procedure above was indeed optimal. The fit isn’t perfect but that is expected with simulation; even with one million iterations there will still be a substantial amount of random noise to contend with. The important point is that the simulated line typically is very close to our predicted black line, offering strong evidence that the statement is indeed correct. In general, the noise introduced by simulation is worse for large $n$. On the other hand, the true validity of the optimal method improves as $n$ increases. For small $n$ (say, single digit values) using a cutoff of $\\frac{n}{e}$ will perform well but will not quite be optimal. If your value of $n$ falls into this category then you are better off using the table below instead. It is also reasonable to ask the chances be of finding the one if we were to follow this strategy. It turns out that Euler’s number, $e$, makes another appearance here as the probability of ending up with the best match approaches $\\frac{1}{e}$ or about $0.368$ as $n$ grows. Not terrible odds if you ask me. A CompromisePerhaps, though, you simply don’t believe in the one and would rather just maximise the expected value of the person you settle down with instead of aiming for nothing less than the best. If this is the case then you’re in luck; a small modification to the above method can solve this problem too. In this scenario, we will assign each person a value between $0$ and $1$ which represents what proportion of all possible partners would be less preferable to settle with. For example, a person with a value of $0.6$ would be preferred over $60%$ of other people. Our question now becomes, what strategy do we follow to maximise the expected value of the person we end up settling with. It turns out that the optimal strategy for this scenario is exactly the same as before except now we set $k=\\sqrt{n}-1$ (after rounding) rather than $\\frac{n}{e}$. As before, we can confirm this using simulation. Prove Your LoveThis section will likely only be of relevance to those who have an interest in pure mathematics. Before we bring things to a close, I wanted to offer a proof of the optimality of the procedure for maximising the chances of finding the one. A proof of the expected value optimising method can be found here though it is slightly more involved. The proof for the former scenario is surprisingly short and elegant. First observe that the optimal strategy has to be one of the form “reject $k$ people then choose the next best” since we should never settle for someone who is not the best that we have seen so far, since this could never be the best option. Furthermore, as we have no knowledge of the distribution of the potential partners’ values it also makes no sense to reject a new best person after the first $k$. With this in mind we denote $P(k)$ to be the probability of settling with the best person after rejecting the first $k$. Then we have: $$\\begin{align}P(k) &amp;= \\sum_{i=1}^n \\mathbb{P}(\\textrm{person } i \\textrm{ is selected and is the best})\\\\ &amp;\\stackrel{\\text{Bayes}}{=} \\sum_{i=1}^n \\mathbb{P}(\\textrm{person } i \\textrm{ is selected}|\\textrm{person } i \\textrm{ is the best})\\cdot\\mathbb{P}(\\textrm{person } i \\textrm{ is the best})\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n \\mathbb{P}(\\textrm{person } i \\textrm{ is selected}|\\textrm{person } i \\textrm{ is the best})\\\\ &amp;= \\frac{1}{n}\\sum_{i=k+1}^n \\mathbb{P}(\\textrm{person } i \\textrm{ is selected}|\\textrm{person } i \\textrm{ is the best})\\\\ &amp;= \\frac{1}{n}\\sum_{i=k+1}^n \\frac{k}{i-1} \\quad [\\textrm{best of first } i-1 \\textrm{ people is in first } k]\\\\ &amp;= \\frac{k}{n}\\sum_{i=k+1}^n \\frac{1}{i-1}\\\\\\end{align}$$ We now rearrange this so it becomes a Riemann sum. $$P(k) = \\frac{k}{n}\\sum_{i = k+1}^{n}\\frac{n}{i-1}\\frac{1}{n}$$ We then let $n$ tend to infinity to obtain the following. $$P(k) = \\lim_{n\\rightarrow\\infty}\\frac{k}{n}\\sum_{i = k+1}^{n}\\frac{n}{i-1}\\frac{1}{n} = x \\int_x^1 \\frac{dt}{t} = -x\\ln x$$ Here $x=\\lim_{n\\rightarrow\\infty}\\frac{k}{n}$ and $t=\\lim_{n\\rightarrow\\infty}\\frac{i-1}{n}$. It is not hard to verify that $x=\\frac{1}{e}$ is the maximum of this function and so we optimise our odds of finding the one by setting $k=\\frac{n}{e}$. For small $n$ this method will not hold and we are instead forced to use dynamic programming (which is how I obtained the table for $n&lt;10$ above). If you have found this post interesting and would like to go deeper, it is best searching for this problem’s common name—the secetary problem.","link":"/guide-to-love/"},{"title":"The Inaccuracy of Accuracy","text":"Imagine for a moment that, as part of your job, you are tasked with solving a statistical modelling problem. You’ve been instructed to develop a predictive tool - trained on a collection of historical data - that will take in new, unseen data and output a binary response variable (‘yes’/‘no’, 1/0, etc.). You crack open your favourite IDE, swiftly throw down a few lines of code, hit ‘run’ and there you have it: 99% accuracy. Great job! Clearly that machine learning course you took has paid off. You pack up your work and prepare to head home early. After all, you earned it. Right? Let’s pause for a second and contemplate whether this is really a worthy accomplishment. The answer is: it depends. The importance of this ‘99%’ figure will ultimately be determined by what you are trying to model and, more critically, what problem you are trying to solve. Let’s look at this scenario again, but this time with reference to a specific real-world problem. Suppose that you’ve been contracted by a biomedical company to build a model that will predict whether a patient has an illness based on a set of biological data collected from them. The illness in question is quite rare, affecting only around 1 in 500 people as illustrated below. Since the illness affects people with such infrequency, it’s difficult to even spot the affected patients in the illustration of the sample (or near impossible for the dichromats - I can only apologise in advance). Let’s go about fitting a model to this data. We use the following algorithm to determine whether or not a person has the illness. Assume the illness does not exist See step 1 We simply classify every patient as not having the illness. This is a preposterous idea and yet it will give us an accuracy of around 99.8%, far higher than the 99% accuracy we flaunted earlier. After all, we may never correctly diagnose a patient who does have the illness, but we will get every prediction correct whenever a patient doesn’t have the illness, and this accounts for the vast majority of observations. This, however, is not the behaviour we wish to capture. Correctly identifying the few people who have the illness so that they can seek medical assistance quickly, is far more important than requiring a handful of people to be checked over by their doctor when it wasn’t essential. In similar cases, such as fraud detection, we wish to obtain the same behaviour. It is vital that we detect the few true cases of fraud even if doing so results in a few incorrect guesses, which can be quickly disregarded after the necessary security checks have been completed. The conclusion we reach - quite paradoxically - is that for a large range of problems, if we want to build an ‘accurate’ model in the colloquial sense, using the statistical measurement of accuracy is no use. Maximising accuracy just can’t be guaranteed to reflect the problem we are trying to solve. But what can? A Finer Look at the ProblemLet’s take a look at our diagnosis problem once more, but through a new lens: the confusion matrix. We take a sample of 1000 people from the population and use our naive model to predict whether they have the illness (that is, we predict that everyone is unaffected). We then compare our predicted classifications with the true value to get the following confusion matrix. Reference Prediction No Yes No 998 2 Yes 0 0This table shows that, of the 998 people who did in fact have the illness, we correctly predicted that all 998 of them were unaffected and of the 2 people who did, we incorrectly predicted that both of them did not have the illness. We calculate our accuracy by summing the number of correct predictions (the diagonal) and dividing by the total size of our sample (sum of all cells) giving the $\\frac{998+0}{998+2+0+0} = 99.8%$ we stated before. In order to generalise this confusion matrix for use with other problems, we will need some terminology to describe each cell. The upper-left corner gives the number of true negatives (TN). This is the number of patients that we correctly identified as not having the illness. The upper-right corner gives the number of false negatives (FN). This is the number of patients who did have the illness but for whom we didn’t identify it. The lower-left corner gives the number of false positives (FP). This is the number of patients who didn’t have the illness but our model predicted that they did. Lastly, the lower-right corner gives the number of true positives (TP). This is the number of people we correctly identified as having the illness. Then, for a general binary classification problem with a positive (P) and negative (N) class (these were “yes” and “no” respectively for our diagnosis problem), the confusion matrix will look like this. Reference Prediction N P N TN FN P FP TPRemembering how we calculated the accuracy using the matrix diagonal before, we obtain the following formula for the statistical accuracy of our model in terms of the false/true, positive/negative value counts. $$\\textrm{Accuracy} = \\frac{\\textrm{TN} + \\textrm{TP}}{\\textrm{TN} + \\textrm{FN} + \\textrm{FP} + \\textrm{TP}}$$ Precision and RecallThinking back again to the diagnosis problem, let us contemplate why our use of statistical accuracy as a measure of success failed. The main issue was that what we really valued in our model was the ability to correctly identify ill patients (the positive class) more so than the avoidance of labelling some perfectly healthy patients as likely to be ill. This would not be as much of an issue if the two classes (ill and not ill) were in similar proportions. In that case, failing to correctly identify ill patients would have a noticeable effect on our accuracy. When, however, the positive class is dwarfed by the negative class (or vice versa) the accuracy metric will no longer be of much use. RecallWe have already seen that the accuracy metric fails spectacularly for our diagnosis problem. But there is a metric that works much better. It goes by the name of ‘recall’ and its formula is as follows. $$\\textrm{Recall} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}}$$ In words, we calculate this value by taking the number of cases in which we correctly identified the illness and divide that by the total number of people that did in fact have the illness. This essentially asks how likely our model is to identify that a disease carrier is indeed affected. In our example above, since $\\textrm{TP} = 0$ (we didn’t predict that any one was in the positive class so couldn’t possibly have a true positive), we have a recall of $0$. Whereas we gained an extremely high value of accuracy, the recall of our model is as low as it physically can be. It is clear that this metric can’t be duped by the rarity of the positive class. PrecisionRecall is important for handling scenarios in which we wish to focus on the correct identification of the positive class; but what about the negative class? An example of such a scenario is the judicial system. The assumption of innocence shows that we care much more about ensuring an innocent person is not sent to prison, than making sure that all guilty persons are locked away. This suffers from the reverse of the issue that the diagnosis problem had. Here the positive class is likely to outsize the negative since it would be rare to bring someone to court without at least a reasonable amount of evidence against them. Because of this, if we were to evaluate our judicial system using statistical accuracy as our metric, we would obtain a strong performance from the practice of immediately jailing anyone who enters the courtroom. Another example would be spam email detection; letting some spam get through is tolerable but hiding important emails in the junk folder is not. Thankfully, there is an alternative metric which counteracts this behaviour. It is called ‘precision’. $$\\textrm{Precision} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}}$$ This is very similar to recall, but rather than dividing the number of true positive cases by the total number of actual positives, we instead divide by the total number of positives we predicted. In other words, it is asking what proportion of the observations we predicted to be positive were actually positive. In other words, a model with high precision is more likely to be correct when it says that an observation falls in the positive class. Precision for the diagnosis problem In the diagnosis example—since we don’t ever predict a positive class—our precision isn’t even defined since we end up dividing by zero in the formula above. This make it clear that our model is not at all suitable for purpose, even though its accuracy looked promising. There are many other statistical measurements used to evaluate binary classification models such as specificity, fall-out, and false discovery rate. These are described in intricate detail on the Wikipedia page for sensitivity and specificity. The Precision-recall SpectrumAlthough these metrics may seem like miraculous solutions to all accuracy-related issues, they do have their own drawbacks. These both tackle particular types of problems well - precision is useful for ensuring you don’t inaccurately identify too many negative class members, and recall makes sure that you only identify positive class members when you have enough confidence. However, the success of one can lead to the failure of the other - precision does not care whether you are correctly identifying the positive class members, and recall isn’t bothered whether you erroneously identify a large amount of negative class members. They do their own jobs well, but do not focus on anything else. They exist on a spectrum; succeeding in one opens you up to the possibility of failure in the other. If you know that the type of problem you are trying to tackle will be solved better by using a specific one of these then go ahead. But, if you want something in the middle of this spectrum - combining their behaviour - there is another option. The $\\textrm{F}_1$ ScoreWhen you want to reflect the combination of the behaviour of two metrics, a common strategy is to take their mean. But which mean? That very much depends on the context. In this case, the best course of action is to use the harmonic mean. In doing this we define the $\\textrm{F}_1$ score of a model. $$\\textrm{F}_1 = \\frac{2}{\\frac{1}{\\textrm{Precision}}+\\frac{1}{\\textrm{Recall}}}\\left(=\\frac{2 \\times\\textrm{Precision}\\times\\textrm{Recall}}{\\textrm{Precision}+\\textrm{Recall}}\\right)$$ This method offers an effective balance between precision and recall. Whenever either of precision or recall get two small, the denominator will grow massively, forcing the $\\textrm{F}_1$ score to shrink to near zero. More importantly, unlike with the arithmetic and geometric mean, having a small value of one of precision and recall is extremely difficult (and in some cases impossible) to fix just by adjusting the other value; you are forced to fix the issue at hand to improve your score. Furthermore, since precision and recall are restricted to the range $[0,1]$ so will their harmonic mean, $\\textrm{F}_1$. This metric will account for a perfect balance of precision and recall, no matter the split of the positive and negative class quantities, and still return a value that we can process in a similar way to accuracy. This is exactly the behaviour we want in our metric. Relation to accuracy By replacing the references to precision and recall in the formula for the $\\textrm{F}_1$ score with their definitions, it can be shown that metric takes a value equal to the accuracy of a model in the case of balanced classes. It follows, that there is little reason to use accuracy over the $\\textrm{F}_1$ score even when the classes are reasonably balanced. Notice that when one of precision or recall is zero, then the $\\textrm{F}_1$ score is undefined. Therefore we need at least one correctly identified positive case in order to have a valid $\\textrm{F}_1$ score. This seems resonable; if your model doesn’t get a single positive prediction right, it is barely even a model. Defining the extremes There is also a convention (which you are free to use as long as you don’t mind angering the pure mathematicians) that whenever the $\\textrm{F}_1$ score is undefined, we say that its value is zero. This uses the false assumption that division of a positve number by zero yields $+\\infty$ but is preferred since it allows our metric to function for all confusion matrices with accurate limiting behaviour. Going further The $\\textrm{F}_1$ score is actually just one member of a family of metrics called the $\\textrm{F}_\\beta$ scores where $\\beta$ can take any real value $\\beta&gt;0$. The $\\textrm{F}_1$ score is simply the special case where $\\beta = 1$. The general definition is similar. $$\\textrm{F}_\\beta = \\frac{\\left(1 + \\beta^2\\right) \\cdot \\textrm{Precision}\\cdot\\textrm{Recall}}{\\beta^2 \\cdot \\textrm{Precision}+\\textrm{Recall}}$$ The parameter $\\beta$ in this metric allows you control the tradeoff between recall and precision, by setting it such that you value recall $\\beta$ is a much as precision. When $\\beta=1$ we are simply left with the case where we value recall and precision an equal amount (a sensible default for most cases). Why This MattersAs discussed in great depth by Virginia Eubank’s book Automating Equality, we are beginning to live in a world where automated systems - rather than humans - control a large proportion of the way we live; be it socially, economically, or politically. In a perfect world, we would have all of these algorithms thoroughly audited, but that is currently far from the case. Until we get to that stage (of which Cathy O’Neil, the author of Weapons of Math Destruction, is building excellent foundations for through her new algorithmic auditing and risk consulting company) the next best thing we can do is start using a metric that does a decent job of reflecting our goals as data scientists. The use of statistical accuracy is pervasive throughout the data science community. Every Kaggle competition, machine learning showcase, and PR campaign, relies on it almost in entirety. For many of the budding data scientists who may not come from a rich statistical background, this can be a great danger. It builds bad habits and avoids the discussion of important topics surrounding data ethics. The field is still young and hopefully we will have a chance to right this wrong at least to some extent, before it becomes too ingrained in the standard process. Is it finally time to concede, and admit the inherent inaccuracy of accuracy?","link":"/inaccuracy-of-accuracy/"},{"title":"Enforcing Input Permanence with Shiny","text":"As part of my job at AstraZeneca as a Data Scientist, I have spent a lot of time building Shiny dashboards. In this work, I have come across a recurring problem in which reactive UI elements have a tendency to reset their stored values to the default. This blog post will be extremely short. I plan to detail the problem that I have been facing and then offer a quick solution that I came up with after working on this problem for a while. I couldn’t find any discussion of this issue on Stack Overflow or on the RStudio forums yet I am sure that it is an issue affecting many people so I hope that this post can be used as a reference for solving the issue. The ProblemTo help explain the problem that I was encountering, I have made a minimal reproducible example. The code for this is as follows. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# only run this example in interactive R sessionsif (interactive()) { ui &lt;- fluidPage( titlePanel(\"Value Permanence\"), sidebarLayout( sidebarPanel( checkboxInput( 'non_primary', 'Allow non-primary colours', value = FALSE), uiOutput( 'colour_selector' ) ), mainPanel( plotOutput(\"iris_plot\") ) ) ) server &lt;- function(input, output) { output$colour_selector &lt;- renderUI({ palette &lt;- c('blue', 'red', 'yellow') if (input$non_primary) { palette &lt;- append(palette, c('green', 'orange', 'purple')) } selectInput( 'colour', 'Colour', choices = palette ) }) output$iris_plot &lt;- renderPlot({ plot( x = iris$Sepal.Length, y = iris$Sepal.Width, col = input$colour ) }) } shinyApp(ui = ui, server = server)} To summarise the above code, we create a UI with a side panel and main panel. The side panel contains two inputs. The first is a check box which lets you select whether to use only the primary colours as your palette or to also include secondary colours. The second is a reactive select input. This lets you choose a colour from the current palette. Therefore this select input is a reactive dependent of the check box input. The main panel then contains a plot of the iris data set with the colours of the points set to whichever colour was chosen with the select input. The output looks like this.We can now change the colour selector to update the plot colour without any trouble.The problem arises if we now try to extend the palette to secondary colours by toggling the check box. This will re-render the select input so that its choices include secondary colours but in doing so it will reset the value to the default (blue - since this is the first item in the vector of choices).We are now free to set the colour input to any primary or secondary colour.This may not seem like much of an issue but imagine if we scale up this app. We now have 10 inputs all dependent on one other input. Changing the parent input may only need us to change a few of the dependent inputs or perhaps even none and yet they will all get reset every time we make a change. With behaviour like that, it would be unlikely that any user would be bothered to make it passed 5 minutes of use with your app. The SolutionThe solution is annoying simple. It took me a while to get to this and I had many failed attempts but now that I have a working solution, I’m glad that it is very easy to implement. The only tweaks that need to be made are to the server code so I will just show that and walk through the changes made right after. 12345678910111213141516171819202122232425262728server &lt;- function(input, output) { # make sure this is before output$colour_selector current_colour &lt;- eventReactive(input$non_primary, { input$colour }) output$colour_selector &lt;- renderUI({ palette &lt;- c('blue', 'red', 'yellow') if (input$non_primary) { palette &lt;- append(palette, c('green', 'orange', 'purple')) } selectInput( 'colour', 'Colour', choices = palette selected = current_colour() ) }) output$iris_plot &lt;- renderPlot({ plot( x = iris$Sepal.Length, y = iris$Sepal.Width, col = input$colour ) })} There are two additions to the original code. The first is creating a reactive value current_colour which is invalidated every time that the check box is toggled. The other is setting select = current_colour() in the selectInput definition. To explain why these changes fix our problem, let’s walk through the reactive process when we first toggle the check box input to add secondary colours. Suppose that we currently have selected some colour other than the default blue. We toggle the check box and this invalidates both current_colour() and output$colour_selector (and in turn, output$iris_plot). Since current_colour is defined before the colour selector, this is re-rendered first, setting its value to the current colour selected. We then move on to re-rendering the select input with the updated palette. The difference is that we now set the default value of this updated input to be current_colour() which we know currently stores the previous colour and so the input is preserved. The reactive-conscious of you may worry about the change to input$colour by resetting the selecting leading to a further update of current_colour() and hence an infinite loop. Thankfully, this does not happen as the the second argument of eventReactive() is isolated from reactivity. Another concern may be what happens when we toggle the checkbox back to primary colours only when we have a secondary colour selected; won’t we then be setting selected to an invalid value. The answer is, yes, we are. But that’s nothing to worry about. When we pass an invalid selection to a selectInput declaration, it just defaults to the first value in the choices vector and so we go back to blue. In this case, it makes sense to reset the input so this isn’t a problem. Try out the improved code for yourself and verify that it does indeed work. I hope you have fun implementing input permanence in your future Shiny projects.","link":"/input-permanence-with-shiny/"},{"title":"Integrating Hexo and Jupyter to Build a Data Science Blog","text":"We all know the saying, “New Year, New (The)me”. Inspired by that thought - or perhaps just by the idea of using that as an opener - I decided that I would begin the 2019/20 academic year by re-theming this blog. This was meant to be a short and sweet project; move some files, install some packages, and off we go. That couldn’t have been further from the truth. In transitioning, I have come across many challenges regarding the immaturity of the integration between static site generators and Jupyter. I have spent considerable time over the summer remedying such issues, and I hope that by documenting my struggles and eventual solutions in this post, I can make a similar move much simpler for anyone doing so in the future. Hugo and BlogdownWhen I first created this blog almost a year ago, I decided to use Hugo as the static site generator behind the site. Reaching this decision was simple. At that time, I was only using R for any data science work and RStudio offered an easy-to-use set of add-ins built upon the blogdown package. This made the production of a data science blog a piece of cake. You could build your site directly in RStudio using interactive R notebooks. These would then be processed automatically by the package and converted into a form ready for Hugo to work with. This worked well for me for several months but I soon started to grow fed up with system for the following reasons. Language AgnosticismI am a strong believer in using the right tool for the right job. And through this philosophy, I soon realised that solely using R wasn’t going to cut it for complicated projects. I wanted to be learning about and discussing the use of other languages for data science. Currently this has only amounted to a splattering of posts using Python and Bash, but I have plans to be including many more languages in my projects in the future. I therefore want to make sure that I am using a system that is language agnostic and easily expandable. RStudio and Blogdown is not such a system. Although it is technically possible to use Python, SQL, and Bash code in R notebooks, the experience is buggy and generally unpleasant. There is a clear preference towards blogging in R. Although this is the main language I use at the moment, who knows what the data science landscape will look like in five years? And so, I decided that it was time to move away from such a tight-knit ecosystem. On top of this, when it comes to sharing my code on GitHub, supplying people with an R notebook is not generally helpful. If this notebook contains R code then that makes sense, but if it is written for a different language then it is unlikely that the intended user of the notebook will even have RStudio installed and so won’t be able to open it. Static Site GenerationHugo is a good tool for static site generation, but it is not for me. I don’t like the available themes, the limited extendability, the manual configuration of the site. It just doesn’t sit well with me, even though I can understand why many people would like it. The Blogdown package defaults to using Hugo as its static site generator and although it is possible to set up the package to use a different generator, the documentation on this is limited and many features - such as syntax highlighting stop working. I want a website that is modern and feature-rich. This means tools such as site search, hierarchical categorisation, and customisable word-count/reading-time by default. Hugo almost certainly has the potential to add these features but this would require some serious graft and I do not have the time or skills for that. Hexo and JupyterIt was clear that I was finding Blogdown and Hugo frustrating but I hadn’t considered any alternatives. I then stumbled across this blog post on dataquest.io. This still wasn’t exactly what I wanted - Pelican themes are painfully minimal in both features and design - but it started the ball rolling. I then came across this GitHub repository for a Hexo theme called Icarus. Now that was exactly what I wanted. The theme is gorgeous, the documentation for Hexo looked clear and intuitive, and using the ideas from the DataQuest blog post, I cloud easily integrate it with Jupyter notebooks (or so I thought). I installed Hexo, downloaded the theme, filled out the config files, and opened a single post from my old Blogdown site to use as a test. I copied the Markdown and code chunks from the original file into a Jupyter notebook (using the R kernel), converted it to markdown with jupyter nbconvert --to markdown path/to/file, and voilà…it didn’t work. Well, it did work, just not as I expected. There were several issues that needed to be fixed. Hours became days, days became weeks, and soon I was wondering whether it would be easier to just use a Dropbox folder full of TXT files for my website. I did, however, manage to fix the major issues. So let’s begin walking through them, and seeing how I came to a solution. Syntax HighlightingWe’ll start simple. When I first generated by Hexo site I noticed that none of the code blocks had syntax highlighting. This was strange since the Icarus theme definitely included highlight.js and yet all codeblocks featured only large blocks of matte grey text. Looking through the Icarus documentation I soon realised what was wrong. There was a clear disparity between the way that the Jupyter notebook converter and Hexo defined codeblocks. This syntax for the former uses triple backticks to open close a codeblock. Whereas the latter uses a more verbose tag notation shown here. I would like to include examples of these in this post but I can’t find a way to stop the Hexo render no matter how I try to escape them so you will just have to use your imagination. This is very easy to fix using a simple pattern-replace regular expression. For the same reason as above, I cannot include the code directly in this post but, as you will see later, I have created a Python script which you can download to fix all of these issues automatically. Image DirectoriesArguably, this problem has an even simpler solution than the first. The root of this issue is that the notebook converter places all outputted figures into a folder called {name-of-post}_files and then points all image links in the markdown file to that folder. This needs to be changed to the actual directory that you will be storing such images. Since this change is the same every time, you don’t even require any regex; a simple call to the .replace() string method will do the trick although I did end up going for regex for simplicity. LaTeX issuesIn almost all cases, LaTeX works the same both with Hugo and with Hexo. I have however noticed some edge-cases that needed fixes. For a start, the align* environment (align environment without line numbering) does not work with Hexo. This is because the asterisk is interpreted as italics before the MathJax engine renders the LaTeX code. Instead you can use the standard align environment which for some reason doesn’t have line numbers when used with Hexo. Another point to note is that the standard // new-line notation does not work as expected. This is because the first slash is used to escape the second, leaving only one slash for MathJax to render. The solution to this is simple - replace every occurrence of two slashes with four. Figure OptionsWhen writing a blog post using Blogdown, it was possible to format the figure output of any block of code by using chunk options. Jupyter and Hexo do not offer such functionality although there is a work around. Figure SizeWhen creating Python plots using matplotlib, this is trivial; you simply specify the figsize parameter when calling matplotlib.figure(). But what about with other kernels? So far I have only been generating figures with R and so I only have a solution for that language, however I will edit this post with updates if/when I use others. To format figure output when using R, all you have to do is start the relevant code cell with a call to the options() function as so. 123options(repr.plot.width=8, repr.plot.height=5, jupyter.plot_mimetypes = \"image/png\")The MIME type can also be set to image/svg+xml which will output an SVG or image/jpeg for a JPEG. For this to run you will need to have install the repr package from GitHub with devtools::install_github('IRkernel/repr') though it doesn’t need to be loaded. The call to this function can then be removed from the Markdown file using the following regular expression r'options\\((?:(?!\\()(?:.|\\s))*\\)\\s*' Figure AlignmentUsing R notebook chunk properties, it is possible to set figure alignment to one of left, center, or right. I have not found a way to do this with Jupyter. This, however, has not presented an issue since I would like all my figures to be centred. Hence, I just tweaked the CSS properties related to blog post images to get the desired result. Figure CaptionAnother useful feature of the blogdown package is the ability to set a figure caption using chunk options. This is a feature that I really wanted to replicate but there appeared to be no standard way of doing so. The source code for the options() function makes no mention of figure captions. Instead, the caption created by nbconvert is just the file type of the image. For example if you used image/svg+xml for the MIME type, the figure caption would be svg. I decided that the best way to fix this was to use a Python script. This will run through each line of the outputted markdown file and look for the pattern @caption=&quot;This is the caption&quot;. The script will then extract the caption text and then carry on searching through the file until it finds an caption-less image, denoted by ![svg](/path/to/image) or similar. The script can then replace the default caption with the caption it is storing from the line above. Therefore, to use this script, all you have to do is add a cell of type Raw NBConvert containing @caption=&quot;...&quot; before your figure-producing cell and then the rest is handle automatically. Other Chunk PropertiesThe above fixes were really quite simple once I knew which direction to head in, but this last issue gave me a bit more of a headache. Never-the-less, I did eventually come to rather simple solutions. These problems revolve around the R notebook chunk options eval and echo. These, respectively, tell R to either not run a chunk of code or to only show the output of a chunk and not the code that produced it. The former of these is easy to replicate with Jupyter/Hexo. You simply replace the code cell with a Raw NBConvert cell containing manually entered codeblock tags. The latter, however, required some thought. In the end, I decided to use a solution similar to the one before for figure captions. This means that before any cell you don’t wish to echo, you need to add a Raw NBConvert cell containing the command @noecho. Then we can use the regular expression r'@noecho(?:.|\\s)*?endcodeblock %}'. This uses the lazy regex quantifier *? to only remove code between the call for @noecho and the next closing codeblock tag. When using the script provided in the next section, it is import to make sure that any manually entered codeblocks following the @noecho tag are closed else any output/text between that block and the next will be removed. SummaryI have packaged up these tweaks into a short Python script which can be run on a Markdown file outputted by nbconvert. It can be found here. It is worth noting that I have also added a few edits to the theme’s CSS file. I did not make a list of these but none of them were difficult to discover by using the Chrome page inspector tool. That concludes the main part of this post. I do however have a few more words to add on the use of colourful quotes and the remaining problems which I have been unable to solve. Colourful QuotesThe author of the Icarus theme, ppoffice has also created a Hexo theme called Minos (sensing a pattern?). The theme is a bit too minimalist for my liking but it does have a feature that I really love — colorquotes. You will see these dotted all over this site and even on this page. There are four variants as shown below. Info Success Warning Danger These are not usually a part of the Icarus theme but can be added as so. First you need to register a colorquote extend tag. This is done by creating a file called colorquote.js in the directory themes/icarus/scripts containing the following. 1234567891011121314/*** Color Quote Block Tag* @description Color Quote Block* @example* &lt;% colorquote [type] %&gt;* content* &lt;% endcolorquote %&gt;*/hexo.extend.tag.register('colorquote', function (args, content) { var type = args[0]; var mdContent = hexo.render.renderSync({text: content, engine: 'markdown'}); return '&lt;blockquote class=\"colorquote ' + type + '\"&gt;' + mdContent + '&lt;/blockquote&gt;';}, {ends: true}); You can then edit the main CSS file for the theme. This can be found at themes/icarus/source/css/style.styl. All you need to do is add the following as a top-level declaration. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970blockquote position: static font-family: font-serif font-size: 1.1em padding: 10px 20px 10px 54px background: rgba(0,0,0,0.03) border-left: 5px solid #ee6e73 &amp;:before top: 20px left: -40px content: \"\\f10d\" color: #e2e2e2 font-size: 32px; font-family: FontAwesome text-align: center position: relative footer font-size: font-size margin: line-height 0 font-family: font-sans cite &amp;:before content: \"—\" padding: 0 0.5em.colorquote position: relative; padding: 0.1em 1.5em; color: #4a4a4a; margin-bottom: 1em; &amp;:before content: \" \"; position: absolute; top: 50%; left: -14.5px; margin-top: -12px; width: 24px; height: 24px; border-radius: 50%; text-align: center; color: white; background-size: 16px 16px; background-position: 4px 4px; background-repeat: no-repeat; &amp;.info border-color: hsl(204, 86%, 53%); background-color: hsl(204, 86%, 93%); &amp;:before background-color: hsl(204, 86%, 53%); background-image: url(\"../images/info.svg\"); &amp;.success border-color: hsl(141, 71%, 48%); background-color: hsl(141, 70%, 88%); &amp;:before background-color: hsl(141, 71%, 48%); background-image: url(\"../images/check.svg\"); &amp;.warning border-color: hsl(48, 100%, 67%); background-color: hsl(48, 100%, 91%); &amp;:before background-color: hsl(48, 100%, 67%); background-image: url(\"../images/question.svg\"); &amp;.danger border-color: hsl(348, 100%, 61%); background-color: hsl(348, 100%, 85%); &amp;:before background-color: hsl(348, 100%, 61%); background-image: url(\"../images/exclamation.svg\"); You will then need to copy the files check.svg, exclamation.svg, info.svg, and question.svg from the Minos repository to the directory themes/icarus/images. By this point, the colorquote blocks will work just fine as long as you do not intend to nest a codeblock inside of them. If you wish to do this then you will need to add one more JavaScript file called tags.js under themes/icarus/scripts with the following content. 12345678910111213141516171819202122232425/*** Tags Filter* @description Fix the code block using ```&lt;code&gt;``` will render undefined in Nunjucks* https://github.com/hexojs/hexo/issues/2400*/const rEscapeContent = /&lt;escape(?:[^&gt;]*)&gt;([\\s\\S]*?)&lt;\\/escape&gt;/g;const placeholder = '\\uFFFD';const rPlaceholder = /(?:&lt;|&amp;lt;)\\!--\\uFFFD(\\d+)--(?:&gt;|&amp;gt;)/g;const cache = [];function escapeContent(str) { return '&lt;!--' + placeholder + (cache.push(str) - 1) + '--&gt;';}hexo.extend.filter.register('before_post_render', function(data) { data.content = data.content.replace(rEscapeContent, function(match, content) { return escapeContent(content); }); return data;});hexo.extend.filter.register('after_post_render', function(data) { data.content = data.content.replace(rPlaceholder, function() { return cache[arguments[1]]; }); return data;}); Remaining IssuesBefore I close off this post, I would like to discuss the Jupyter/Hexo integration issues that I have not been able to solve. This may be just for cathartic purposes or perhaps someone may be able to suggest a workaround that I have not yet considered. If I do find a solution to any of these issues in the future then I will certainly update this next section. Inline CodeR notebooks offer a great feature where you can run code whilst in a markdown block using the notation `r print(&quot;Hello World&quot;)`. In these inline blocks you can also reference variables from the main code chunks. This is incredibly useful when your code has an element of randomness to it and you wish to quote the results that a main code block gave in some explanatory text below. I do not believe there is a simple way to do this and so I have instead resorted to setting random seeds. This works for some cases but not for those such as benchmarking. CachingAnother nifty feature of R notebooks is the ability to cache chunk outputs. This means that if you have a long-running piece of code, you can run it once and then store the output for future use. This is more an issue of convenience then actual practicality and so it’s not a priority for me to solve right now. GIFsAs a proponent of animated visualisations, I am partial to the creation of GIFs in my posts. As mentioned before, the only valid MIME types for R output are PNG, JPEG, and SVG. Therefore GIFs have to be generated separately to the main post and embed using Markdown code. Again, this works fine, although seamless integration would be nice.","link":"/integrating-hexo-and-jupyter/"},{"title":"Letter Distributions in the English Language and Their Relations","text":"The following packages will be required for this post: 123library(gplots) # heatmaplibrary(stringr) # string cleaninglibrary(viridis) # colourblind-friendly palette MotivationThere are many unspoken but unanimously understood rules in the English language. For example, starting a word with the letter ‘Q’ feels completely reasonable; “queue”, “quark”, “quizzical”, and many other words do just this. But can you think of any English word ending with the letter ‘Q’? Unless you are a scrabble-nerd, I imagine that this is a near impossible task. In my research I could only find 6 examples and most of these were alternate spellings for already obscure words. For example, the shortest of such words is ‘suq’ an alternative spelling of ‘souk’, a North African or Middle Eastern marketplace. Great for annoying your opponents in scrabble but not so much for everyday speech. There are many more such rules. A large amount of which have exceptions though in the majority of cases are obeyed. Yet they rarely cross our mind. They just look natural. They just look English. There are many other conventions guiding the construction of English words that involve how letters relate together. Examples being that a ‘q’ is almost always followed by a ‘U’ and that only certain letters can be doubled - certainly a word with a double-x would not look like it belongs in the English language. I do not wish to analyse such rules in this post but instead focus on the patterns that arise in the distributions of individual letters. I will return to this more general idea in a blog post I wish to write in the future concerning the use of Markov chains in language generation which will be linked here when published. The idea for this blog post was spurred on by the data collection for my recent blog post concerning the optimum layout of the T9 typing system. In this, I sourced data on the letter frequencies of various languages including English. Alongside the frequency tables describing the general letter distributions in English there were also tables looking at the same data for just the first letter, second, or third. Whereas the most common letter in the English language without restriction on position is ‘E’, making up 13% of all use, when we just look at the first position, the dominant letter becomes ‘T’ with a whopping 16% of all use (‘E’ contributes only 2.8%). These are fascinating but not very reflective measures of the overall distribution of letters within words. They don’t account for the different lengths of words in English and, more importantly, that very long words are far less common than the typical lengths. In order to have a serious look at how letters are distributed within words, what we would really want is a sort of continuous probability distribution on $(0,1)$ with the value at each point in that interval relating to how likely it is for a particular letter to be located at that proportion of a word. I spent hours searching for this sort of data but to no avail. It simply didn’t appear to exist yet. This might have been an appropriate point to put this project to rest. On the other hand, with the right skills and data sources, there’s no reason why this information can’t be constructed from scratch. The process of such data collection and the analysis that follows is what I wish to discuss in this post. Data CollectionIn order to complete the desired analysis we need words. Lots of words. My favourite location for random text data is the website ‘textfiles.com‘. This contains a massive selection of free-to-access books in ‘.txt’ format. They range over a wide spectrum of genres and styles featuring both fiction and non-fiction books from throughout history. We will be specifically looking at the collection of fiction books they have, located here. This page contains 328 hyperlinks, each of which leads to a text file transcription of a classic piece of fiction. In order to make this post self-contained and reproducible, instead of downloading these files manually, we will download a selection of them directly from the site using R. Due to the simple structure of the site and its constitute webpages, this is extremely easy. 12345678910111213141516171819202122232425262728293031323334353637383940# minimum number of words we wish to sourcenwords &lt;- 10^6# url of webpage listing available fiction booksdata_source &lt;- \"http://textfiles.com/etext/FICTION/\"# download HTML code of this webpagehtml &lt;- scan(file = data_source, what = character(0), sep = \"\\n\", quiet = T)# extract links to text files from each line of HTML codebooks &lt;- str_match(html, \"&lt;A HREF=\\\"(.*?)\\\"\")# remove any lines that didn't contain a linkbooks &lt;- books[!is.na(books[,1]),]# extract the relative url from the matched linksbooks &lt;- books[,2]# make url absolutelinks &lt;- paste0(data_source, books)# randomly order the links to the bookslinks &lt;- sample(links)# define empty vector to store wordswords &lt;- character(0)# loop through links, downloading contents until we have enough wordsfor (l in links) { words &lt;- append(words, scan(file = l, what = character(0), sep = \" \", quote = \"\", quiet = T)) if (length(words) &gt;= nwords) break} We now have a list of at least one million random words taken from a variety of fictional works. Let’s take a look at a small sample of them. 1sample(words, 40) 'Elvira' 'at' 'in' 'stay.\"' 'least,' 'like' 'The' 'meant' '' 'tender,' 'all' 'I\\'ve' 'month.' 'them,' '' 'is' 'years' '' 'half-unburn\\'d.' 'but' 'process' 'for' 'and' 'Contek,' '' 'three' 'state' '' 'board' 'has' 'He' 'there?\"' 'done,' 'attempt' 'in' 'jolt' 'her' 'is' 'Greek,' 'child' The data is awfully messy, but it is there. We now use the stringr package and some base R functions to perform a bit of text-tidying. 12345678# remove any characters that are not standard letterswords &lt;- str_replace_all(words, \"[^[a-zA-Z]]\", \"\")# remove any blank stringswords &lt;- words[words != \"\"]# convert to upper-case for consistencywords &lt;- toupper(words) Taking a look at a sample of the words now gives a more pleasing result. 1sample(words, 40) 'AND' 'NOT' 'FIRST' 'OF' 'DOOR' 'VERTUOUS' 'TOLD' 'BECOME' 'POSITION' 'HALICARNASSUS' 'OF' 'IT' 'ROLLED' 'MONEY' 'I' 'AMMON' 'BORDER' 'SEIZED' 'HOW' 'TO' 'THAS' 'SIDES' 'I' 'MY' 'AS' 'OF' 'LAST' 'MEANS' 'WILL' 'INVOLUNTARY' 'TILL' 'AS' 'HAD' 'AGAIN' 'MANETTE' 'LAST' 'REJOICE' 'DO' 'MADE' 'WILL' Notice that we had the choice to either entirely remove any words containing a hyphen or apostrophe or simply omit the offending characters from the results. I went for the latter as I don’t believe it will have a significant effect on the result and so would be an unnecessary waste of data. Patterns and RelationsGenerating probability distributionsNow that we have our word-bank, we can begin analysing it. We first need a way of converting this list into a collection of probability distributions, one for each letter, representing where in a random word that letter is likely to be situated. My first idea was to use a weighted kernel density estimation. For each letter in a word, you take its position as a proportion (e.g. the third letter in a four letter word has position $\\frac{2\\times3 - 1}{4 \\times 2} = \\frac{5}{8}$) and then place a kernel at this point weighted by the inverse of the length of the word. If this sounds complicated, don’t worry; it was no good. Although it did produce some rather pleasing results, it did not scale well. It took around a minute to process just a thousand words so the idea of it working for one million or more is almost comical. Instead, I decided the best way was to proceed was to compute a probability distribution directly. For a specific letter of a particular word the process goes as follows: Equally split the interval $(0,1)$ into a number of classes equal to the length of the word Consider the class whose number (when ordered in an increasing fashion) is the same as the position of the letter in the word Increase the value all points in this interval by an amount proportional to the length of the interval When repeated for all letters and words, this will give rise to a reasonably continuous pseudo-PDF. We code this up as follows. 1234567891011121314151617181920212223242526# create matrix to store the values of the points in the # interval discretised with a resolution of 500count_mtrx &lt;- matrix(0, nrow = 500, ncol = 26)rownames(count_mtrx) &lt;- 1:500colnames(count_mtrx) &lt;- c(LETTERS)# loop through all wordsfor (w in words) { # split word into characters chars &lt;- strsplit(w, split = \"\")[[1]] len &lt;- length(chars) # loop over all letter positions in the word for (pos in 1:len) { # limits of the sub-interval lower_limit &lt;- floor(500 * (pos - 1) / len) + 1 upper_limit &lt;- ceiling(500 * pos / len) # discrete points to increase the value of updating_cells &lt;- lower_limit:upper_limit # increase value of these cells new_val &lt;- count_mtrx[updating_cells, chars[pos]] + 1 / len count_mtrx[updating_cells, chars[pos]] &lt;- new_val }}# scale values so the area under the curve is approximately onefreq_mtrx &lt;- apply(count_mtrx, 2, function(x) 500 * x / sum(x)) We now have a 500x26 matrix, each column representing a letter and each row representing the discrete approximation of the probability density function we designed. Let’s take a look at these distributions.@caption=’The probabalitity distribution of English letters within a random word’ 1234567891011# split plotting window into a 4x7 grid and reduce marginspar(mfrow = c(4,7), mar = c(rep(c(2,1), 2)))for (l in LETTERS) { # position bottom row at centre if (l == \"V\") plot.new() plot(freq_mtrx[, l], main = l, xlab = \"\", ylab = \"\", xaxt = 'n', yaxt = 'n', type = \"l\")} There is clearly a lot of noise in the distributions (which is unsurprising considering the primitiveness of the method used to generate them) but their overall shapes are clearly visible. To highlight these trends, we will fade the current piecewise function and add a LOESS approximation to smooth out the noise.@caption=’The probabalitity distribution of English letters within a random word using LOESS smoothing’ 1234567891011121314# split plotting window into a 4x7 grid and reduce marginspar(mfrow = c(4,7), mar = c(rep(c(2,1), 2)))for (l in LETTERS) { # position bottom row at centre if (l == \"V\") plot.new() plot(freq_mtrx[, l], main = l, xlab = \"\", ylab = \"\", xaxt = 'n', yaxt = 'n', type = \"l\", col = \"#00000080\") # add LOESS curve lines(lowess(freq_mtrx[, l], f = .2, iter = 10), col = \"red\", lwd = 2)} Comparision and clusteringLooking at these distributions we can see similarity between between certain letters. For example, ‘B’ and ‘J’ both feature heavily at the start of a word and then become less likely to appear as you progress through until at about halfway through the word, the odds drop sharply. Other similar distributions are that of ‘A’ and ‘I’ which both start with moderate density then become more common halfway through the word then drop off towards the end. It would nice to have a way to numerically quantify the similarity of two such distributions. My first instinct was to use a two-sample Kolmogorov-Smirnov test, a non-parametric hypothesis test for the equality of the two continuous distributions. The hypothesis test itself has little relevance to us as we know the distributions are different already but the test statistic $D$ measuring dissimilarity of the distributions from 0 to 1 would be a useful measurement to obtain. This test, however, behaved very strangely in many ways. For example it said that Z was more similar to R than U which is clearly wrong. I’m not sure of the exact cause of this behaviour but I assume that it is to do with the test being more sensitive to points in common than to differences. As has been the spirit of this post throughout, if there is no working existing solution, why not make your own? I therefore decided to use the trusted $L^2$-norm, a generalisation of the Euclidean distance to vectors or functions, such as the discrete approximations of our densities. Using this metric we can build a dissimilarity matrix as follows. 123456789101112# function to calculate L2_distance from two rows of the# count matrix using the LOESS approximationsL2_distance &lt;- Vectorize(function(x, y) { x_smooth &lt;- lowess(freq_mtrx[, x], f = .2, iter = 10)$y y_smooth &lt;- lowess(freq_mtrx[, y], f = .2, iter = 10)$y sqrt(sum((x_smooth - y_smooth)^2))})# generate dissimilarity matrixdissim_matrix &lt;- outer(LETTERS, LETTERS, L2_distance)rownames(dissim_matrix) &lt;- LETTERScolnames(dissim_matrix) &lt;- LETTERS Taking a look at the first few values of the matrix we get the following. 1dissim_matrix[1:10, 1:5] $(document).ready( function () {$('#table0').DataTable();} ); ABCDE A 0.00000016.77646312.7874427.2829722.359695 B16.776463 0.00000018.0404537.3906036.416457 C12.78743718.040450 0.0000029.0389625.082159 D27.28297237.39060429.03896 0.0000011.546242 E22.35969536.41645725.0821611.54624 0.000000 F18.64071231.65725618.3748815.34660 9.030299 G11.75300120.59233814.9896818.3806917.744923 H 6.09556317.44990217.2947431.6145526.434965 I 2.70575917.94577914.6683428.0974022.766470 J23.697318 9.31852520.3649040.6973441.005730 As we can see, every letter has a dissimilarity with itself of zero since they have exactly the same distribution. As stated before ‘A’ and ‘I’ have similar distributions and this is reflected by the small dissimilarity value of 2.7. We can visualise the entire matrix using a heat map.@caption=’Heatmap of the dissimilarity matrix for smoothed letter distributions using the $L^2$-norm’ 12345678910111213# don't show a value for letters paired with themselvesdissim_matrix_na &lt;- dissim_matrixdiag(dissim_matrix_na) &lt;- NA# create a heatmap using the gplots packagegplots::heatmap.2(dissim_matrix_na, dendrogram = 'none', Rowv = FALSE, Colv = FALSE, trace = 'none', na.color = \"white\", # reverse viridis scale col = viridis::viridis_pal(direction = -1)) This closely matches our expectation. ‘E’ and ‘J’ have very different distributions and so have a dark cell with a dissimilarity value in the 30s whereas ‘Q’ and ‘W’ are very similar and so have a light cell with a value under 15. Now that we have a form of distance metric between any two letter distributions, we can perform cluster analysis. Since we don’t have the exact coordinates of the distributions in some general space (though we could formulate this using a method I will soon discuss in another blog post) we can’t use k-means clustering. We are instead forced to use an agglomerative method such as hierarchical clustering. We will use the complete linkage method of clustering. This is chosen by elimination more than anything else. We would like a monotone distance measure so that our resulting dendrogram has no inversions so centroid (UPGMC) and median (WPGMA) methods are out of the question. Furthermore, single and average (UPGMA) methods do not force enough similarity within clusters. Lastly, Ward’s method of minimum variance will aim to find spherical clusters which is an unreasonable assumption for our data. We therefore proceed with complete linkage to produce the following dendrogram. 12clust &lt;- hclust(as.dist(dissim_matrix), method = \"complete\")plot(clust, xlab = \"\", sub = \"\") This, again, matches the behaviour we would expect. The distributions that we previously said were similar such as ‘B’, ‘J’, ‘Q’, and ‘W’ are clustered very close together whereas the highly dissimilar distributions such as ‘E’ and ‘J’ only connect at the max height of 41 From this dendrogram it appears that an appropriate cut would be somewhere between a height of 16 and 21. The exact choice would depend on the number of clusters we were after. I decided to cut at a height of 16, giving the following groupings.@caption=’The probabalitity distribution of English letters within a random word using LOESS smoothing and coloured by their cluster group’ 123456789101112131415groups &lt;- cutree(clust, h = 16)palette &lt;- rainbow(length(unique(groups))) par(mfrow = c(4,7), mar = c(rep(c(2,1), 2)))for (l in LETTERS) { # position bottom row at centre if (l == \"V\") plot.new() plot(freq_mtrx[, l], main = l, xlab = \"\", ylab = \"\", xaxt = 'n', yaxt = 'n', type = \"l\", col = \"#00000080\") lines(lowess(freq_mtrx[, l], f = .2, iter = 10), col = palette[groups[l]], lwd = 2)} Attempting to categorise these leads to the following possible interpretations of the groupings: The red letters are those which feature most prominently in the middle of words, very little at the end and occasionally at the beginning The yellow letters are those that are common at the start of a word but become less likely the further towards the end you get The green letters are most common at the start of words but barely feature in the middle. They are also reasonably prominent at the end of words (‘G’ being an outlier due to the common use of ‘ing’) The blue letters feature often at the ends of words but less so at the beginning The purple letters appear most often in the middle of the words The rules have clear exceptions. For example, ‘C’ could just as easily be categorised as a new colour since it isn’t all to similar to the other green letters (this is evident form its late join to that branch of the dendrogram). In general though, this clustering does a promising job of grouping the letters by their functions in words.","link":"/letter-distributions/"},{"title":"Ordering Factors within a Faceted Plot","text":"Recently, I stumbled across an interesting dataset on Kaggle. It contained information on every event, for every relevant year of the last 120 years of Olympic Games. The dataset can be found at this link, and although it does have some minor data integrity issues (at least at the time of writing this post) it has clear potential for telling some amazing stories. My plan was to start simple, and create a faceted column chart showing how many medals the top 10 countries won over a selection of four games. At least, I thought this would be simple. In fact, this plot ended up taking me down a data-viz rabbit hole, desperately trying to get by factors to order themselves how I wanted. Thankfully, I did eventually emerge, and so I am now here to share my journey so that the next unlucky victim of ggplot’s tyranny can reach a solution without so much frustration. The DataI will skip over the exact details regarding the full scope of the dataset and how I processed it for my use. The Kaggle page explains the contents of the dataset in clear terms and the source code for this project can be found on this blog’s GitHub repository. The important point is that after some messing about, I ended up with a dataset looking like this. 1medals_df $(document).ready( function () {$('#table0').DataTable();} ); TeamYearBronzeGoldSilverTotal &lt;chr&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; Soviet Union 1956323729 98 United States1956173125 73 Australia 19561313 7 33 Germany 1956 7 613 26 Hungary 1956 7 910 26 Italy 1956 9 8 7 24 Great Britain1956 9 6 6 21 Japan 1956 3 410 17 Sweden 1956 6 6 5 17 Finland 195611 3 1 15 Soviet Union 1976354941125 United States1976253435 94 East Germany 1976254025 90 West Germany 1976171012 39 Romania 197614 4 9 27 Poland 197613 7 6 26 Japan 197610 9 6 25 Bulgaria 1976 7 6 9 22 Hungary 197613 4 5 22 Cuba 1976 3 6 4 13 United States1996254331 99 Germany 1996262018 64 Russia 1996162621 63 China 1996111320 44 Australia 199622 9 9 40 France 19961514 7 36 Italy 1996121310 35 Cuba 1996 8 9 8 25 Ukraine 199612 9 2 23 South Korea 1996 3 613 22 United States2016364536117 China 2016252518 68 Great Britain2016172723 67 Russia 2016201817 55 France 2016141018 42 Germany 2016151610 41 Japan 20162112 8 41 Australia 201610 811 29 Italy 2016 8 811 27 Canada 201615 4 3 22 To summarise, this is a dataset of 40 rows. Each row corresponds to the medals won by a particular country at a particular summer Olympic games. Each value of the year column is one 1956, 1976, 1996, and 2016, and only the ten countries with the most medals for each year are included. Attempt 1 - HopeWhat I then wanted to do, was to create a column chart showing the medals won by each country, faceted by the year of the games. My first solution was somewhat naive, going something like this. 12345678910# tidy the dataset...gather(medals_df, c(Bronze, Gold, Silver), key = 'Medal', value = 'Count') %&gt;% mutate(Medal = factor(Medal, levels = c('Gold', 'Silver', 'Bronze'))) %&gt;% # ...and plot ggplot(aes(x = Team, y = Count, fill = Medal)) + geom_col() + facet_wrap(~Year, nrow = 2, scales = 'free_y') + coord_flip() + # superfluous additions for aesthetics - see GitHub for contents labels_and_colours It’s a valiant effort, but frankly, it’s just ugly. By default, ggplot uses the factor’s underlying ordering when deciding how to arrange a categorical axis. Since we did not specify an ordering, R defaults to using alphabetical order and so, as we can see, the y-axes is sorted alphabetically. Not only does this look bad but it makes the plot difficult to interpret. Which team had the 5th higher medal total in 1976? You’d have to take a second to figure it out; if the levels had been correctly ordered, this would be much simpler. Attempt 2 - CompromiseThe solution seems obvious now (I thought): the problem is brought upon by not specifying an order for the Team factor before plotting. So if we were to do just that, our problem should disappear. We can use the reorder function from base R (alternatively forcats::fct_reorder) combined with a mutate to achieve this (or so I thought). The code and result look something like this. 123456789101112# first, reorder the factors by total...mutate(medals_df, Team = reorder(Team, Total)) %&gt;% # ...then tidy the dataset... gather(c(Bronze, Gold, Silver), key = 'Medal', value = 'Count') %&gt;% mutate(Medal = factor(Medal, levels = c('Gold', 'Silver', 'Bronze'))) %&gt;% # ...and plot ggplot(aes(x = Team, y = Count, fill = Medal)) + geom_col() + facet_wrap(~Year, nrow = 2, scales = 'free_y') + coord_flip() + # superfluous additions for aesthetics - see GitHub for contents labels_and_colours Close, but no cigar. Some ordering has taken place, but things still aren’t quite right. The problem is that reordering has taken place at a global level, not per facet. This is not a bug, but the expected behaviour of reorder. When faced with duplicates in the factor it is given, it only orders the levels by using the median value for each unique level (forcats::fct_reorder uses the mean). Since our dataset contains teams that feature in the top 10 over multiple years (in fact, there are quite a few), this approach to ordering fails too. We tried to play nicely and cooperate with ggplot, but that got us nowhere. It’s now time to bring out the big guns. Attempt 3 - AggressionAt this point, I was starting to lose hope. Maybe this just wasn’t something within the scope of ggplot’s arsenal. I had one more idea though. It wasn’t going to be pretty or even that sensible, but sure enough it worked. Here is the final code I came up with and the resulting plot. I will explain the mechanics of this code immediately after. 123456789101112131415161718192021222324medals_df %&gt;% # create a column called Order to store the factor ordering for each year arrange(Year, Total) %&gt;% mutate(Order = row_number()) %&gt;% # tidy the dataset as before gather(c(Bronze, Gold, Silver), key = 'Medal', value = 'Count') %&gt;% mutate(Medal = factor(Medal, levels = c('Gold', 'Silver', 'Bronze'))) %&gt;% # wrap in curly brackets so we can access the augmented dataset multiple times { # use Order for the x aesthetic instead of Team ggplot(., aes(x = Order, y = Count, fill = Medal)) + geom_col() + facet_wrap(~Year, nrow = 2, scales = 'free_y') + coord_flip() + # add custom breaks and labelling to the x-axis scale_x_continuous( breaks = .$Order, labels = .$Team, expand = c(0,.4) # just for looks ) + # superfluous additions for aesthetics - see GitHub for contents labels_and_colours } Perfect! Not only is this easier to interpret but it looks much better. So why does this work? Let’s get into the details. First, we create a new column, Order, which stores the order that each factor level should appear in within each year. We do this by first arranging by year, then total, and using the row_number() helper to save that ordering. The observations for the year 1956 now have orderings spanning from 1 to 10, and if we carry on till 2016, these have orderings from 31 to 40. When we get to plotting, rather than using Team as our x aesthetic, we use this new Order. Since we use a free_y scale when faceting this results in us still having ten y-axis ticks for each facet. At this point though, the ticks will be labelled 1-10, 11-20, etc. for each facet. To correct this, we need to manually set our x-axis breaks and labels. We do this in scale_x_continuous(). We set the breaks to be equal to the Order column (note we use . here to access the dataframe we piped into the curly bracketed section) and then use Team as the labels. This means that the numeric labels are replaced with the team name that they original corresponded to. This is exactly what we were after. Wrapping UpThere you have it. It may not be the most elegant solution, but it certainly works. This approach can be adapted for any data that you wish to plot in this form. I hope that with this example to guide the way, a significant amount of frustration can be avoided.","link":"/ordering-with-facets/"},{"title":"Paradoxical Tournaments","text":"All graphs in this post are drawn using the R igraph package. The original code can be found on the GitHub repository for this site. A Paradoxical ProblemSuppose that you are organising a tournament in which $n$ each take turns to play one another exactly once. This is known as a round robin tournament. We can represent this visually using a complete oriented graph on $n$ vertices where a directed edge from vertex $v$ to $w$ means that player $v$ won the game against $w$. We should probably roughly define the terms that we just used. A complete graph on $n$ vertices is one where there is an edge between every distinct pair of vertices. Further, an oriented graph is a directed graph in which only one of the directed edges $(x,y)$ or $(y,x)$ appear in the edge set. Hence a complete oriented graph is a graph in which every distinct vertex pair has exactly one directed edge between them. The important thing is that for any tournament, we can find a graphical representation in the style of the following. The term ‘round robin’ is an 18th century anglicisation of the French ‘ruban rond’ which literally translates as ‘round ribbon’[src]. The original French term referred to the act of signing petitions in a circular order around a round ribbon, hence disguising the ringleaders. Its relation to tournaments came later into use at the closing of the 19th century[src]. We use zero indexing for our vertices as this makes it easier to represent the graphs algebraically. Given a tournament, we need a way to decide who the winner is. A naïve approach would be to look for a player who beats every other player and then crown them the victor. In the example above, this would be player 3. When this can be done, there is no denying that the selected champion is deserving; it is only possible for one player to beat everyone else since each pair of players only compete a single time, hence this player will have won the most games. The problem is, that this dominating player cannot always be found. In fact, it is not difficult at all to construct a counter-example. The smallest $n$ we need to do this is 3, as shown here. The mathematical term for such a violating tournament is a paradoxical tournament. GeneralisationAnother way of thinking about this is that a paradoxical tournament satisfies the property that for every subset of 1 player (i.e. any player), there exists another player that beats every player in that subset (i.e. beats that player). This rewording leads us into thinking about how we can generalise the notion of a paradoxical tournament. We start by renaming a paradoxical tournament as a 1-paradoxical tournament, since the subsets we were considering above only had one player in each of them. We can then define a $k$-paradoxical tournament to be a tournament in which for every subset of $k$ players, we can find another player outside of that subset that beat every player in that subset. 2-paradoxical TournamentsFor small tournaments, this notion is redundant. For any tournament with 6 or fewer players, it’s impossible to have a 2-or-higher-paradoxical tournament as we will prove in a moment. When a seventh player joins, however, everything can start to fall apart and we can find our first 2-paradoxical tournament. Here is the graphical representation of an example that I came up with. You can verify this for yourself. I’ll look at a few cases now as an example. Let’s start with the 2-subset (0, 1) — both these players were beaten by player 4. Likewise, consider the subset (2, 6) — we can see that player 5 is the common victor. Similar checks can be done for all subsets. This also proves that there are 2-paradoxical tournaments of for any number of players greater than 7. Simply take the graph above and repeatedly add a new vertex that is beaten by all existing vertices until you attain the desired number of players. Technical Note There is a beautiful underlying structure to the graph above. The edge set is given by $${(v, (v + d) \\textrm{ mod } 7):v \\in {0,\\ldots, 6}, d \\in {1, 2, 4}}$$ Therefore the vertices beating a given vertex $v$ are $${(v - d) \\textrm{ mod } 7:d \\in {1, 2, 4}}$$ Following on from this, if we wish to find a vertex, $u$, beating both $v$ and $w$, we seek a value $$u \\in V \\cap W$$ where $$V = {(v - d_v) \\textrm{ mod } 7:d_v \\in {1, 2, 4}}$$ and $$W = {(w - d_w) \\textrm{ mod } 7:d_w \\in {1, 2, 4}}$$ In other words, $u$ will be a solution to $$v - d_v = w - d_w \\quad (\\textrm{mod } 7) \\quad d_v, d_w \\in {1, 2, 4} $$ That is $$d_v - d_w = v - w \\quad (\\textrm{mod } 7) \\quad d_v, d_w \\in {1, 2, 4} $$ But $(v-w) \\textrm{ mod } 7$ is just some integer between zero and six. It is therefore not hard to verify that this equation can be satisfied by choosing $d_v$ and $d_w$ wisely. For example, if $(v-w) \\textrm{ mod } 7 = 5$, we would choose $d_v = 2$ and $d_w = 4$. This is not a constructive proof but it does show that the above tournament is indeed 2-paradoxical. That’s great! We know that we can find a 2-paradoxical tournament with seven players, but what about proving that this is the smallest such example. As can be the nature of many graph theory problems, proving lack of existence can be much tougher than finding a clever example. Never-the-less, here is the simplest proof I could come up with, aided by the following graphic. The method I intend to use to solve this problem is proof by exhaustion. Perhaps there is a more elegant and concise path of attack but if so, it eludes me. If any reader has a better suggestion of how this can be proved, please get in touch. We start with graph A in which we arbitrarily orient the three edges between players 0 and 1, 2 and 3, 4 and 5. We can do this without loss of generality since the players are indistinguishable so we could simply re-label at this point. We could continue to arbitrarily orient edges such as the one between vertices 1 and 3 since we could then switch the labels of the pairs $(0,1)$ and $(2,3)$ to enforce generality, but this ends up making the case analysis more complicated. We now choose a vertex which will beat the 2-subset ${0, 1}$. Since the pairs $(2,3)$ and $(4, 5)$ are indistinguishable, without loss of generality we can choose this dominating vertex to either by 2 or 3. In the case where we choose vertex 2, we are left with graph B. At this point, we are done since it is impossible for any vertex to beat both players 2 and 4. This is because 2 beats all players but 4 and 5 and the only possible saviour, player 5, beats player 4. We can now turn our attention to the other more complicated case. If we had instead chosen vertex 3, we would reach graph C. Unfortunately, we cannot stop here as there is still plenty of choice over how we orient our edges. We now consider the two subset ${2, 3}$ and try to find a vertex that can beat both of them. Since player 3 beats both player 0 and 1, we are only left with the choice of player 4 or 5. Suppose we select vertex 4, this gives rise to graph D. We are lucky this time and we can immediately handle this case. This is because there is no player that could possibly beat both players 3 and 4. Indeed player 3 beats both of players 0 and 1, and player 4 does the same for 2 and 5. If we had instead chosen vertex 5, we would be left with graph E. Every choice we make is now forced, but we still have to make a few additions before we reach a contradiction. First we need a vertex which beats both players 3 and 4. This can only be player 2 so we add a yellow edge to denote this. We next observe that the only vertex that can beat both players 0 and 2 is player 5 and so we add the purple edge. Lastly, we spot that the only player that can beat both players 4 and 5 is player 1, giving rise to the addition of the last 2 pink edges. After all additions have been made, we attain graph F. At this point we observe that no player can beat both players 1 and 5, and so we are done. This also proves that no 2-paradoxical tournament exists for less than 6 players and that no $k$-paradoxical tournament exists for 6 players for any $k\\geq2$. I will leave the reader to ponder why these statements are in fact true. Diving DeepNow that we have investigated the behaviour of 1-and-2-paradoxical tournaments, we can turn our attention to larger $k$. When researching the more general case, it surprised me how little material on this topic is available online. All Wikipedia has to say on the topic is a single paragraph within the page on general tournaments in graph theory. Further, my two favourite combinatorics textbooks—Combinatorics and Graph Theory (Harris) and Foundations of Combinatorics with Applications (Bender, Williamson)—make no mention of it. There are a handful of research papers on the topic, though these prefer the name Schutte tournament to paradoxical, yet these are quite dense. My plan is to walk through some strategies on how we can handle general $k$-paradoxical tournaments and try to produce a coherent introductory guide to their study. A Brute Force StrategyI am not the type to shy away from leveraging computation to solve a difficult problem. This, however, is far from a suitable problem for using brute force. Consider the 7-player tournament above. It just so happens to be 2-paradoxical but that is only because it was carefully chosen. What if we were to approach the problem by generating all possible tournaments and checking whether they were 2-paradoxical or, more generally, $k$-paradoxical? For our 7 player example there are 21 edges. You are welcome to count them but a simpler method to come up with this number is to note that each edge can be constructed by picking one of the 7 vertices to start on and then choosing one of the 6 remaining vertices to connect it to. This gives us 42 possible edges but since the edge ${0,1}$ is the same as ${1,0}$, we need to divide by two to get the true number. We then have a choice of two orientations for each edge, and so the total number of tournaments is $2^{21} = 2097152$. That is a lot of cases to check, and for each one, proving whether or not a given tournament is $k$-paradoxical is no easy feat. On a powerful computer it would most likely be tractable to find the smallest 2-paradoxical tournament but for larger $k$ we stand no chance. For a general tournament on $n$ vertices, we can use the argument above to show that there are $\\frac{n(n-1)}{2}$ possibilities. We could reduce this slightly by using some symmetry arguments but we will not be able to change the fact that this is $O(n^2)$. Therefore when we exponent this we get something of the order $O(2^{n^2})$ which will quickly become intractable for larger n. For example, by the time we have a tournament with 25 players, there will be more possibilities than atoms in the observable universe. We will later see that this would be enough to find a 3-paradoxical tournament but not 4-paradoxical. We need a new strategy A Constructive ApproachThe next thing we will look at is constructive method for finding an upper bound on the smallest tournament that is $k$-paradoxical. This was formulated by Graham and Spencer in 1971[src] and is known as a Paley tournament. In order to look into that we need to take a short tangent into the world of number theory. We say that an integer $a$ is a quadratic residue modulo $n$ if an $x$ exists such that $a\\equiv x^2 (\\textrm{mod } n)$. Let $p$ be a prime such that $p \\equiv 3 , (\\textrm{mod } 4)$. We define the edges of a tournament with vertex set ${0, 1, \\ldots, p-1}$ by directing an edge from $i$ to $j$ if and only if $i-j$ is a quadratic residue of $p$. It can be shown that since $p \\equiv 3 , (\\textrm{mod } 4)$, exactly one of $i$ and and $p-i$ is a quadratic residue of $p$ and so the tournament is well-defined. Graham and Spencer proved that whenever $p$ is chosen such that $p &gt; k^2 2^{2k-2}$ and $p \\equiv 3 , (\\textrm{mod } 4)$ as before, then the Paley tournament produced be the process above is $k$-paradoxical. The first prime satisfying $p &gt; k^2 2^{2k-2}$ and $p \\equiv 3 ,(\\textrm{mod } 4)$ for $k = 2$ is $19$ and for $k=3$ it is $151$ These are certainly not optimum solutions but they do offer an upper bound. More precisely, the bound has $O(k^2 4^{k})$ players. The proof of this requires a strong knowledge of number theory and we’ve already way too far from home (combinatorics) so I will direct any curious reader to the original paper on the topic. Tightening BoundsWe can use the idea of quadratic residues to construct smaller examples. The above theorem will be of no use if we don’t satisfy its conditions but we can use the concept to construct good ‘guesses’ for paradoxical tournaments and then verify them using computation. The quadratic residues of 7 are ${1,2,4}$. If you recall from above, these were the sizes of steps that we connected together in to construct the 2-paradoxical tournament above. Meanwhile, the quadratic residues of 19 are ${1, 4, 5, 6, 7, 9, 11, 16, 17}$ so if for each vertex $v$ we add a directed edge from $v$ to $(v + 1) \\textrm{ mod } 19, \\ldots, (v + 17) \\textrm{ mod } 19$ we obtain the following graph which so happens to be 3-paradoxical. Perhaps not so easy to verify, but this is indeed 3-paradoxical. It also turns out to be the minimal such tournament[src]. Probabilistic MethodsAs we just saw, the bounds obtained using the constructive method are very weak. Perhaps we could do better if we relax the need to find an explicit example. This is where probabilistic methods come in; we can use some stochastic magic to show that, even though we may not be able to find it, at least on tournament of a certain size $n$ is $k$-paradoxical. This method is based on a paper by Erdős published in 1963[src]. Before we reach a final value of n for which we can be sure that a $k$-paradoxical tournament exists, we need to proof an intermediate theorem: Theorem Let $k,n\\in\\mathbb{N}$, with $k\\geq2$ and ${n \\choose k}(1-2^{-k})^{n-k}&lt;1$. Then there exists a $k$-paradoxical tournament with $n$ vertices. The proof of this has a distinctly probabilistic flavour to it. First we consider a random tournament on $n$ vertices where $n$ is chosen such that the assumptions of the theorem hold. We then fix a subset of size $k$, $K$, and let $A_K$ denote the random event that there is no vertex beating all elements in $K$. Since edges are oriented independently and uniformly, the probability of any specific vertex beating every vertex in $K$ is $2^{-k}$. There the probability of such a vertex not beating every vertex in $K$ is $1-2^{-k}$. Using independence again we see that the probability of no vertex beating all elements in $K$, $\\mathbb{P}(A_K)$ is $(1-2^{-k})^{n-k}$, as there are $n-k$ vertices not in $K$ each of which has probability $1-2^{-k}$ of satisfying the property. Now by sub-additivity (also know as Boole’s inequality) we have that $$\\mathbb{P}\\left(\\bigcup_{|K|=k}A_K\\right) \\leq \\sum_{|K|=k} \\mathbb{P}(A_K) = {n \\choose k} (1-2^{-k})^{n-k}$$ Where the last step comes from the fact that there are $n \\choose k$ ways to choose a k-subset from $n$ vertices. Therefore, by the assumption of the theorem we have $$\\mathbb{P}\\left(\\bigcup_{|K|=k}A_K\\right) \\lt 1$$ Since the probability of the event $\\bigcup_{|K|=k}A_K$ is less than one, its complement, $\\bigcap_{|K|=k}A_K^C$ must occur with non-zero probability. It follows that there must at least one tournament on the $n$ vertices which satisfies $\\bigcap_{|K|=k}A_K^C$, that is, a tournament in which every k-subset has at least one vertex beating all elements of the subset as we required. Note, that since our proof is not constructive, we do not have a way to directly find the $k$-paradoxical tournament. We do however know that one exists. So what bounds does this give us? For $k=2$, trying our know optimum $n = 7$ gives ${7 \\choose 2}\\frac{3}{4}^5 = 4.983\\ldots\\gg 1$. It is actually not until $n = 21$ that the theorem condition is satisfied. This is even worse than the bound of 19 that the constructive method gave us. By $k=3$, however, things have improved. The first $n$ satisfying the inequality is 91, much less than the value of 151 given by the constructive approach. This may suggest that the probabilistic method has a more favourable asymptotic than the constructive approach. We will explore the in more detail in a moment. Limiting BehaviourAn interesting observation is that for any $k&gt;1$, we have that $\\lim_{n\\to\\infty}{n \\choose k}(1-2^{-k})^{n-k} = 1$. Indeed $$\\begin{align}{n \\choose k}(1-2^{-k})^{n-k} &amp;\\lt \\frac{n^k}{k!}(1-2^{-k})^{n-k} \\\\&amp;\\lt n^k(1-2^{-k})^{n-k} \\\\&amp;\\leq n^k(1-2^{-k})^{\\frac{n}{2}} &amp; \\textrm{for } n \\geq 2k\\\\&amp;= n^kC^{n} &amp; \\textrm{with } C = \\sqrt{1-2^{-k}} \\lt 1\\\\\\end{align}$$ It is clear that the right hand side tends to zero as $n\\to\\infty$ and so so must the left hand side by the squeeze rule. The implication of this is that ,for any given $k$, if we have a tournament with enough players, it is very likely (or as $n\\to\\infty$, almost certain) that a random tournament of that size will be $k$-paradoxical. It just can’t be avoided! Comparison to Constructive ApproachAs we saw early, the constructive method gives us a $k$-paradoxical tournament with $O(k^2 4^{k})$ players. Does this new probabilistic method improve on this bound? It turns out that it, as we will now see. Take $k&gt;1$ and observe that $$\\begin{align}{n \\choose k}(1-2^{-k})^{n-k} &amp;\\lt \\frac{n^k}{k!}(1-2^{-k})^{n-k} \\\\&amp;\\lt \\frac{n^k}{k!}\\exp\\left(-2^{-k}\\right)^{n-k} &amp; [1+x \\leq e^x] \\\\&amp;= \\frac{n^k}{k!}\\exp\\left(-(n-k)2^{-k}\\right) \\\\&amp;\\lt n^k\\exp\\left(-n 2^{-k}\\right) &amp; [\\textrm{Stirling’s Bound}]\\end{align}$$ Now choose $n &gt; 2^k k^2 \\ln(2+\\epsilon)$ for some $\\epsilon &gt; 0$ and suppose for contradiction that there is no $k$-paradoxical graph on $n$ vertices. By the theorem above we must have that $1 \\leq {n \\choose k}(1-2^{-k})^{n-k}$ and so from the previous working it follows that $$\\begin{align}&amp; 1 \\lt n^k\\exp\\left(-n 2^{-k}\\right) \\\\\\iff &amp; 1 \\lt n\\exp\\left(-nk 2^{-k}\\right) \\\\\\iff &amp; 1 \\lt n^{\\frac{1}{n}}\\exp\\left(-k 2^{-k}\\right) \\\\\\iff &amp; \\exp\\left(k 2^{-k}\\right) \\lt n^{\\frac{1}{n}} \\\\\\iff &amp; \\frac{1}{k2^k} \\lt \\frac{\\ln n}{n}\\end{align}$$ Now,, since $\\frac{\\ln n}{n}$ is decreasing for $n\\geq3$ and, by assumption, $n &gt; 2^k k^2 \\ln(2+\\epsilon) \\geq 3$, for $k&gt;1$ we have that $$\\frac{1}{k2^k} \\lt \\frac{\\ln q}{q} \\qquad \\textrm{with } q = 2^k k^2 \\ln(2+\\epsilon)$$ That is to say that $$k\\ln(2 + \\epsilon) \\lt \\ln(2^k k^2 \\ln(2+\\epsilon))$$ Which is $$\\ln(2 + \\epsilon) \\lt \\ln(2) + \\frac{1}{k}\\ln(k^2 \\ln(2+\\epsilon))$$ This however will not hold for any $\\epsilon&gt;0$ if we choose $k$ to be large enough. Therefore, for sufficiently large $k$, whenever $n &gt; 2^k k^2 \\ln(2+\\epsilon)$, we will have at least one $k$-paradoxical tournament on $n$ vertices. It follows from this that there exists a $k$-paradoxical tournament with $O(k^2 2^k)$ players. This is a massive improvement on the $O(k^2 4^k)$ players that we had before. In fact, it is almost the square root of the original complexity Lower BoundsSo far we have taken a look at some upper bounds, that is, how many players we need before it is certain that we can find a $k$-paradoxical tournament. But what of lower bounds? What is the minimum number of players that we require before a $k$-paradoxical tournament could even be possible. An inductive argument can show that we must have at least $2^{k+1} - 1$ players for a tournament to be $k$-paradoxical. It goes like this. We know that this inequality holds for $k=1$ — a tournament with less than $2^2-1=3$ players certainly can’t be paradoxical. We proceed inductively. Suppose that the statement holds for $k = m-1$ for some integer $m &gt; 1$; we want to show that, under this assumption, it also holds for $k=m$. Suppose, for contradiction, that this is not the case. That means that we have a graph $G^{(n)}$ which is $m$-paradoxical but with $n \\leq 2^{m+1} - 2$. For each vertex $v$ of $G^{(n)}$, let $G^{(n)}(v)$ denote the set of starting points of all edges of $G^{(n)}$ which end at $v$, that is, the set of vertices which are beating $v$. As we showed in ‘A Brute Force Strategy’, $G^{(n)}$ has $\\frac{1}{2}n(n-1)$ edges, so at least one of $G^{(n)}(v)$ must have $\\frac{1}{2}(n-1)$ elements or fewer by the Pigeonhole Principle. Let $v^$ be such a vertex with $G^{(n)}(v^)=:N\\leq\\frac{1}{2}(n-1)$. Since we assumed that $n \\leq 2^{m+1} -2$, it follows that $N \\leq 2^m - 2$. Remember, $N$, is an integer so $N \\leq 2^m - \\frac{3}{2} \\implies N \\leq 2^m - 2$. We now have two possibilities to look at: $N \\geq m - 1$ $N \\lt m -1$ We start with the first case. Our plan of attack is to show that $G^{(n)}(v^*)$ is $(m-1)$-paradoxical. It then would follow from our inductive hypothesis that $N \\geq 2^m - 1$ which contradicts that $N \\leq 2^m -2$. Let $V$ be any set of any $m-1$ elements of $G^{(n)}(v^)$. Since $G^{(n)}$ is $m$-paradoxical by assumption, and $V \\cup v^$ is of size $n$, we must be able to find a vertex $w$ which beats all of $V \\cup v^$. In particular, this means that $w$ beats $v^$ and so $w \\in G^{(n)}(v^)$ by definition. Since this is true for arbitrary $V$, we have that $G^{(n)}(v^)$ is $(m-1)$-paradoxical, giving the contradiction mentioned above. We now turn our attention to the second case. Take $G^{(n)}(v^)$ and add any $m-1-N$ vertices from $G^{(n)}$, excluding $v^$, to obtain an $m-1$ vertex subgraph $G^{(m-1)}$. This subgraph will be $(m-1)$-paradoxical. Why? Let’s see: Consider the vertices of $G^{(m-1)}$ along with $v^$. This forms a set of $m$ vertices in $G^{(n)}$ and since we have assumed that this is $m$-paradoxical, we can find $w$ as before that beats all of $G^{(m-1)}$ and $v^$. Using the same argument as before, $w\\in G^{(m-1)}$. But this is impossible since $G^{(m-1)}$ has $m-1$ vertices all of which are beaten by $w \\in G^{(m-1)}$, implying the existence an $m$th vertex. Either way, we arrive at a contradiction. Hence it must be the case that $n \\gt 2^{m+1} - 2$ and we are done. Improving BoundsThis bound was improved by Esther and George Szekeres in 1965 to $(k+2)2^{k-1}-1$[src]. But this blog post is already very long so we will leave it their and move on to summarising these results. SummaryWe have covered an incredible amount of material on $k$-paradoxical tournaments in just one post so I suggest that it would be beneficial to summarise all of the results we have arrived at in a table. table { width: 100% !important; } th { text-align: center !important; } k Erdos LB Szekeres LB Optimum Erdos UB Graham UB 1 3 2 3 3 3 2 7 7 7 21 19 3 15 19 19 91 151 4 31 47 ? 149 1031 5 63 111 ? 353 6427 As eluded to in the table, we are still yet to have found the optimal values of $n$ for $k$ bigger than 3. Looking at the Szekeres Lower Bound is it is clear that using traditional computing, this problem is completely intractable for a brute force approach. Never-the-less breakthroughs in the field could open up new possibilities but for now, it’s probably best to close your console and get back to pen and paper if you really want to make any progress. At this point, I would normally suggest further reading, but frankly, we have exhausted almost all of what the literature has to offer. Perhaps the next step would be to have a look at the original source documents that I have used to write this post (using ctrl-F to search for ‘src’ will help you find those).","link":"/paradoxical-tournaments/"},{"title":"Bank Holiday Bodge: Parametric Snowflakes","text":"I guess I should have seen this coming, but I still feel rather surprised to be writing another post so soon. Yesterday, I released a blog post showing how the matplotlib.animation Python module can be used to generate a beautiful Christmas tree animation. Why should Python get all the Christmas love though? Well, to compensate, today’s Bank Holiday Bodge will be written in R using a combination of shiny and ggvis. For those unfamiliar with the format, as of the start of the 2019/20 academic year, I decided to spend each bank holiday working on a project from scratch, writing a blog post to match and releasing the whole thing on the same day. This has been a real challenge and has pushed my coding and writing efficiency to its limit. On the other hand, I’ve found it highly satisfying to occasionally through out pieces of work that may not be completely polished but are still interesting to me. Today’s project was a very short one; I feel bad enough doing any coding on such a family-focused day, but I found time to sneak this one in. The result of my work is a simple web app which allows a user to draw snowflakes by altering three parameters: stretch, twist, and wiggle. Furthermore, the colours used in the drawing as well as the line thickness and length can be controlled. The final product can be found here and the source code is located here If we let $\\alpha$, $\\beta$, and $\\gamma$ represent stretch, twist, and wiggle respectively, then the parametric equations for a snowflake are given by: $$x = \\sin\\left(\\frac{t}{2}\\right)-\\alpha \\sin\\left(\\beta t\\right)\\cos\\left(t\\right)-\\frac{\\gamma}{10}\\sin\\left(10\\beta t\\right) \\\\y = \\cos\\left(\\frac{t}{2}\\right)-\\alpha \\sin\\left(\\beta t\\right)\\sin\\left(t\\right)-\\frac{\\gamma}{10}\\cos\\left(10\\beta t\\right) \\\\0 \\leq t \\leq 4 \\pi ,, \\alpha \\in (-1.5, 1.5), , \\beta \\in {3, 4, \\ldots, 15}, , \\gamma \\in (0, 1.5)$$ The contents of the code are pretty standard except for the use of ggvis over ggplot2. ggvis is a package still lacking maturity. It was brought into the world many years ago with the hope of replacing ggplot2. The beauty of the package is that it plots directly to a web canvas, making animation and interactivity with the resultant graphics simple to implement and smooth to view. Unfortunately, support for the package has slowly faded and it currently sits in limbo waiting for the right developers to take it where it needs to go. For now, it still has many useful features (although the difficultly of implementing these due to the lack-luster documentation makes it questionable whether it is worth the effort). I decided to use this package so that I could easier animate between snowflakes when the shape parameters were changed. I am happy with the result although I would have liked to make the transitions smoother if I had time. Anyway, have a play around and I will see you on New Year’s Day!","link":"/parametric-snowflakes/"},{"title":"A Polish Approach to Countdown","text":"The post was originally posted on Warwick Data Science Society‘s Research Blog. You can find the source at this link. Last night, to celebrate the end of term one, WDSS ran a social based on the classic British gameshow Countdown. Countdown consists of two games—focused on numeracy and literacy, respectively—as well as a bonus condundrum. For this post we’re going to take a look at the numbers game, and see how we can use some relatively basic Python coding to build an automatic solver for the problem. The motivation for writing this post (on top of it just being fun to do some Pythonic problem-solving) is two fold: A solution to the Countdown numbers game can be created using the concepts taught in WDSS’s Beginner’s Python course. This makes a nice example of how we can apply the learnings from the course to a real problem. Specifically, we will be manipulating lists (session 5), writing functions (session 6), and using a built-in Python library for benchmarking our solution (session 8). The main personal appeal of this problem, is that there is a clever way of looking at it which makes deriving a solution much simpler. We will talk about this more in the body of the post, but the key idea is that, when programming, the way you choose to represent the problem makes a massive impact on the complexity of your solution. This idea is vital for completing coding assessments for job applications or for competing in coding competitions (such as WDSS’s 12 Days of Python—coming soon, so keep your eyes on our socials!). Before we start to build our solution, let’s quickly recap the rules of the Countdown numbers game: Six numbers are chosen at random for the contestants to use. A three-digit target number is chosen at random. The contestants are given 30 seconds to get as close to the target number as possible using only the operators $+$, $-$, $\\times$, $\\div$. Not all numbers need to be used, and each number can only be used once At all times, the running total must be a non-negative integer Points are then awarded to each contestant based on how close they came to the target number. The rules above are deliberately brief. For the full set of rules see here. As an example, the six available numbers might be $50, 25, 4, 6, 2, 9$ with a target of $303$. A solution could then be $$\\begin{aligned}50 + 25 &amp;= 75 \\\\75 \\times 4 &amp;= 300 \\\\6 \\div 2 &amp;= 3 \\\\300 + 3 &amp;= 303\\end{aligned}$$ Notice that we never used the $9$, but this is allowed. We could also represent this solution as a single expression. $$(50 + 25) \\times 4 + (6 \\div 2) = 303$$ That said, it is often easier to think of the problem by looking at the running total. Brackets BegoneAlthough working through the vast possible combinations of numbers and operations to reach the target is a difficult task for a human, this is actually a fairly lightweight task for a modern computer. Even if we take a brute force approach, simply testing every possible way of arranging the numbers, we will still only need a matter of seconds to reach a solution. That said, such a brute force solution will not scale way. Add more operators and increase the amount of numbers available and the solution’s runtime will grow exponentially, quickly becoming intractable even on the fastest computing hardware. For our use case though, restricted to the standard rules of Countdown, brute force will do just fine. With that in mind, we have the following approach to the problem: Run through every possible solution Check that the solution is valid (no negatives/fractions) Record how far the solution was to the target Print out the best solution This sounds simple enough, but how exactly do we run through every single possible solution? If we jumped at this problem before pausing to think, we might start by trying to iterate through all expressions that look like the one we had above ($(50 + 25) \\times 4 + (6 \\div 2)$). This would certainly be a valid approach, but it would also be nightmarish to code up. The reason for this, is that there is an implicit restriction that our brackets must match. Furthermore, this is not a simple rule: Brackets must be opened before being closed By the end of the expression, all brackets must be closed Brackets must contain a meaningful expression (i.e. we would want to bracket a single number) Even if we could somehow find the energy to create such valid bracketed expressions, we would still have the issue of having to determine when brackets add value; in many cases, brackets are redundant and so we would be wasting time checking equivalent solutions multiple times. It may seem at this point that we’ve reached a dead end. Either we have to give up or face the complexities of efficiently iterating over these expressions. Thankfully, though, there is an alternative solution which will drastically simplify our approach. Polish NotationFrom our early years we start to build up a sense of what maths looks like. It’s always the same pattern: take two numbers, slap an operator between them, and you’re done. Obviously, there are many elements of mathematical notation that do not follow this template, but it is certainly true for lowest level building blocks of addition, subtraction, multiplication and division. We are so used to this way of performing arithmetic, that it may seem strange to even consider that there could be another way. Nevertheless, there is, and it has some remarkable properties that will make you question why we even bothered with our usual notation in the first place. This new mathematical system is known as Polish notation, named for the nationality of logician Jan Łukasiewicz, who invented the method in 1924. The difference between this system and our standard approach is that we always put the operator before the two values it acts on, rather than in between. For example, what was $$3 + 4$$ in our old system would become $$+ \\, 3 \\, 4$$ in Polish notation, the same going for the other standard arithmetic operators. Our standard notation has a name too—infix notation—arising from how the operator sits inbetween the two operands. Polish notation is also called prefix notation (since the operator prefixes the operands) and there is also a variant called reverse Polish (postfix) notation in which the operator follows the operands. We can also use multiple operators in a Polish notation expression. In this case, we evaluate them one at a time. $$\\begin{aligned}&amp;\\times , 5 , + , 3 , 4 \\\\=&amp;, \\times , 5 , 7 \\\\=&amp;, 35\\end{aligned}$$ Notice how we knew to evaluate the addition before multiplication as this was the only operator followed by two numbers. Once we evaluated $+ , 3 , 4 = 7$, the multiplication operator becomes followed by two numbers and so we can evaluate that too. In looking at this example, we just discovered what makes Polish notation so powerful. It may at first seem like this new approach adds nothing but confusion to our arithmetic, but it in fact comes with an incredible benefit: we never need to use brackets! It may be hard to wrap your head around why this is the case, but it is true. Polish notation has the special property that it is never ambiguous. Whereas an expression such as $1 + 2 \\times 3$ in our standard notation could be interpreted as $(1 + 2) \\times 3$ or $1 + (2 \\times 3)$, Polish notation never has this ambiguity and so we can completely remove the need to consider brackets. This is the secret ingredient to our Countdown numbers game solution, allowing us to enumerate all possible solutions without worrying about bracketing rules, or duplicating equivalent cases. Building a SolverThe AlgorithmAlthough it is possible to create a solver that simply runs through all possible permutations of the 6 numbers and each choice of 5 operators, evaluating each using Polish notation and comparing to the target, there are some quick efficiency gains that we can invest in. That said, even the most basic solution will almost always find a solution in less than a second (requiring around a minute to find all solutions). We improve on this naive approach by considering the skeleton of a solution. This is a term I have coined to refer to the patterns of operators and numbers in a solution. For example a skeleton could be OONNOONONNN, where O represents an operator and N a number. It is simpler conceptually (though admittedly not great for performance) to store a skeleton as a list of positions where we have operators. In this case, the above skeleton would be represented by [0, 1, 4, 5, 7]. This sounds like a lot of work but the benefit is massive. This is because there will be many skeletons that are invalid. We can therefore discard them before trying to fill them with numbers or operators, saving a lot of time in the process. We can check that a skeleton is valid by noting that each operation requires two inputs and returns one. For that reason, we can loop through the skeleton from left to right as we always do with Polish notation, incrementing our number count ever time we see a number, and decreasing it if we see an operator. As long as we never come across an operator whilst we have one or less numbers left, we have a valid skeleton. The CodeWith that in mind, we can look at the following solution to the Countdown numbers game. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import operatorfrom itertools import combinations, permutations, productfrom itertools import combinations_with_replacementfrom more_itertools import distinct_permutationsOPERATORS = { \"+\": operator.add, \"-\": operator.sub, \"*\": operator.mul, \"/\": operator.floordiv,}def is_valid(skeleton): num_count = 0 last_op_pos = -1 for op_pos in skeleton: # All elements between operators must be numbers num_count += op_pos - last_op_pos - 1 # Invalid if not enough numbers for operator if num_count &lt; 2: return False num_count -= 1 last_op_pos = op_pos return Truedef evaluate(sequence, target): # Use stack to keep track of unused numbers stack = [] solutions = [] # Loop through sequence and evaluate result for i, s in enumerate(sequence): if isinstance(s, int): stack.append(s) else: if len(stack) &lt; 2: break n1 = stack.pop(0) n2 = stack.pop(0) # If sequence results in invalid intermediate step, stop evaluation if s == \"-\" and n2 &gt;= n1: break if s == '/' and n1 % n2 != 0: break r = OPERATORS[s](n1, n2) if r == target: solutions.append(sequence[:i+1]) stack.append(r) return solutionsdef distinct_ordered_combinations(iterable, r): seen = set() for combination in combinations_with_replacement(iterable, r): for permutation in permutations(combination): if permutation not in seen: yield permutation seen.add(permutation)def solve(numbers, target): solutions = [] n = len(numbers) sequence = [None for __ in range(2*n-1)] for skeleton in combinations(range(2*n-1), n-1): if is_valid(skeleton): for nums in distinct_permutations(numbers): for ops in distinct_ordered_combinations(OPERATORS, n-1): # Build sequence from skeleton and number/operator choice nums_iter = iter(nums) ops_iter = iter(ops) i = 0 for j in range(2*n-1): if skeleton[i] == j: sequence[j] = next(ops_iter) i += 1 else: sequence[j] = next(nums_iter) new_solutions = evaluate(sequence, target) solutions.extend(new_solutions) return solutions TestingWe can now test our code on an example problem. 1234NUMBERS = [75, 50, 6, 3, 8, 2]TARGET = 513solutions = solve(NUMBERS, TARGET) There is a large number of solutions, many essentially equivalent. 1len(solutions) 5076These solutions are still written in Polish notation. I will leave it as a challenge to the reader to write code to translate these back to our usual infix notation or a set of running total instructions. {% codeblock lang:python %} solutions[0] {% endcodeblock %} [2, 3, &apos;*&apos;, 8, &apos;*&apos;, 50, &apos;+&apos;, 6, &apos;*&apos;, 75, &apos;-&apos;]In this case, however, the solution is: $$\\begin{aligned}2 \\times 3 &amp;= 6 \\\\6 \\times 8 &amp;= 48 \\\\48 + 50 &amp;= 98 \\\\98 \\times 6 &amp;= 588 \\\\588 - 75 &amp;= \\mathbf{513}\\end{aligned}$$ Going BeyondBefore we wrap-up, I will briefly run through a few ways to speed up this solution, allowing it to be used for more general applications. Turbocharging the AlgorithmOur solution is very loop-heavy, with multiple layers of nested for loops. Python is not-renowned for the speed of its loops and so we likely lose a considerable amount of efficiency in doing this. We can largely mitigate this issue by using JIT-compiled Python, which optimises our code before running it to make loops and other slow techniques more efficient. One approach to this is using PyPy a JIT-compiled implementation of Python. Alternatively (and my preference), you can use Numba, a package for JIT-compiling the standard Python implementation. Numba also has great compatibility with NumPy, which could perhaps be used to extract further performance. Guided SearchCurrently, our approach is searching the solution space completely at random. This is a valid approach when we want every solution, but not when we need just one. Instead, we might want to look into an approach that searches for a solution. This could be based on an evolutionary method or by building a sort of tree structure connecting solutions together (perhaps working towards and back from the solution at the same time). These ideas are far beyond the scope of an introductory tutorial, but are worth looking into if this sort of problem interests you. Optimisations and Branch-CuttingFinally, there are still many time-saving tricks we can use with our existing approach. The most pressing optimisation, is to avoid copying memory around. At many points in our code we are moving and creating new memory. It is possible to create an implementation in which we only have one state that is modified as we search the space of solutions. We could also avoid identical solutions, perhaps by enforcing that addition and multiplication expressions have their operands in ascending order. In a similar way, we can avoid multiplying by 1, as this does not change the solution. These small changes add up and could lead to an implementation capable of finding all solutions in a matter of seconds.","link":"/polish-countdown/"},{"title":"Pymetrics Reenvisioned: Using statistics to balance risk and reward when faced with uncertainty","text":"IntroductionThe New Kid in TownEver since its formation in 2011, pymetrics has taken the recruitment world by storm, attempting to reinvent the way that employers screen their candidates. The prevalence of this company is seen most in the application process for summer internships. Partnered with the likes of Unilever, Accenture, Mercer, and JPMorgan and receiving $40 million in its series B funding round, pymetrics is certainly making its mark on the job application scene. The philosophy of pymetrics revolves around leveraging neuroscience, AI, and behavioural science to create a bias-free and effective means of matching candidates with recruiters. Rather than your typical numerical literacy or situational awareness tests, pymetrics prides itself on using unconventional ‘games’ to access a candidate’s behaviour and abilities. Pymetrics claims that its games focus less on assessing the skill-level of a candidate and more on determining how well they fit the job role that they are applying for. Although I believe this for the majority of their games, there are certain games where it is clear that there is an optimal way of playing. In this post we will take a look at one such game in which you have to manage risk in the face of uncertainty. We will reenvision the game through the lens of statistics and data science to gain some insight into how we can solve it effectively. If you are after a way to cheat the system and improve your test score, this post is not for you; if however you are curious about some of the ideas that can be applied to this game as well as many other challenges that require the management of risk, read on, there is a lot in store. Credit I would like to thank my friend Ciaran Evans for pointing out the interesting ideas that pymetrics have been implementing. It was on his advice that I decided to look in more detail at how pymetrics works and what sort of games their assessments are composed of. Bandits!In this post, we are going to be looking at a particular game from the dozen used by JPMorgan in recruiting for their 2020 summer analyst programme. The setup is as follows: There are four decks of cards (with a large, unknown number of cards) laid face-down next to each other on a table You repeatedly choose one of these four decks and draw the top card Each card has a monetary value associated with it. It could be +£200, -£600 or +£0. You continue this process, free to choose a different deck at any point until the game terminates at some point unknown to you Simple enough. Once you begin to experiment with your choice of deck, you will quickly notice that each one has a distinctly different behaviour. Perhaps one only gives small positive returns but appears to do so consistently. There could be another deck with a tendency to offer both large rewards and devastating losses. A final deck may offer steady rewards only to suddenly punish you for your optimism with the nasty introduction of a £2000 loss. This sort of problem also exists outside of the realm of pymetrics games. In the world of statistics and machine learning, it is known as the multi-armed bandit problem. This name comes from imagining a gambler at a row of slot machines (sometimes known as “one-armed bandits”), who has to decide which machine to risk his money on next with the (optimistic) hope that one machine, if he could all but find it, would give him better returns. In particular we are looking at a 4-armed bandit. The multi-armed bandit is an example of reinforcement learning; problems in which an algorithm is designed to make choices and then alter its future decisions based on the response it receives. As is customary in statistics, if we want to tackle this problem we will have to formulate it as a probabilistic model. To do this, we will have to make some assumptions. Although it would be possible to analyse this problem with minimal assumptions—supposing that the arms of the bandit (decks of cards) each have a completely free distribution of rewards—the results we obtain from this are not very insightful, and require large amounts of effort to even reach. Instead, we shall assume that each arm returns rewards that are normally distributed (i.e. a bell-shaped likelihood curve centered around the most likely reward). We will choose each distribution’s mean (centre) and standard deviation (spread) randomly with the value of such parameters unknown to the player. Technical Note Before I annoy any statisticians, I should clarify what I mean by ‘randomly’ selecting the mean and standard deviation. Essentially, I should specify what my priors are. In my model of this problem the means of each distribution are sampled from a standard normal distribution and the standard deviations are sampled from a half-t distribution (also known as a folded-standard-t distribution). There is no exact science behind these choices other than that they seemed reasonable. For most sensible priors (e.g. inverse-gamma for standard deviation) the results shown in this post should hold. As assumptions go, these are very relaxed. Most introductory guides to the multi-armed bandit problem start by analysing what is known as a Bernoulli bandit, where each arm is essentially just a biased coin flip—heads giving a reward of £1 and tails taking away the same amount. It’s no fun to follow the crowd though, so we will jump right into the deep-end by modelling the game above as a 4-armed normal bandit with unknown means and standard deviations. For a particular selection of these parameters, our reward distributions may look something like this. Clarification Each curve shows how likely it is for a given arm (remember, these are decks in our specific example) to offer a specific reward. A higher density means that a certain reward is more likely. For example, the 3rd arm is most likely to return a reward of around 1.9 although it wouldn’t be unlikely for it to give any value between -4 and 8. A DilemmaLooking at the above graph we see what makes this problem so fiendish. When we start playing with a multi-armed bandit, we have no idea what these distributions look like. Only with time and experimentation can we learn the ‘behaviour’ of each arm. The trouble is that the information we obtain through our initial trials may mislead us. Note for example that because arm 1 has such a low standard deviation (narrow peak) it is actually more likely that arms 2-4 will give us a reward below -2 than arm 1 will. If we started by sampling both arms 1 and 2, it wouldn’t be unlikely for our sample from arm 1 to turn out better than arm 2. Despite this, arm 1 is worse on average than arm 2 (it has a lower mean/peak) yet we would not know this from our initial foray. It is important to remember also that experimentation isn’t free. Since the game will eventually end (we have no idea when this will be, but we can at least count on the heat-death of the universe to guarantee it will come) we will only ever be able to earn a finite amount of money and any sub-optimal choices that we made along the way, either through ignorance or desire to experiment, can be looked on with regret. This is how we frame this problem statistically; as an optimisation problem in which we aim to seek a policy (set of rules that determine which arm to use at any given moment) that minimises regret. This may sound like a reasonable objective but taking a closer look will show us that the situation is more nuanced. It all comes down to when we want to minimise regret. After a thousand or so trials we may be pretty confident on the behaviour of all arms but one. We then have a difficult decision to make. We are confident that of the arms we have frequently used, we know which one is best. We could keep using this indefinitely, reaping reasonable rewards. This seems like a sensible approach but there will always be the question of “what if?”. Maybe, just maybe, the other arm we have barely experimented with has a slightly higher reward than our current known best. It could be better by only 10 pence per use but what if the game were to go on for another 1000 turns? Well, those 10 pences add up to a handsome reward. On the other hand, what if the other arm is no good at all? By experimenting with it we are only wasting turns that could have been spent on our known best arm, possibly losing money in the process. This conundrum is known as the explore-exploit dilemma and is a key concept in the general field of reinforcement learning. As hinted at above, there are two conflicting types of policy that can be used to tackle the multi-armed bandit problem. Exploitative policies don’t worry about the “what if?”s. They focus on the current known best and keep applying that strategy till the cows come home. Exploratory policies are much more cautious. They agonise over the chance that there may be a better choice, if they had just known to use it. Even when it is clear which strategy is best, an exploratory policy may occasionally make a seemingly sub-optimal choice just in case the information it gleans from doing so is beneficial for the future. In reality, most successful policies are a mix of these two approaches. Policies are also free to change their behaviour over time, perhaps starting off more exploratory and then becoming more exploitative as the confidence in their own knowledge improves. In general, exploitative policies do best in the short-term whereas exploratory policies can offer long-term benefits provided there is eventually some exploitation to take advantage of the knowledge gained by experimentation. The only way to really know which approaches work best on a given problem is to let them loose. With that said, let’s introduce a handful of bandit solvers and let them try their luck on some simulated data. A Handful of SolutionsKeeping it SimpleFor completion, we will start with two very simple, and equally useless policies. The first is a completely exploratory policy. In other words, blind-guessing. In reality, this policy would never be used although it is good to be introduced as a benchmark to compare other policies to. To make things explicit, we will lay out the single step for this policy: Select one arm out of the $n$ choices each with uniform probability $\\frac{1}{n}$ On the other end of the spectrum, we have the completely exploitative strategy. This will look at all previous rewards from each arm and estimate the mean of each distribution by summing up all rewards and dividing by the number of times that arm was chosen. We denote this for the $k$th arm by $\\hat{\\mu_k}$. We initially set each $\\hat{\\mu_k}$ to be zero. The policy can be explicitly written as follows: Pick arm $k$ to maximise $\\hat{\\mu_k}$ Update $\\hat{\\mu_k}$ based on the response This is, again, a fairly useless strategy. It is essentially equivilant to picking the first arm that looks like it might give a positive average reward and then stubbornly sticking with it as long as the estimate of its mean never drops below zero. We will get round to graphing the performance of these policies on simulated data in just one moment. First though, we will introduce a policy that might actually do a good job. Forgive Me Father For I Have SinnedIt may not be the worst of the Seven Deadly Sins but greed is certainly on the list. So for this next solution, we will throw away our morals (at least partially) to look at the $\\varepsilon$-greedy approach to the multi-armed bandit problem. This policy (or in fact, family of policies) is a simple way to find a mix between the two pure strategies mentioned above. We start by picking a fixed value of $\\varepsilon$ between zero and one, inclusive, and then make each choice by walking through the following steps: Generate a random number uniformly between zero and one If this number is below $\\varepsilon$, select an arm uniformly from all choices (explore) Otherwise, pick the arm which maxmises $\\hat{\\mu_k}$ (exploit) In both cases, update $\\hat{\\mu_k}$ We usually pick $\\varepsilon$ to be small so that most of the time, our policy is exploitative; just every now and again it has an urge to explore. Choosing such an $\\varepsilon$ is a difficult matter. First note that if we have $\\varepsilon=0$ then we have the pure exploitative policy mentioned above. Likewise, $\\varepsilon=1$ gives a purely exploratory approach. For other $\\varepsilon$, we have to strike a balance. In the limit, a smaller $\\varepsilon$ will always be better. This is because eventually (due to law of large numbers) we can be extremely confident that our estimates for the distribution means are correct. Therefore, any further exploration is a waste of time and leads to regret. Despite this, small $\\varepsilon$ policies can struggle in the short-to-medium-term as they may be missing out on a good choice by having not explored it well enough. To illustrate these differences, let’s code up a few $\\varepsilon$-greedy bandits and let them loose on some simulated data. We will use multiple values of $\\varepsilon$ including zero and one and let them run for 2500 steps (trials) for 500 randomly generated normal bandits. We then take the mean regret at each step over all of these bandits. The reason we do this is to make sure that we quantify how well our solvers perform on a typical bandit, which may be different from their performance on a specific one. Despite this, we still include $95%$ confidence intervals on each curve. After a few minutes coding and a lot of fan noise, we get a graphic like this. The results are as we would expect. The pure exploitative and exploratory strategies do not perform well at all, and the best strategy over this time-span appears to be the moderate $\\varepsilon=0.02$. Despite this, we can see that the $\\varepsilon = 0.01$ solver is likely set to overtake all others in the not too distant future, it just takes some time to make up for the regret it developed in the first 1000 steps whilst it was being slow to explore the available choices. From now on, we will continue with just the pure strategies and the $0.02$-greedy policy, so that we can add two more approaches to our list without making our plot too messy. Additional Notes There is actually much more we could have done with the $\\varepsilon$-greedy approach if we had the time. A simple improvement on the policy is known as $\\varepsilon$-first. The idea of this is that we begin our game by making purely exploratory choices for the first, say, $k$ steps and then switch to a $\\varepsilon$-greedy stategy. This allows us to have the small $\\varepsilon$ we desire in the long-run without introducing too much regret in the short-term. Another idea is to use an $\\varepsilon$-decreasing strategy. As the name would suggest, in this we start with a larger $\\varepsilon$ and decrease it with time as our mean estimates become more confident. This lets use balance exploration and exploitation at the times when each our beneficial. Don’t Get CockyThe $\\varepsilon$-greedy approach described above can achieve some reasonable results but it has an obvious oversight. At no point in the policy definition do we ever try to quantify our uncertainty about our current estimates of the distribution means. This can lead to highly inefficient exploration. Imagine for example that we are confident that one distribution is far worse than the others. We may be close to certain of this fact yet when we complete the exploration step (occurring with probability $\\varepsilon$) we have an uniform chance of picking this arm to explore and receiving a metric tonne of regret in response. What we really want to do is favour exploring areas that have a strong potential for reward in the future. This way we can avoid exploring choices which we know are bad and spend more time looking for choices that could end up benefiting us significantly in the long-run. One way of implementing this thought process is using an upper confidence bound (UCB) policy. Rather than choosing an arm which maximises our current estimate of the means, we aim to maximise an upper bound on what this mean could be with reasonable certainty given the information we have already received. There are many types of UCB algorithms which depend on our prior knowledge of what the reward distributions look like. We will keep it simple and use a method based on Hoeffding’s Inequality which requires no assumption of priors. I will summarise the mathematical details of this method in the technical details at the end of this post but for now I will just state the policy without justification: Suppose that on trial $t$, the $k$th arm has been pulled $N_k$ times Pick arm $k$ to maximise $\\hat{\\mu_k} + \\sqrt{\\frac{2\\log t}{N_k}}$ Update $\\hat{\\mu_k}$ Here, $\\hat{\\mu_k}$ is the same mean estimate we had with our purely exploitative method except now we add on the term $\\sqrt{\\frac{2\\log t}{N_k}}$ to quantify the uncertainty of this estimate. We will simulate this policy as before in just a moment. First though, let’s turn to the statistical literature to see what it has to contribute to this problem. Additional Notes Once we assume a prior on the reward distribution, we can step away from using Hoeffding’s Inequality to obtain a Bayesian UCB policy. Provided our prior is accurate, this will perform a considerable amount better than the vanilla approach. I am wary of the length of this post though and a dive into Bayesian statistics might be a step too far so I will refrain from discussing it in more detail at this time, though I am open to going into more detail in a separate post in the future. Bring in the Big GunsThere’s no point reinventing the wheel. In light of that, I decided to have a look at what papers had been published on the topic of multi-armed normal bandits with unknown means and variances. Lo and behold, in 2018, the trio Cowan, Honda, and Katehakis published a paper on this exact topic. The paper is a nice read and does a good job of comparing their policy to other existing methods as well as the Bayesian approach of Thompson Sampling. I appreciate that most don’t have the time to read a 28-page paper on this topic so we will just skip to the policy definition: For the first $3n$ steps, sample each bandit three times For all other steps, pick arm $k$ to maximise $\\hat{\\mu_k} + \\hat{\\sigma^2_k}\\sqrt{t^\\frac{2}{N_k-2} - 1}$ Update $\\hat{\\mu_k}$ and $\\hat{\\sigma^2_k}$ We define $t$ and $N_k$ as in the previous policy and denote $\\hat{\\sigma^2_k}$ to be the maximum likelihood estimator of the $k$th arm’s variance—i.e. our ‘best guess’ at the square of the standard deviation (spread) of each reward distribution (more details in the technical notes). With these two new policies introduced, let’s return to our simulations. As would be expected, both the UCB and CHK policies perform far better than the simplistic approaches. Comparing the UCB and CHK policies is more difficult. The CHK policy takes a while to start performing well. This is because it begins by sampling each arm 3 times which may result in significant regret if one arm is much worse than the others. This exploration process has its benefits though, as the information gained from this process then allows CHK to significantly minimise its future regrets. In the end, the choice between these two efficient solvers comes down to how long you expect the game to last for. Additional Notes As far as I am aware, a Bayesian policy for the priors we have specified has not been recorded in literature. For other priors though, a Bayesian method known as Thompson Sampling can produce strong results. See Cowan-Honda-Katahakis for a discuss with one such suitable prior. ConclusionA Whole New WorldAnd so our whirlwind tour of the multi-armed bandit problem comes to a close. We have looked at a variety of approaches ranging from the naive and simplistic to state-of-the-art. Before I sign off, I would like to have a discussion regarding the applications of the multi-armed bandit problem to other disciplines as well as how reinforcement learning in general is becoming a key part of practical data science. Out of the twelve pymetrics games, this one stood out to me the most due to its similarities with a problem that we have been discussing at my workplace. As some may know, I am currently undertaking a placement year as a data scientist at the drug company AstraZeneca. The nature of this field means that experimentation is often expensive and time-consuming. There are many situations in which we want to optimise a specific property of a compound over a space of tweak-able parameters yet performing a standard grid search as you would in traditional data science is impossible due to the cost. Instead, you have to decide on which experiments to perform in a deliberate manner, balancing the need to explore the entire space to make sure we don’t get stuck in a local extremum, without wasting money searching fruitless areas of the parameter space. In fact, this problem goes much beyond the standard multi-armed bandit problem and begins to venture into the dark depths of reinforcement learning. One difference is that there are often many different types of experimentation that can be performed. On one end, we have access to chemical and statistical models which can be used to make predictions regarding how alterations in the parameter space affect the resultant compound. These models can be much quicker to run than a real experiment and are almost always orders of magnitude cheaper. The problem is that these methods are only approximations, subject to high variability (and perhaps even—eek!—bias). They may be cheap and quick but we can never trust their results as much as we could a lab-based experiment. To make matters even worse, we may have problems that are multi-objective: maximise solubility, whilst keeping melting point about a certain temperature, and make sure that the compound doesn’t become too sticky. Reinforcement learning is certainly a rabbit hole yet the potential it offers is massive in these time and resource scarce scenarios. I hope that this introduction has opened your mind to the possibilities of the field and perhaps inspired you to read further into the topic. Getting TechnicalOkay, now for the nitty-gritty. If you aren’t a fan of mathematics or statistics, it might not be wise to read on. First I want to point out, that the code used to simulate the bandits and solvers in this post, as well as producing the visuals can be found on the GitHub repository for this site. There has been a lot of talk of regret. We can define this rigourously at time $t$ as $$R(t) = \\sum_{i=1}^t \\left( \\mu_{\\theta^*} - \\mu_{\\theta_i}\\right)$$ where $\\theta_i$ is the choice we make at step $i$ and $\\theta^$ is the index of the best arm ($\\theta^ = \\textrm{argmax}_{i\\in[n]} \\mu_i$). Next I would like to take some time to discuss the exact definitions of $\\hat{\\mu_k}$ and $\\hat{\\sigma^2_k}$ and how we can efficiently update them. If we define $r_i$ to be the reward received at time step $i$ we can then define for time step $t$ $$\\hat{\\mu_k} = \\sum_{i=1}^t \\frac{\\mathbb{1}(\\theta_i = k)r_i}{N_k}$$ $$\\hat{\\sigma^2_k} = \\sum_{i=1}^t \\frac{\\mathbb{1}(\\theta_i = k)(r_i - \\hat{\\mu_k})^2}{N_k}$$ where these estimates obviously depend on $t$. Note that the second estimator is biased. As noted in the Cowan-Honda-Katahakis, the paper’s key result still holds when we use the standard unbiased estimator for variance ($N_k-1$ in the denominator). Storing all rewards is inefficient in terms of storage and recalculating our estimates at each step is unthinkably slow. Instead, some messy—though relatively simple—algebra will show us that we can write $$\\hat{\\mu_k}(t + 1) = \\frac{N_k(t) \\hat{\\mu_k}(t) + r_t}{N_k(t) + 1}$$ $$\\hat{\\sigma^2_k}(t + 1) = \\frac{N_k(t) \\hat{\\sigma^2_k}(t) + \\frac{(r_t - \\hat{\\mu_k}(t))^2}{N_k(t)}}{N_k(t) + 1}$$ and so we only need to store the latest mean and variance estimates for each arm as we as the number of times we have used it in order to update our estimates. Full marks for efficiency! Finally, I wanted to discuss how we reached the policy definition for the Hoeffding UCB. Hoeffding’s Inequality applies to i.i.d random variables $X_1,\\ldots, X_n$ and states that the sample mean $\\bar{X_n} = \\frac{1}{n}\\sum_{i=1}^n X_i$ satisfies for any $u&gt;0$, $$\\mathbb{P}\\left[\\mathbb{E}[X] &gt; \\bar{X_n} + u\\right] \\leq e^{-2nu^2}$$ In our case, at time $t$, if we define our upper confidence bound to be $U_k(t)$, we have, $$\\mathbb{P}\\left[\\mu_k &gt; \\hat{\\mu_k}(t) + U_k(t)\\right] \\leq e^{-2N_k(t) U_k(t)^2}$$ We would like to pick a bound so that the likelihood that the true mean is below the sum of the mean estimate plus the upper confidence bound is small. In other words we would like a small $e^{-2tU_k(t)^2}$. Let this small probability be $p$ and we can rewrite, $$e^{-2tU_k(t)^2} = p \\iff U_k{t} = \\sqrt{\\frac{-\\log p}{2N_k(t)}}$$ It would make sense to reduce the threshold $p$ with time so that our bounds become more confident as we observe more rewards. In the example above we chose $p = t^{-4}$ which gives $\\hat{\\mu_k} + \\sqrt{\\frac{2\\log t}{N_k}}$ as we saw above.","link":"/pymetrics-reenvisioned/"},{"title":"Efficiently Removing Zero Variance Columns (An Introduction to Benchmarking)","text":"Packages required for this post: 12library(rbenchmark) # for benchmarkinglibrary(readr) # for reading CSVs The Issue With Zero Variance ColumnsIntroductionWhenever you have a column in a data frame with only one distinct value, that column will have zero variance. In fact the reverse is true too; a zero variance column will always have exactly one distinct value. The proof of the former statement follows directly from the definition of variance. The proof of the reverse, however, requires some basic knowledge of measure theory - specifically that if the expectation of a non-negative random variable is zero then the random variable is equal to zero. The existance of zero variance columns in a data frame may seem benign and in most cases that is true. There are however several algorithms that will be halted by their presence. An example of such is the use of principle component analysis (or PCA for short). If you are unfamiliar with this technique, I suggest reading through this article by the Analytics Vidhya Content Team which includes a clear explanation of the concept as well as how it can be implemented in R and Python. The MNIST data setLet’s suppose that we wish to perform PCA on the MNIST Handwritten Digit data set. We shall begin by importing a reduced version of the data set from a CSV file and having a quick look at its structure. 1234567# import data setmnist &lt;- read_csv(\"mnist_reduced.csv\", col_names = c(\"Label\", paste0(\"P\", 1:28^2)), col_types = cols(.default = \"i\"))# get the dimensions of the datadim(mnist) 1000 785 12# look at a sample of the predictorshead(mnist[, c(1, sample(1:785, 10))]) $(document).ready( function () {$('#table0').DataTable();} ); LabelP333P419P185P752P18P668P510P361P218P41 &lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; 50025300000 560 000252000001220 400 0000002100 100 000000 620 900 000000 00 20025200030 00 As we can see, the data set is made up of 1000 observations each of which contains 784 pixel values each from 0 to 255. These come from a 28x28 grid representing a drawing of a numerical digit. The label for the digit is given in the first column. We can visualise what the data represents as such. The code used to produce Figure 1 is beyond the scope of this blog post. However, the full code used to produce this document can be found on my Github. An attempt at PCANow that we have an understanding of what our data looks like, we can have a go at applying PCA to it. Luckily for us, base R comes with a built-in function for implementing PCA. 1mnist.pca &lt;- prcomp(mnist[,-1], scale. = TRUE) If we run this, however, we will be faced with the following error message. Error in prcomp.default(mnist[, -1], scale. = TRUE): cannot rescale a constant/zero column to unit varianceThe issue is clearly stated: we can’t run PCA (or least with scaling) whilst our data set still has zero variance columns. We must remove them first. It would be reasonable to ask why we don’t just run PCA without first scaling the data first. In this scenario you may in fact be able to get away with it as all of the predictors are on the same scale (0-255) although even in this case, rescaling may help overcome the biased weighting towards pixels in the centre of the grid. The importance of scaling becomes even more clear when we consider a different data set. For example, one where we are trying to predict the monetary value of a car by it’s MPG and mileage. These predictors are going to be on vastly different scales; the former is almost certainly going to be in the double digits whereas the latter will most likely be 5 or more digits. If we were to preform PCA without scaling, the MPG will completely dominate the results as a unit increase in its value is going to explain far more variance than the same increase in the mileage. Removing scaling is clearly not a workable option in all cases. We are left with the only option of removing these troublesome columns. Removing Zero Variance ColumnsMethods for removing zero variance columnsNote that for the first and last of these methods, we assume that the data frame does not contain any NA values. This can easily be resolved, if that is the case, by adding na.rm = TRUE to the instances of the var(), min(), and max() functions. This will slightly reduce their efficiency. Method 1We can now look at various methods for removing zero variance columns using R. The first off which is the most simple, doing exactly what it says on the tin. 123removeZeroVar1 &lt;- function(df){ df[, sapply(df, var) != 0]} This simply finds which columns of the data frame have a variance of zero and then selects all columns but those to return. The issue with this function is that calculating the variance of many columns is rather computational expensive and so on large data sets this may take a long time to run (see benchmarking section for an exact comparison of efficiency). Method 2We can speed up this process by using the fact that any zero variance column will only contain a single distinct value. This leads us to our second method. 123removeZeroVar2 &lt;- function(df){ df[, sapply(df, function(x) length(unique(x)) &gt; 1)]} This function finds which columns have more than one distinct value and returns a data frame containing only them. Further advantages of this method are that it can run on non-numeric data types such as characters and handle NA values without any tweaks needed. Method 3We can further improve on this method by, again, noting that a column has zero variance if and only if it is constant and hence its minimum and maximum values will be the same. This gives rise to our third method. 123removeZeroVar3 &lt;- function(df){ df[, !sapply(df, function(x) min(x) == max(x))]} Comparing the efficency of our methodsWe now have three different solutions to our zero-variance-removal problem so we need a way of deciding which is the most efficient for use on large data sets. We can do this using benchmarking which we can implement using the rbenchmark package. There are many other packages that can be used for benchmarking. The most popular of which is most likely Manuel Euguster’s benchmark and another common choice is Lars Otto’s Benchmarking. rbenchmark is produced by Wacek Kusnierczyk and stands out in its simplicity - it is composed of a single function which is essentially just a wrapper for system.time(). It is more obscure than the other two packages mentioned but it’s elegance makes it my favourite. Benchmarking with this package is performed using the benchmark() function. This accepts a series of unevaluated expressions as either named or unnamed arguments. It will then produce a data frame giving information about the efficiency of each of the captured expression, the columns of which can be choosen from a comprehensive set of options. The ordering of the rows in the resultant data frame can also be controlled, as well as the number of replications to be used for the test. For more information about this function, see the documentation linked above or use ?benchmark after installing the package from CRAN. We use the benchmarking function as follows. 12345678benchmark( 'Variance Method' = removeZeroVar1(mnist), 'Unique Values Method' = removeZeroVar2(mnist), 'Min-Max Method' = removeZeroVar3(mnist), columns = c(\"test\", \"replications\", \"elapsed\", \"relative\"), order = \"elapsed\", replications = 100) $(document).ready( function () {$('#table1').DataTable();} ); testreplicationselapsedrelative &lt;fct&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt; 3Min-Max Method 1000.141.000 1Variance Method 1000.755.357 2Unique Values Method1001.007.143 As we can see from the resulting table, the best method by far was the min-max method with the unique values and variance method being around 5 and 7 times slower respectively. When we next recieve an unexpected error message critiquing our data frames inclusion of zero variance columns, we’ll now know what do!","link":"/removing-zero-variance-columns/"},{"title":"Bank Holiday Bodge: The Sentiment of Shakespeare","text":"IntroductionBefore I started my commitment to producing a quick blog post every bank holiday, I don’t think I really considered how frequently they come in pairs. Well, I certainly know now, and for that reason, this post will be short and sweet. With the limited free time I have available, I try my best to have at least some understanding of culture. In particular, I have spent the last year trying to work through ‘the classics’. That is, the classic films, books, and even research papers. This has been a colossal challenge as, well, there’s just so much to get through. This got me thinking, my time would be better spent in simply understanding the gist of these cherished pieces. In fact, there are many services devoted to this notion. Take as an example, Blinkist, a company promising to spare your valuable time by presenting popular books in condensed 15-minute discoveries. Maybe though, you don’t even have time for this. Could there be a way to understand the general arc of a story with just one glance? The answer to this is almost certainly no, but that didn’t stop me from having a shot at it anyway. I decided that the most important summarising feature of a book is the overall arc of sentiment—that is, how positive or negative the text is at any point. This, along with some general context of what the book is about, could give you an idea of how turbulent the story is, what the overall mood is, and how we would be left feeling at the end. Admittedly, this is still far from the level of insight you would glean by thoroughly digesting the same book, but in terms of value added per time spent it is a strong candidate. I decided to test this approach using a selection of Shakespeare’s plays. These have the handy feature of being composed of scenes and acts, offering an intuitive way of dividing the visualisation. That is not to say that this approach couldn’t be easily generalised to one long text such as the script of a film—the only change tweak would be to the final type of visualisation. So without further ado (about nothing—eh?) let’s quickly discuss how I implemented such a system and discuss a selection of the resulting visualisations. Sourcing SentimentI do not plan to dissect my source code in detail, but rather link anyone interested to the GitHub repository for this site where they will be able to find. I started by scraping the text of several of Shakespeare’s plays from this site, which contains a large collection of his works. There are other sources that I could have used such as Project Gutenberg which would have also allowed for more general texts, but the site I decided to use made it easy to detect the boundaries of scenes/acts so I kept with that for simplicity. With these texts obtained, I performed some basic cleaning and then used the tidytext package to analyse the overall sentiment (positive or negative) of each scene. I’m not going to go into details regarding exactly how these sentiments are calculated but rather direct curious readers to this talk I gave for Warwick Data Science Society which offers an introduction to sentiment analysis and how it can be used to predict presidential approval ratings in the US. Using these sentiments, it was then straightforward to create a visualisation for the data. I decided to use a waterfall plot as this presents both the overall arc of sentiment as well as scene-to-scene changes. VisualisationA full version of this visualisation can be found here. For now, we will just take a look at one example to see how their sentiments match up with the narrative arc. If you are familiar with Romeo and Juliet, I hope you will agree that this plot quite accurately portrays the overall narrative arc of the play. We start with warring factions, which is then disrupted with hopes of love. This sentiment then grows to reach a climax at the end of the second act with marriage of the pair imminent. Sadly, from then on, it’s only downhill, concluding with a mournful mutual-suicide in the last scene of the play. Without knowledge of the play, picking out those details is admittedly impossible. Despite this, picking out the general turbulence of the story’s sentiment, the climax of happiness in the middle and the eventual sad ending is complete feasible in one glance. Either way, we are left with a pretty visualisation. I hope you enjoyed this post. I would encourage anyone to take the source code for this project (linked above) and apply it to other Shakespeare plays or entirely new sources. Who knows what interesting patterns you may find.","link":"/sentiment-of-shakespeare/"},{"title":"Yet Another Sorting Algorithm Visualisation","text":"I’m sorry. If you’re looking for a completely original, ground-breaking blog post, this one won’t be for you. There are a lot of visualisations of sorting algorithms. And I mean a lot. Ever wondered what a radix sort sounds like? Now you know. Maybe rainbows are more your thing. Here you go. If I still haven’t convinced you, how about performing a quick sort via the medium of Hungarian folk dance? We’ve got that too. I rest my case. Clichés are clichés for a reason though. And frankly, with some many examples already out in the world, what’s the harm in one more. This is the purpose of this blog post; to showcase a sorting algorithm visualisation that I came up with when messing around with the animation module of matplotlib. As is the nature of these visualisations, it turns out that my approach is not completely original even though I did come up with it without seeing anything like it before (great minds think alike, hopefully?). None-the-less, I’ve had a lot of fun writing the code for this project and I love the results so I wanted to share it with the world. I do not plan to go into detail on the code I have used. If you are curious, you can find it here, well-documented. It’s only a few hundred lines to develop the handful of visualisations featured in this post so definitely worth a scan through if your curious about matplotlib.animation or the sorting algorithms featured in this post. Instead, I aim to focus on the lay-reader, and use the pretty visualisations as an incentive to introduce some ideas about algorithms and sorting. I have only included a small selection of common sorting algorithms due to limitations of the matplotlib.animation module (or perhaps just limitations of my understanding of it) so perhaps I will return at some point with a more detailed dive into the topic. For now, let’s get right into it. IntroductionBefore we get onto the visualisations (don’t worry, we’ll get there soon), I want to explain how my visualisations are produced so we can understand how their behaviour relates to the mechanics of the sorting algorithms we use. What’s the Deal with Sorting AlgorithmsFirst though, and you can skip this sounds obvious to you, what is a sorting algorithm? We are going to be looking at lists of numbers. Specifically, we will be looking at lists in which each element is unique. When we are sorting a list, we only care that the elements are in the correct order, not their actual values and so without loss of generality, we can assume that our list contains the elements $0, 1, 2, \\ldots, n-1$ where $n$ is the length of our list. A sorted list of length 5 will look like this: $$[0,1,2,3,4]$$ Whereas an unsorted list may look like this: $$[3, 1, 2, 0, 4]$$ The purpose of a sorting algorithm is to take us from a list like the latter to the former. An algorithm is simply a series of instructions that we are to follow in order to achieve the desired result of a sorted list. There are few rules that we have to follow though. The first is that we can only compare two elements of the list at a time. This may seem very limiting but it makes more sense when we remember that we are writing these algorithms for a computer to use and the architecture of computers is generally limited to these sorts of comparisons. The Visualisation MethodWith the essentials under our belt, let’s take a moment to walk through how my visualisation works. Suppose we have a sorted list of length $n$ containing elements $0,1,\\ldots,n-1$. We create $n$ points in our plot each of which corresponding to an element of the list. We colour these points by the value of the element they represent and arrange them in a circle with their angle from north determined by their value. We then shuffle the list so that the elements’ orders become all mixed up. Now that the elements are out of order, we change the radius of each point in the visualisation so that they correspond with how far off each element is from its sorted position. If by chance an element is shuffled to the same place is was initially, it will retain its original radius and stay on the edge of the circle. If in the other extreme an element is moved so that is a far away as possible from its original position then it will move to the center of the circle. In all other circumstances, points will move to somewhere in between. The important thing is, their colour and angular position will not be affected; just their radial position. As we sort the list we keep updating these radii until the list is sorted again and we return to a perfect circle. To add one more layer of subtlety, we treat the list as cyclic when calculating the distance between a point’s current and initial position. For example if the element $0$ is in position $n-1$ (the last spot in the list) we say that’s its distance from its true position is $1$ and so it will be positioned close to the edge of the circle. Enough introductions though, let’s get to an example. First up: the bubble sort. The Bubble SortThe bubble sort is perhaps the simplest sorting algorithm to conceptualise. The process goes as follows: We move through the list looking at each pair of elements one by one If a given pair is in the correct order then we leave them be If the order of a given pair is incorrect, we swap them around Once we get to the end of the list we go back to the start and repeat the process We continue this until we make no swaps in a given pass through the list, in which case the list must be sorted again Let’s look at a small example to reinforce this explanation. We start with the unsorted list: $$[2, 1, 3, 0]$$ We look at the first two elements ($2$ and $1$) first. These are in the wrong order so we swap them to get: $$[1, 2, 3, 0]$$ The next pair is $2$ and $3$ which happen to be in order so we leave them. We then get to the last pair ($3$ and $0$) which are not in order so we swap them to get: $$[1, 2, 0, 3]$$ This concludes our first pass through the list. We will then repeat this process twice more until the $0$ is in the correct position and the list is sorted. So what does this sorting algorithm look like? Let’s see using a list of length $720$. We start with a sorted list, shuffle it (using Fisher-Yates for those interested) and then proceed to sort it using the bubble sort algorithm. Here is the result: Your browser does not support the video tag. Wow. Who knew computer science could look so good. We can also spot some interesting characteristics of the bubble sort algorithm. The main observation is that after the $k$th pass through the list, the last $k$ elements are sorted. This is shown in the visualisation by the growing arc starting from the top of the circle and moving anti-clockwise at a uniform rate. Why does the bubble sort elicit this property? Think about it this way: In the first pass through the list, once we have the largest element in the pair we are comparing we will always move it to the right of the other value we are comparing to (if it is not there already). We will then continue to swap it, moving it through the list since, by definition, it is larger than any other element in the list. We now have the largest element fixed in the last position of the list. We then pass through again. We pick up the second largest element in a similar way, carrying it through until we compare it to the largest element, at which point we put it down as in the second largest position. Repeat this for each iteration and the pattern described above develops. This behaviour of the current largest, unsorted element ‘bubbling’ up to the end of the list is where the algorithm gets its name from. You may also notice that some points move towards the middle before going back out again. Have a think about why this might be? (Hint: it relates to the cyclic definition of distance we talked about earlier. Suppose the element $0$ gets shuffled to the end of the list; what will its path back be?) The Selection SortWe now move on to our second sorting algorithm. This process is very simple to describe although its terrible efficiency means that it is rarely ever used in practice. None-the-less, it may be interesting to visualise. The algorithm is as follows: Pass through the list and note the smallest element Move this to the start of the list Pass through the list again looking for the second smallest element Move this to the second place in the list Repeat this process until the list is sorted. I won’t give an example of this method as it is quite conceptually simple but I would still like to take a moment before we look at the visualisation to ponder what it might look like. Think through the steps described above and imagine what a typical list will look like at the end of each iteration. More importantly, how will this relate to the position of the points in the animation. Hopefully you’ve had a moment of thought so let’s have a look and see whether you were correct. Your browser does not support the video tag. The Insertion SortThe next sorting algorithm is one that is likely the most commonly used by humans. It is called the insertion sort. The process goes as follows. At each step, suppose that the first $k$ elements are in the correct order (we start with $k=1$ where we only know the first card is in order) Take the $(k+1)$th card and insert it into the correct gap between the first $k$ elements (before the first element or after the $k$th are allowed) Repeat this process until the list is sorted This may seem quite confusing but an example will hopefully clear things up. Let’s start with the unsorted list as follows: $$[3, 0, 1, 2]$$ Vacuously, we know that the first one element(s) is in the correct order. We look at the second element ($0$) and insert it at the start of the list to get $$[0, 3, 1, 2]$$ We now know that the first two elements are sorted correctly. We look at the third ($1$) and insert it into the first two elements so that their order is correct. That is, we insert it between $0$ and $3$. This gives us: $$[0, 1, 3, 2]$$ Finally, we know that the first three elements so we look at the fourth ($2$) and insert it between $1$ and $3$ to complete the sorting process. The corresponding animation for this algorithm looks like this: Your browser does not support the video tag. Cocktail SortBefore we close up, we will look at one more sorting algorithm. This is called the cocktail sort. It is a variant of the bubble sort above but rather than always passing through the list in the increasing direction, the algorithm alternates between increasing and decreasing passes. This forwards and backwards motion is the inspiration for the name of this algorithm. Since this algorithm is so similar to the bubble sort I will hold back from a detailed explanation or example though you can read more here if you are interested. Recalling the behaviour that the bubble sort algorithm induced, we may be able to predict a similar result for the cocktail sort. If not though, let’s just enjoy it: Your browser does not support the video tag. ConclusionWell there you go. A quick introduction to sorting algorithms with some pretty plots to accompany. The computer science savy reader may notice that I have only used iterative sorting algorithms and they would be completely correct in that assessment. This is due to what I believe to be limitations with the matplotlib.animation module in which it doesn’t handle the animation of processes generated using recursion very well. Therefore, any divide and conquer algorithms such as heap, merge, and quick sorts are currently outside what I know how to implement. Maybe there is a way, or maybe additional features will be added to make this easier. Either way, you may well be hearing back from me when the time comes.","link":"/sorting-algorithm-visualisation/"},{"title":"An Analysis of Strange Timezones","text":"This post is going to be code-free to make things easy to follow, but if you are interested in how I mined the data for this project and produced the final visualisation, you can find the code on this site’s GitHub repository. When I was younger, every summer my family and I would pack up our bags and take some time off to holiday in Spain. I can distinctly remember being interested in (and struggling to keep track of) the difference between the time in Spain and the one back home. It seemed strange that I found it so difficult to conceptualise this idea until one day it clicked: despite having flown in a net westward direction to reach our destination, we had somehow changed timezone in an easterly manner. How strange. When we trace back the reasoning for this timezone choice (initiated in 1940 by Franco to align the country with Nazi Germany and Fascist Italy—his political allies) and consider the modern day benefits that the country might glean from being in line with its other western European neighbours, it doesn’t seem unreasonable for things to be this way. If however, you were an alien visiting the Earth for a day lacking any historical, political or socio-economic context—it would seem like quite an odd decision. A Project is BornBefore I knew it, this train of thought was barreling down a hill with burnt-out brakes: “Which country has a timezone that differs the furthest from where it naturally should be based on longitude alone?”. The idea behind this is that in a perfect world (or more accurately, one where I’m in charge) timezones would be entirely dependent on the angle that a country makes with the prime meridian (the imaginary line passing through the Greenwich Royal Observatory). This would leave the UK with its current timezone of UTC+0 and then roughly one hour would be added for each 15 degrees traveled east and subtracted for west. Each country would then choose whichever timezone best matched up with its position. Obviously such an approach would be impractical. It takes into account no ethnic divisions, political allegiances, or many other important factors. Furthermore, it’s not clear how such a system would handle large (or to be exact, wide) countries. It would be ridiculous for a goliath such as Russia to share one common timezone. Despite these clear issues, I’m a mathematician and this is my hypothetical world, so tough luck. With that aside, it begs the question of which countries’ timezones would differ the most between the real world and my mathematically-pristine formulation. I decided to find out just that by scraping both timezone and longitudinal data from Wikipedia. The data represents all recognised countries for which their Wikipedia listings contained the information I required (which was most, but not all). To simplify things slightly, I decided only to look at the capital city of each country and the timezone that it resides in. This way I didn’t have to worry about the calculus of averaging population/land mass or looking at multiple timezones for one country. On top of this, I ignored daylight savings time, although this is probably for the best as it would skew large deviations of timezone towards countries which lie farther from the equator. After performing the analysis, the twenty capitals whose timezone differed the most from my theoretical choice were as follows. There you have it. The worse offending capital city is Rabat, Morocco with a whopping 1 hour 33 minute difference from what would be expected in a purely longitudinal system. The reason for this is that ever since 2018, Morocco has switched to permanently observing daylight saving time making UTC+1 its standard time despite virtually all of the country’s landmass being farther west than even Plymouth. As my younger self would have expected, Spain’s Madrid comes out in a close second, 1 hour 15 minutes ahead off what would be expected. Even after these, there are still many countries with real-world timezones differing significantly from where they mathematically should align. The really interesting observation is that every single one of these twenty worst offenders is ahead of its longitudinal timezone. I thought at first that this might have been a bug in my code and all differences had been converted to being positive. It turns out that this is not the case; there are capitals which fall behind their expected timezone. The five worse of those are shown below. These differences are noticeably smaller. My best guess is that most timezones are ahead of where they should be for the same reason that daylight saving exists—to offer more light in the morning. A colleague of mine suggested instead that it may be to do with the location of capital cities—do these tend to be more easterly? If you have any other suggestions please do let me know. For now, I will leave you with the finalised dataset to have a look through to find whichever country you fancy. $(document).ready( function () {$('#table0').DataTable();} ); capitalcountrylongitudetimezoneexpected_timezonetimezone_diff &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; Abu Dhabi United Arab Emirates 54.367000 4.00 3.62446667 0.375533333 Abuja Nigeria 7.483000 1.00 0.49886667 0.501133333 Accra Ghana -0.200000 0.00 -0.01333333 0.013333333 Addis Ababa Ethiopia 38.740000 3.00 2.58266667 0.417333333 Algiers Algeria 3.058890 1.00 0.20392600 0.796074000 Amman Jordan 35.932780 2.00 2.39551867-0.395518667 Amsterdam Netherlands 4.900000 1.00 0.32666667 0.673333333 Ankara Turkey 32.867000 3.00 2.19113333 0.808866667 Antananarivo Madagascar 47.517000 3.00 3.16780000-0.167800000 Apia Samoa -171.75000013.00-11.45000000 0.450000000 Ashgabat Turkmenistan 58.367000 5.00 3.89113333 1.108866667 Asmara Eritrea 38.925000 3.00 2.59500000 0.405000000 Athens Greece 23.727806 2.00 1.58185373 0.418146267 Baghdad Iraq 44.383000 3.00 2.95886667 0.041133333 Baku Azerbaijan 49.882220 4.00 3.32548133 0.674518667 Bamako Mali -8.002780 0.00 -0.53351867 0.533518667 Bandar Seri Begawan Brunei 114.942220 8.00 7.66281467 0.337185333 Bangkok Thailand 100.494170 7.00 6.69961133 0.300388667 Banjul Gambia -16.577500 0.00 -1.10516667 1.105166667 Basseterre Saint Kitts and Nevis -62.733000-4.00 -4.18220000 0.182200000 Beijing China 116.383000 8.00 7.75886667 0.241133333 Beirut Lebanon 35.513060 2.00 2.36753733-0.367537333 Belgrade Serbia 20.467000 1.00 1.36446667-0.364466667 Belmopan Belize -88.766940-6.00 -5.91779600-0.082204000 Berlin Germany 13.405000 1.00 0.89366667 0.106333333 Bishkek Kyrgyzstan 74.612220 6.00 4.97414800 1.025852000 Bogotá Colombia -74.072220-5.00 -4.93814800-0.061852000 Brasília Brazil -47.882780-3.00 -3.19218533 0.192185333 Bratislava Slovakia 17.109720 1.00 1.14064800-0.140648000 Bridgetown Barbados -59.616670-4.00 -3.97444467-0.025555333 Bucharest Romania 26.103890 2.00 1.74025933 0.259740667 Budapest Hungary 19.051390 1.00 1.27009267-0.270092667 Buenos Aires Argentina -58.381670-3.00 -3.89211133 0.892111333 Cairo Egypt 31.233000 2.00 2.08220000-0.082200000 Caracas Venezuela -66.903610-4.00 -4.46024067 0.460240667 Castries Saint Lucia -60.983000-4.00 -4.06553333 0.065533333 Chi&lt;U+0219&gt;inau Moldova 28.835335 2.00 1.92235567 0.077644333 Conakry Guinea -13.712220 0.00 -0.91414800 0.914148000 Copenhagen Denmark 12.568330 1.00 0.83788867 0.162111333 Dakar Senegal -17.446670 0.00 -1.16311133 1.163111333 Damascus Syria 36.291940 2.00 2.41946267-0.419462667 Dhaka Bangladesh 90.388890 6.00 6.02592600-0.025926000 Dili East Timor 125.567000 9.00 8.37113333 0.628866667 Djibouti Djibouti 43.145000 3.00 2.87633333 0.123666667 Dodoma Tanzania 35.741940 3.00 2.38279600 0.617204000 Doha Qatar 51.533330 3.00 3.43555533-0.435555333 Dublin Ireland -6.267000 0.00 -0.41780000 0.417800000 Dushanbe Tajikistan 68.780000 5.00 4.58533333 0.414666667 Gaborone Botswana 25.912220 2.00 1.72748133 0.272518667 Georgetown Guyana -58.155280-4.00 -3.87701867-0.122981333 Guatemala City Guatemala -90.535280-6.00 -6.03568533 0.035685333 Hanoi Vietnam 105.854170 7.00 7.05694467-0.056944667 Harare Zimbabwe 31.052220 2.00 2.07014800-0.070148000 Havana Cuba -82.358890-5.00 -5.49059267 0.490592667 Helsinki Finland 24.937500 2.00 1.66250000 0.337500000 Honiara Solomon Islands 159.95556011.00 10.66370400 0.336296000 Islamabad Pakistan 73.063890 5.00 4.87092600 0.129074000 Jakarta Indonesia 106.817000 7.00 7.12113333-0.121133333 Jerusalem Israel 35.217000 2.00 2.34780000-0.347800000 Jerusalem Palestine 35.217000 2.00 2.34780000-0.347800000 Juba South Sudan 31.600000 3.00 2.10666667 0.893333333 Kabul Afghanistan 69.178330 4.50 4.61188867-0.111888667 Kampala Uganda 32.581110 3.00 2.17207400 0.827926000 Kathmandu Nepal 85.267000 5.75 5.68446667 0.065533333 Khartoum Sudan 32.560000 2.00 2.17066667-0.170666667 Kiev Ukraine 30.523330 2.00 2.03488867-0.034888667 Kigali Rwanda 30.059440 2.00 2.00396267-0.003962667 Kingston Jamaica -76.793060-5.00 -5.11953733 0.119537333 Kingstown Saint Vincent and the Grenadines -61.225000-4.00 -4.08166667 0.081666667 Kuala Lumpur Malaysia 101.695280 8.00 6.77968533 1.220314667 Kuwait City Kuwait 47.978330 3.00 3.19855533-0.198555333 Lilongwe Malawi 33.783000 2.00 2.25220000-0.252200000 Lima Peru -77.033000-5.00 -5.13553333 0.135533333 Lisbon Portugal -9.150019 0.00 -0.61000129 0.610001287 Ljubljana Slovenia 14.508330 1.00 0.96722200 0.032778000 Lomé Togo 1.222780 0.00 0.08151867-0.081518667 London United Kingdom -0.127500 0.00 -0.00850000 0.008500000 Lusaka Zambia 28.283000 2.00 1.88553333 0.114466667 Luxembourg Luxembourg 6.131940 1.00 0.40879600 0.591204000 Madrid Spain -3.717000 1.00 -0.24780000 1.247800000 Majuro Marshall Islands 171.38300012.00 11.42553333 0.574466667 Malabo Equatorial Guinea 8.783000 1.00 0.58553333 0.414466667 Malé Maldives 73.508890 5.00 4.90059267 0.099407333 Managua Nicaragua -86.251390-6.00 -5.75009267-0.249907333 Manila Philippines 120.977200 8.00 8.06514667-0.065146667 Maputo Mozambique 32.583000 2.00 2.17220000-0.172200000 Maseru Lesotho 27.480000 2.00 1.83200000 0.168000000 Mexico City Mexico -99.133000-6.00 -6.60886667 0.608866667 Minsk Belarus 27.567000 3.00 1.83780000 1.162200000 Mogadishu Somalia 45.333000 3.00 3.02220000-0.022200000 Monaco Monaco 7.417000 1.00 0.49446667 0.505533333 Monrovia Liberia -10.801390 0.00 -0.72009267 0.720092667 Montevideo Uruguay -56.181940-3.00 -3.74546267 0.745462667 Moroni Comoros 43.256000 3.00 2.88373333 0.116266667 Moscow Russia 37.617000 3.00 2.50780000 0.492200000 Muscat Oman 58.408330 4.00 3.89388867 0.106111333 Nairobi Kenya 36.817220 3.00 2.45448133 0.545518667 Nassau Bahamas -77.345000-5.00 -5.15633333 0.156333333 Naypyidaw Myanmar 96.100000 6.50 6.40666667 0.093333333 New Delhi India 77.208890 5.50 5.14725933 0.352740667 Ngerulmud Palau 134.624170 9.00 8.97494467 0.025055333 Niamey Niger 2.125280 1.00 0.14168533 0.858314667 Nicosia Cyprus 33.365000 2.00 2.22433333-0.224333333 Nuku&lt;U+02BB&gt;alofa Tonga -175.20000013.00-11.68000000 0.680000000 Nur-Sultan Kazakhstan 71.433000 6.00 4.76220000 1.237800000 Oslo Norway 10.733330 1.00 0.71555533 0.284444667 Ottawa Canada -75.695000-5.00 -5.04633333 0.046333333 Ouagadougou Burkina Faso -1.535280 0.00 -0.10235200 0.102352000 Palikir Federated States of Micronesia 158.15889011.00 10.54392600 0.456074000 Paramaribo Suriname -55.203890-3.00 -3.68025933 0.680259333 Paris France 2.352222 1.00 0.15681480 0.843185200 Phnom Penh Cambodia 104.921110 7.00 6.99474067 0.005259333 Podgorica Montenegro 19.262892 1.00 1.28419278-0.284192780 Port Louis Mauritius 57.504170 4.00 3.83361133 0.166388667 Port Moresby Papua New Guinea 147.14944010.00 9.80996267 0.190037333 Port Vila Vanuatu 168.31700011.00 11.22113333-0.221133333 Port-au-Prince Haiti -72.333000-5.00 -4.82220000-0.177800000 Port of Spain Trinidad and Tobago -61.517000-4.00 -4.10113333 0.101133333 Prague Czech Republic 14.417000 1.00 0.96113333 0.038866667 Pretoria South Africa 28.188060 2.00 1.87920400 0.120796000 Pyongyang North Korea 125.738060 9.00 8.38253733 0.617462667 Quito Ecuador -78.517000-5.00 -5.23446667 0.234466667 Rabat Morocco -6.841650 1.00 -0.45611000 1.456110000 Riga Latvia 24.106390 2.00 1.60709267 0.392907333 Riyadh Saudi Arabia 46.717000 3.00 3.11446667-0.114466667 Rome Italy 12.500000 1.00 0.83333333 0.166666667 San José Costa Rica -84.083000-6.00 -5.60553333-0.394466667 San Marino San Marino 12.447300 1.00 0.82982000 0.170180000 San Salvador El Salvador -89.191390-6.00 -5.94609267-0.053907333 Sana'a Yemen 44.206390 3.00 2.94709267 0.052907333 Santiago Chile -70.667000-4.00 -4.71113333 0.711133333 São Tomé São Tomé and Príncipe 6.730560 0.00 0.44870400-0.448704000 Sarajevo Bosnia and Herzegovina 18.417000 1.00 1.22780000-0.227800000 Singapore Singapore 103.833000 8.00 6.92220000 1.077800000 Skopje North Macedonia 21.433000 1.00 1.42886667-0.428866667 Sofia Bulgaria 23.330000 2.00 1.55533333 0.444666667 Sri Jayawardenepura KotteSri Lanka 79.887836 5.50 5.32585574 0.174144260 St. George's Grenada -61.750000-4.00 -4.11666667 0.116666667 St. John's Antigua and Barbuda -61.850000-4.00 -4.12333333 0.123333333 Stockholm Sweden 18.068610 1.00 1.20457400-0.204574000 Sucre Bolivia -65.250000-4.00 -4.35000000 0.350000000 Suva Fiji 178.44190012.00 11.89612667 0.103873333 Tallinn Estonia 24.745280 2.00 1.64968533 0.350314667 Tashkent Uzbekistan 69.267000 5.00 4.61780000 0.382200000 Tbilisi Georgia 44.783000 4.00 2.98553333 1.014466667 Tegucigalpa Honduras -87.217000-6.00 -5.81446667-0.185533333 Tehran Iran 51.388890 3.50 3.42592600 0.074074000 Thimphu Bhutan 89.636110 6.00 5.97574067 0.024259333 Tokyo Japan 139.692220 9.00 9.31281467-0.312814667 Tripoli Libya 13.191390 2.00 0.87942600 1.120574000 Tunis Tunisia 10.181670 1.00 0.67877800 0.321222000 Ulaanbaatar Mongolia 106.917220 8.00 7.12781467 0.872185333 Vaduz Liechtenstein 9.521000 1.00 0.63473333 0.365266667 Valletta Malta 14.506940 1.00 0.96712933 0.032870667 Vatican City Vatican City 12.452500 1.00 0.83016667 0.169833333 Vienna Austria 16.367000 1.00 1.09113333-0.091133333 Vientiane Laos 102.600000 7.00 6.84000000 0.160000000 Vilnius Lithuania 25.283000 2.00 1.68553333 0.314466667 Warsaw Poland 21.017000 1.00 1.40113333-0.401133333 Washington, D.C. United States -77.016390-5.00 -5.13442600 0.134426000 Wellington New Zealand 174.77722012.00 11.65181467 0.348185333 Windhoek Namibia 17.083610 2.00 1.13890733 0.861092667 Yamoussoukro Ivory Coast -5.283000 0.00 -0.35220000 0.352200000 Yaoundé Cameroon 11.517000 1.00 0.76780000 0.232200000 Yaren Nauru 166.92086712.00 11.12805778 0.871942220 Yerevan Armenia 44.514440 4.00 2.96762933 1.032370667 Zagreb Croatia 15.983000 1.00 1.06553333-0.065533333","link":"/strange-timezones/"},{"title":"Streamlining Your Data Science Workflow With Magrittr","text":"Packages required for this post: 123suppressPackageStartupMessages(library(dplyr)) # manipulating datalibrary(ggplot2) # data visualisationlibrary(magrittr) # piping (to be elaborated on throughout the post) Warning message: &quot;package &apos;ggplot2&apos; was built under R version 3.6.1&quot;Getting to know magrittrMagrittr (pronounced with a sophisticated French accent, as per the introductory vignette) is an incredibly powerful R package that forms the foundation of the tidyverse. In my personal opinion, it is one of the most underrated packages in the R ecosystem. Without its existence, my productivity in R would be severely hampered. The most fundamental tool this package offers is called the “pipe” operator, %&gt;%. The purpose of this operator is to take the value on the left hand side (LHS) and pass it into whatever function call is on the right hand side (RHS) as the first argument. In other words, if you write something along the lines of x %&gt;% f(), this will be evaluated as f(x). More generally if your function already has some filled parameters as in x %&gt;% f(y, z), this is evaluated as f(x, y, z). This lets you produce chunks of code called “pipelines” which use multiple pipe operators to create a continual flow of functions, the output of one being passed on to the next such as in this example. 123456# data manipulation without magrittrcars_subset &lt;- filter(mtcars, mpg &gt; 15)cars_grouped &lt;- group_by(cars_subset, cyl)cars_aggregate &lt;- summarise(cars_grouped, mean = mean(disp))cars_sort &lt;- arrange(cars_aggregate, desc(mean))cars_sort $(document).ready( function () {$('#table0').DataTable();} ); cylmean &lt;dbl&gt;&lt;dbl&gt; 8320.0500 6183.3143 4105.1364 123456# data manipulation with magrittrmtcars %&gt;% filter(mpg &gt; 15) %&gt;% group_by(cyl) %&gt;% summarise(mean = mean(disp)) %&gt;% arrange(desc(mean)) $(document).ready( function () {$('#table1').DataTable();} ); cylmean &lt;dbl&gt;&lt;dbl&gt; 8320.0500 6183.3143 4105.1364 As you can see, we get the exact same result in either. However, there is no question that the second method, using magrittr, is easier to follow and is far simply to write due to its reduced code duplication. Using magrittr lets you avoid the use of any temporary variables such as cars_grouped and cars_sort and lets you produce one long pipeline with each function leading naturally into the next. Furthermore, using the pipe operator makes your code far more interpretable to anyone less tech-savvy. The pipeline is written in the same way that you would describe the data manipulation process: take the data, filter it for certain MPG, group by number of cylinders, find the mean of each group, arrange the rows using this mean in descending order and print the output. If you’ve never seen magrittr before, I hope that this example confirms its place in your data science tool-kit. Anyone familiar with the use of tidyverse packages, though, will most likely recognise this operator. This is because it is included in the packages dplyr and tidyr - tidyverse packages for data manipulation and tidying respectively- and most likely others too. Paradoxically, this inclusion is in fact unfavourable for magrittr‘s full adoption as it leads many people to believe that this is as far as piping in R reaches, but that is far from the truth. Using the magrittr package directly (as opposed to through another tidyverse package), gives you access to several other pipe operator variants which offer you even more power over your workflow. Furthermore there are plenty of tricks involving magrittr that many people are unaware of. In this blog post, I wish to give a thorough introduction to the main features of the package through the use of practical examples. I hope that I can convince you that magrittr is truly one of the most important packages in the R ecosystem for the productive developer. Further pipe operatorsAll printing has been disabled in this post to avoid adding unnecessary baggage. All of the examples are self-contained however so if you which to see their output, you can simply copy the chunks to your own R session and run them, provided you have dplyr and magrittr installed and loaded. Alongside the standard piping operator %&gt;%, magrittr offers three related operators which offer similar functionality but with slightly different execution. These may not be used as frequently as %&gt;% but they are still extremely important to have familiarity with for the special cases in which they can be effectively utilised. The compound assingment operaterThe first additional operator we look as is called the compound pipe operator. This is implemented using %&lt;&gt;%. The effect of this is very similar to %&gt;%, except rather than simply piping the LHS into the first argument of the RHS and evaluating it, the result of this process is then assigned to the LHS as its new value. This is essentially shorthand for x &lt;- x %&gt;% f(), which is instead being replaced with x %&lt;&gt;% f(). This can be combined with multiple instances of the regular pipe to create pipelines designed to manipulate an existing object. Here is an example use of the compound assignment operator using the iris data set. 1234iris_sample &lt;- iris[sample(nrow(iris), 30), ]iris_sample %&lt;&gt;% filter(Species != \"setosa\") %&gt;% select(Sepal.Length, Sepal.Width) %&gt;% arrange(Sepal.Length, Sepal.Width) The tee operatorThe tee operator, %T&gt;%, is also rather similar in function to the standard pipe, except rather than returning the result of evaluating the RHS, it instead returns the LHS. For example if you were to run x %T&gt;% f, R would run f(x) but will return x instead of f(x). This is useful for when the function you are piping into is used for its side-effects (e.g. plotting, printing) rather than the value it returns. This then lets you carry on your pipeline rather than having to halt abruptly when a function doesn’t return a useful value. An example use of this is generating a plot mid-pipeline as shown here. 12345sample(1:100, size = 50) %&gt;% cbind(sample(1:100, size = 50)) %T&gt;% plot() %&gt;% # plot is used for its side-effect colMeans() %&gt;% # the result of cbind is passed into here diff() A word of warningDespite the many amazing things magrittr can do to help you supercharge your R productivity, it does have its idiosyncrasies. The most prevalent of which is that it doesn’t play well with ggplot2. This is due to the operator precedence (think BIDMAS/PEMDAS but for all operators R uses such as &amp;, !, $, etc.) that R employs, which evaluates the magrittr pipe operators before the ggplot2 plus operator (which is actually just a specific method of the S3 class for binary addition). This means that if we write some code like the following, with the goal of taking some data, plotting it, and then carrying on with data manipulation we receive an error. 12345iris %T&gt;% ggplot(aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() %&gt;% filter(Petal.Length &lt; 2) %&gt;% head() Warning message in eval(expr, envir, enclos): &quot;Error in UseMethod(&quot;filter_&quot;) : no applicable method for &apos;filter_&apos; applied to an object of class &quot;c(&apos;LayerInstance&apos;, &apos;Layer&apos;, &apos;ggproto&apos;, &apos;gg&apos;)&quot;&quot;This is because the R interpreter will evaluate 123geom_point() %&gt;% filter(Petal.Length &lt; 2) %&gt;% head() first. Which gives an error since filter has no idea what to do when it is passed a ggplot object as its first argument rather than a data frame. To avoid this issue we have to explicitly tell the interpreter to evaluate the plus operator before the pipe operators using bracketing. Note, that since auto-printing is disabled inside brackets, we have to explicitly tell R to print the ggplot object using print. 12345iris %T&gt;% {print(ggplot(., aes(x = Sepal.Length, y = Sepal.Width)) + geom_point())} %&gt;% filter(Petal.Length &lt; 2) %&gt;% head() This somewhat diminishes the clarity magrittr is designed to introduce to your code but I would argue that is still more elegant than creating a temporary variable or having code duplication, one pipeline leading to the plot, the other to the further data manipulation functions. Furthermore, if you are familiar with this behaviour and its typical solution, it isn’t as off-putting as on first glance. To learn more about operator precedence in R, use ?Syntax in the console. The exposition operatorFinally, the last magrittr pipe we have to discuss is the exposition operator. Of the three additional pipe operators, this is the one most unlike %&gt;%. As the name perhaps suggests, it it used to expose the names contained within the LHS object to the RHS. This lets you use the names directly in the RHS without having to prefix them with the likes of object$. For example, if you wanted to evaluate in the form f(x$a, x$b, x$c), you could use the exposition operator to write this as x %$% f(a, b, c). This is really useful when the function on your RHS does not have a data argument in the way that lm or aggregate do. Here is an example of its use. 123Orange %&gt;% filter(Tree == 1) %$% cor(age, circumference) Tips and tricks to supercharge your magrittr usagePiping as a later argumentSo far, we have only considered pipes in which the LHS is used as the first argument of the RHS function. The flexibility of magrittr means that this isn’t the only way we can do things. What the pipe operators will in fact do is use the LHS as the first unspecified parameter. This means that if you want to use the LHS input as the 3rd argument of a function, all you have to do is give values for the first two parameters using named values. For example if we wanted choose a random number from a uniform distribution on $[0,1]$ and use this as the variance for a sample of normal variables, we would do this as follows. 12runif(n = 1) %&gt;% rnorm(n = 20, mean = 0) Using placeholdersMagrittr‘s plasticity doesn’t even end there! The package also lets you make use of what are called “place-holders”. These are implemented using the period symbol and will be replaced with the LHS input when they are evaluated. For example the code x %&gt;% f(5, nrow(.), .^2) will be evaluated as f(5, nrow(x), x^2). This lets you completely remove any duplication of x in the RHS function call. This method can be extended to very complex scenarios by using curly braces to enclose a series of statements such as in this example. 123456sample(1:10, size = 5) %&gt;%{ if (sum(.) &gt; 25) max(.) else min(.)} Using place-holders also allows you to use magrittr with not only functions, but expressions to. You simply write out the expression as normal, replacing any instance of RHS with a period and then enclose it in curly braces. For example we can use this to normalise a random sample of integers. 12sample(1:100, size = 10, replace = FALSE) %&gt;% {(. - min(.)) / (max(.) - min(.))} Pipes and binary operatorsThe last feature of magrittr that I wish to discuss is how it can be used with binary operators. In fact, there is a way to force magrittr to work with such operators using standard R code. This uses the trick that operators can be called in a similar style to functions by enclosing them in single quotes and giving them two arguments to be used as the right and left hand side of the operator. For example we can write 4 + 5 as `r '\\x60+\\x60(4, 5)'` or x[4] as `r '\\x60[\\x60(x, 4)'`. You can then pipe into these functions using magrittr. This is a far from an ideal solution so its lucky that the package offers a set of helper functions called “aliases” designed to make this process easier. Examples of such functions include extract(), add() and divide_by though there are plenty more. You can find a whole list of them by using ?extract after magrittr is loaded. Here is an example use. 123456matrix(runif(100, max = 10), nrow = 10) %&gt;% mod(10) %&gt;% # alias for `%%` multiply_by_matrix(t(.)) %&gt;% # alias for `%*%` equals(t(.)) %&gt;% # alias for `==` all()# should return TRUE as any matrix multiplied by its transpose is symmetric","link":"/streamlining-with-magrittr/"},{"title":"Rethinking the T9 Typing Layout","text":"I’ll confess. This blog post is a bit late. 10 years too late, in fact. For those not aware, the T9 typing layout was a technology designed by Tegic Communications for use on mobile phones with a 3x4 numeric keypad. The letters of the alphabet would be distributed in order among the keys with three or four characters assigned to each of the numbers 2-9 with the 0 and 1 key reserved for punctuation. The T9 system actually involved predictive technology too, allowing you to press a series of numbers each of which gave a few possibilities for each letter composing the word you wanted to type and then there would be an algorithm designed to guess the word you most likely intended.As you can quite possibly imagine, this was not the most reliable method of input. For example, all of the words “goner”, “goods”, “goofs”, “homes”, “hones”, “hoods”, “hoofs”, and “inner” all have the same T9 key sequences. You had very little chance of getting the correct word in these scenarios and would have to fumble through the possibilities until you came across the one you wanted and, with this technology being used heavily in the early 2000’s, phones were no where near capable of using context clues to decide which of the T9 synonyms you were after. This left many people to give up on the predictive ability of the T9 system and instead manually input each letter by altering the number of presses that is made to each key. For example, the standard T9 layout has the letters “ABC” for the number 2 and so to get a letter “C”, you would have to press this key three times. Undeniably slower but far more consistent. With the smartphones now being a commonplace item, the T9 system is now generally obsolete, a few inexpensive and unimaginative budget phones aside. With the adoption of touchscreen keyboards, there is no need for T9 and its use is becoming rarer and rarer. If there ever was going to be a redesign of the T9 layout, it should have been a lot time a go. Never-the-less, that is just what I plan to do now. The Flaws of the T9 LayoutWhen I was younger, and the T9 layout was in its prime, I was always baffled by the grouping of letters the original designers had employed. I assume that their main goals were aesthetic: three letters for every available number with an extra one in the bottom left and right corners. It’s got a nice symmetry to it but at the same time it’s painfully inefficient. For a start, the most common letter in the English language, “e”, which composes a whopping 12.7% of the character usage, requires two clicks of the 3 key to get to it. That’s the same number as for “x” or “q”, the third and second least used letters in the English language respectively. I could go on: The letter “o” (the fourth most common) requires three button presses and the letter “s” (the seventh most common letter though still making up 6.3% of all use) requires a whole four presses. It’s an efficiency nightmare!This is not the only possible layout in the T9 style that could be used though. If you are willing to sacrifice some beauty and order, you can gain a considerable amount of typing speed in return. The aim of this post is to find the optimum such layout and see how it compares in terms of efficiency to the standard implementation. A Layout OverhaulBefore we begin optimising our T9 layout we first need to decide on what a valid layout actually is. I propose the following rules: The letters corresponding to a larger number must fall later in the alphabet than those corresponding to a smaller one Within the set of letters corresponding to a shared number, the letters corresponding to more button presses should be later in the alphabet Only the keys 2-9 can be used for inputting letters Every number must have at least one letter corresponding to it It is worth noting that the last constraint is redundant as an optimal layout would never leave a number key corresponding to no letters but I add this for completeness anyway. By changing up the layout even further you will be able to squeeze out even more speed. For example, if you ignore the constraint of keeping things in alphabetical order, you can improve the efficiency by having the eight most common letters each be at the start of a block. This free range actually makes the solution a lot less interesting and far too easy to compute so we instead decide to stick with the description of a T9 layout described above. Using this definition, we can think of these layouts in a different way: To form any layout, simply take the list of all letters (in alphabetical order) and insert 7 dividers into 7 of the 25 gaps between the characters. This will result in 8 non-empty groups, all in alphabetical order within and outside too. In other words, this a T9 layout. In fact, this gives us all possible layouts. Since we have 25 spaces to put these 7 dividers, the number of possible choices is $${25\\choose7} = 480700$$ and so this is how many layouts we have too. This number is small enough that we can just test the efficiency of each one in turn and then pick the best. But how are we going to determine the efficiency? A Metric for Layout EfficiencyThere are many possible ways to evaluate how efficient a layout all of which have their merits. The method I have decided to use is the mean number of key presses needed to type a random letter of the English alphabet occurring with probability equal to its proportional frequency in the language. That’s quite a mouthful but should be clear with an example. In English, the letter frequencies for the first 5 letters are 8.2%, 1.5%, 2.8%, 4.2% and 12.7%. Supposing for now that we are still using the standard layout, we need 1 button press for “a” and “d”, 2 for “b” and “e” and 3 for “c”. Therefore our mean number of key presses metric will begin with $$0.082 \\times1+0.015\\times2+0.028\\times3+0.042\\times1+0.127\\times2+\\ldots$$and carry on likewise for all letters. This is probably not the most accurate metric for measuring pure typing speed as we are ignoring the fact that if you want to type “b” then “a”, you have to press the 2 key twice, then wait a moment until the button count resets and then press the 2 key once more. Our metric does not account for this pause and this could make the difference between which layout is optimum. The issue with implementing a metric more suited to this, however, is that the waiting time needed before retyping will differ by phone model and configuration so we have no way of knowing what an appropriate wait time is. Therefore we stick with our mean number of button presses metric and hope that since the occurrences of two letters from the same number key in a row are rare, this will affect our optimum negligibly. Lastly, just before we start computing these optimums, it is worth considering why we should be limiting ourselves to the English language only. We definitely shouldn’t as we may be missing out on interesting differences in the results between languages. Therefore, rather than just using English language letter frequency data, I have scraped a collection of such frequency tables off Wikipedia (specifically, this page). I did this using an R script which is also responsible for the later visualisations in this post. I have not included any code in this post as it doesn’t add much of interest but if you wish to reproduce my work, the source code can be found on my GitHub. Letter frequency tables are rather dull and hard to read so I’ve cooked up a visualisation of the distributions for each of the languages we’re looking at. This gives us some insight into how we may want to design our optimum layout for each language but to know the exact best solution, we’ll have to start our computations. Warning message: &quot;funs() is soft deprecated as of dplyr 0.8.0 please use list() instead # Before: funs(name = f(.)) # After: list(name = ~ f(.)) \u001b[90mThis warning is displayed once per session.\u001b[39m&quot; Finding Optimum LayoutsWe are now ready to compute the optimum layouts for each language. The method I intend to employ to calculate this is simply brute force, finding the efficiency for every valid combination and then choosing the best. There are others methods worth considering. For example, we could use a dynamic programming technique - a method for solving a complex problem by breaking them down into a collection of simpler sub-problems. Or, at the very least, we could reduce our space of possible layouts using some heuristics. Both of these are not worth the effort, however, as our task is small enough to compute by force in a reasonable amount of time. If we were to expand the problem however, say to allow more dividers or a longer alphabet, we’d have to reconsider these other options. After running the optimisation function I produced for a few minutes, the following result popped out: $(document).ready( function () {$('#table0').DataTable();} ); Break 1Break 2Break 3Break 4Break 5Break 6Break 7 Czechc|dh|ik|lm|nq|rs|tu|v Danishc|dd|eh|ik|lm|nq|rs|t Dutchc|dd|eh|ik|lm|nq|rs|t Englishb|cd|eg|hk|lm|nq|rs|t Esperantoc|dd|eh|ik|lm|nq|rs|t Finnishd|eh|ij|km|nq|rs|tt|u Frenchb|cd|eh|ik|lm|nq|rs|t Germanb|cd|eg|hk|lm|nq|rs|t Icelandicd|ef|gh|ik|lm|nq|rs|t Italianb|cd|eh|ik|lm|nq|rs|t Polishb|cd|eh|im|nq|rv|wx|y Portugueseb|cd|eh|il|mn|oq|rs|t Spanishb|cd|eh|ik|lm|nq|rs|t Swedishc|dd|eh|ik|lm|nq|rs|t Turkishc|dh|ij|km|nq|rs|tx|y This is far from the nicest format to view the results in so I’ve produced this graphic here to display the same information in a more aesthetically pleasing way. And for the specific case where the language we are optimising for is English, I have produced a mock-up image of what a phone would look like with the most efficient layout: Comparing the Standard and Optimum LayoutsWe are now ready to compare the standard T9 layout with the new optimum layouts we have produced. Here is a graphic I produced to show the typing efficiency (using the metric described earlier) for each language and type of layout. As you can see, using our new layouts provided a considerable improvement in typing speed for every single language. The language with the best efficiency for the standard layout was Italian with a MNBP (Mean Number of Button Presses) or 2.01 and the worst was Turkish with a MNBP of 2.35. After optimising the layouts, the new most efficient language is French with a MNBP of 1.61 (though Italian is a close second with 2.04) and the new least efficient is Polish with a MBNP 1.89. This is notably different from the other optimal speeds which are all below a MBNP of 1.8. This can most likely be explained by the abnormally high usage of “z” in the language. Being the last letter of the alphabet, this can either by included by itself (meaning other groups will have to be larger and so slower) or as a later letter in a shared group (with the same result). The language that improved the most from having its layout optimised was German with an decreased MBNP of 0.629 and even the least improved language, Czech had an improvement of 0.339. It’s safe to say that this project was a massive success. Sadly though, the T9 layout is still not going to change but we can feel glad in knowing that if it were to, we’d know exactly what replacement we should be putting forward.","link":"/t9-layout/"},{"title":"The Look-and-Say Sequence: A Tribute to John Conway","text":"One week ago today, we lost one of the most inspirational mathematicians of this generation. After a lifetime of incredible achievement, John Conway died on Saturday, 11th April 2020. This left many people—from the mathematically avid to those with merely a passing interest—only to reminisce about both the amazing leaps forwards he made in his work, but more importantly how he brought out the beauty and charm in areas of mathematics that previously seemed bland. Conway was a unique character with many a story to his name. Some of these tales are so wonderful that I don’t dare mention them myself for fear of butchering the narrative. Instead, I would like to point readers towards this short episode of the Numberphile Podcast in which many respected mathematicians and acquaintances of Conway share their memories of his life. Conway was as close as you could get to the mathematical equivalent of a rock star, with the stage presence, devoted fans, and involvement in crazy anecdotes to match, so please do have a listen. Conway wasn’t just an inspirational and amusing figure, but a talented and insightful mathematician. Despite being best known for his “Game of Life“—a zero player game with simple rules, from which immense complexity and beauty emerges, and ironically one which he didn’t think much of himself—Conway made fascinating novel discoveries across many areas of mathematics. Be it, geometry, number theory, combinatorics, or group theory, Conway contributed incredible ideas to the mathematical community, a few highlights being his work with surreal numbers, moonshine theory, and the icosians. Even if I only plan to mention Conway’s Game of Life in passing, I can’t help but share Randall Munroe’s touching short animation, paying tribute to Conway in his latest xkcd post. As a form of tribute to Conway, I wanted to discuss a particular discovery he made in his career that still fascinates me to this day. It concerns the ‘look-and-say sequence’. The next section of this post will be used to quickly introduce this sequence and share some Python code that can be used to generate its terms (don’t worry if you can’t code—this will be short and incidental). We will then discuss Conway’s relevance to this sequence, and close with an interesting puzzle to ponder, for which I’m yet to obtain a solution. The Look-and-Say SequenceThe beauty of the look-and-say sequence is how simple it is to define. You don’t need any mathematical background and little thought is required to find the next term. You simply ‘look-and-say’. The sequence starts plainly with a single digit 1: 1 We look at this and say what we see. That is, “I can see one one”. Then, all we have to do is write down what we said—“one, one”: 1 1 That’s it, the second term of the sequence consists of two numbers, both of which are ones. We now say this term out loud. That is “I can see two ones”. We ignore the plural and focus on the key part of what we said—“two, one”, our next term: 2 1 Repeating this one more time we get “I can see one two, and one one”—“one, two, one, one”: 1 2 1 1 And one more for good measure. We read “one one, one two, and two ones” giving us: 1 1 1 2 2 1 If we were to continue this sequence a few more times we would obtain the following terms: 3 1 2 2 1 1 1 3 1 1 2 2 2 1 1 1 1 3 2 1 3 2 1 1 We could go on forever if we wanted to, though I think you get the point. And frankly, writing these all out by hand is labourious and prone to mistakes. Instead, it would be useful if we could leverage computing power to generate these for us. Thankfully this can be done with little effort using the Python script below (if you don’t care for coding, don’t worry, we’ll move promptly past this). 1234567891011121314151617181920212223242526272829from time import timedef look_and_say(n): \"\"\"Generate the look-and-say sequence up to term n. Returns a list of sequence members and elapsed times. \"\"\" sequence = [[1]] elapsed = [0] start = time() for i in range(n-1): next_elem = [] run_val = sequence[i][0] run_len = 1 # loop through rest of sequence with dummy value at end for val in sequence[i][1:] + [-1]: if val != run_val: next_elem.extend([run_len, run_val]) run_len = 1 run_val = val else: run_len += 1 sequence.append(next_elem) elapsed.append(time() - start) return sequence, elapsed From this point onwards, there will be no more code included in the post directly. If, however, you wish to see the source code used to produce the graphics in this piece, you can find the corresponding notebook here. Whether this code makes sense to you or not makes no difference. The important idea is we can use this to quickly generate some more terms. Here are the next 10 terms from where we got to before (1 1 1 3 2 1 3 2 1 1): 3 1 1 3 1 2 1 1 1 3 1 2 2 1 1 3 2 1 1 3 1 1 1 2 3 1 1 3 1 1 2 2 1 1 1 1 1 3 1 2 2 1 1 3 3 1 1 2 1 3 2 1 1 3 2 1 2 2 2 1 3 1 1 3 1 1 2 2 2 1 2 3 2 1 1 2 1 1 1 3 1 2 2 1 1 3 1 2 1 1 3 2 1 1 1 3 2 1 1 3 2 1 3 2 1 1 1 2 1 3 1 2 2 1 1 2 3 1 1 3 1 1 2 2 2 1 1 3 1 1 1 2 2 1 1 3 1 2 2 1 1 1 1 3 1 2 2 1 1 3 1 2 1 1 1 3 1 2 3 1 1 2 1 1 1 3 1 1 2 2 2 1 1 2 1 3 2 1 1 3 2 1 3 2 2 1 1 3 3 1 2 2 2 1 1 3 1 1 2 2 1 1 3 1 1 3 1 1 2 2 2 1 1 3 1 1 1 2 3 1 1 3 1 1 1 2 1 3 2 1 1 2 3 1 1 3 2 1 3 2 2 1 1 2 1 1 1 3 1 2 2 1 1 3 1 2 1 1 1 3 2 2 2 1 2 3 1 1 3 2 2 1 1 3 2 1 2 2 2 1 1 3 2 1 1 3 2 1 3 2 2 1 1 3 3 1 1 2 1 3 2 1 1 3 3 1 1 2 1 1 1 3 1 2 2 1 1 2 1 3 2 1 1 3 1 2 1 1 1 3 2 2 2 1 1 2 3 1 1 3 1 1 2 2 2 1 1 3 1 1 1 2 3 1 1 3 3 2 1 1 1 2 1 3 2 1 1 3 2 2 2 1 1 3 1 2 1 1 3 2 1 1 1 1 1 3 1 2 2 1 1 3 1 2 1 1 1 3 2 2 2 1 2 3 2 1 1 2 1 1 1 3 1 2 2 1 2 3 2 1 1 2 3 1 1 3 1 1 2 2 2 1 1 2 1 1 1 3 1 2 2 1 1 3 1 1 1 2 3 1 1 3 3 2 2 1 1 2 1 3 2 1 1 3 2 1 3 2 2 1 1 3 3 1 1 2 1 3 2 1 2 3 1 2 3 1 1 2 1 1 1 3 1 2 2 1 1 3 3 2 2 1 1 3 1 1 1 2 2 1 1 3 1 2 2 1As you can see, these get big, fast. In fact, let’s create two plots to visualise this growth. The first will show the how the length of each term of the sequence is increasing and the second will show how long it takes us to compute terms up to that point. Reading off the graph, we see that the 70th term of our sequence was composed of over one hundred million digits. Thank god we decided to switch to Python! On top of that, it took the script (which is reasonably efficient) over a minute to reach this point. Both of these graphs are clear examples of exponential growth. That is, the length of a a term in the sequence can be approximated by taking the length of the previous term and multiplying it by some fixed scale factor (likewise for the computation time). What exactly is this scale factor though? This is where our friend Conway can offer a hand. Conway’s ConstantEnter: John Conway. As we eluded to in the introduction, Conway was not a complacent man when it came to mathematics, and one of his numerous discoveries was that of Conway’s constant. This number is of interest to us because it is exactly the value of the scale factor related to the look-and-see problem. This in itself is not necessarily interesting—many people solve mathematical problems just like this all the time; why is this particular solution so special? Well, the real wonder of Conway’s constant comes from seeing the crazy way in which it is defined. In short, Conway’s constant is the unique positive value $x$ such that to the following expression is equal to zero. This is a monsterous equation, making it notable in itself that Conway managed to find it and prove its relation to the look-and-say puzzle. What fascinates mathematicians about this equation is the question of ‘why?’—Why is this the solution? Why is the highest power 71? Why is there no 70th term? Why are the coefficents the values they are? Why does this even relate to the look-and-say problem? These are all incredibly difficult questions whose answers largely come down to ‘just cos’. It almost feels like the look-and-see problem is some sort of cosmic joke the universe is playing on us. Normally when you solve a problem with this sort of beauty and structure the answer comes out to be something stunningly simple: perhaps some multiple of the famous constants $e$, $pi$, or $\\phi$, or at the very least the solution to an equation of degree 2 or 3 with a pattern to the coefficients. But degree 71 with no clear pattern to the coefficients? That’s something special. And frankly, I don’t believe this could be more fitting. Conway was an odd yet utterly wonderful man, both unusual and iconoclastic, yet full of ideas with such beauty and fascination. His constant seems to barely differ. And just as the mathematical laws governing this constant will live on, so will Conway, and his weird but wonderful ways, in our memories. An Intriguing PatternI don’t think Conway would have liked a soppy ending, but rather closing by revealing new avenue of thought to persue, so this is what I will do. When I was coding up the look-say-generator above, I noticed an interesting pattern emerging. First, it’s worth noting that it is impossible for a term in the look and say sequence to contain a digit greater than three. The argument is the same for all numbers over three so we will focus on proving this for just four. The argument is rather simple, involving backtracking through the sequence. Suppose our sequence contained a number four at some point. Then, in the previous term there must have been a subsequence such as 1 1 1 1 or the same with 2/3. This would only arise if we had said “one, one, and one, one” or the equivalent. That is, we must have had 1 1 in the term before that. But this would be read as 2 1, not 1 1 1 1 and so we don’t in fact obtain a four. With that out of the way, we can move towards the pattern I discovered. This concerned the proportions that the digits 1, 2, and 3 shared as the sequence progressed. For example in the sixth term (3 1 2 2 1 1), the split is $\\left(\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}\\right)$. What is the long term behaviour of this distribution? I expected it to be semi-chaotic, but was instead surprised to find that the proportions eventually converged sharply: As we can see, once we pass the 20th or so term, the digit proportions rapidly converge to what seems to be $\\left(\\frac{99}{200},\\frac{8}{25},\\frac{37}{200}\\right)$ or something close by at the least. This leaves us with one more ‘why?’ to address. Of the many articles I have read about the look-and-say sequence, none make explicit mention of this phenomenon, and nothing particularly obvious jumps out at me regarding these ratios. I haven’t had the time to delve into the problem properly as I wanted to release this post in a timely manner but I will explore this pattern further at some point. And if do come across an interesting explanation, I will write a follow up post. That is, if I never mention this problem again, I either failed to reach a solution or the solution turned out too be dull to write about (and I would prefer it if we just assume the latter). If any of my more mathematical readers want to have a crack at this problem by themselves, I would be interested to hear about a solution if one is obtained. For now, all I can offer is what my gut instinct says, and hope that this shows promise: perhaps modeling the limiting terms of the problem as a Markov chain and attempting to find a stationary distribution could provide a reasonable explanation. Furthermore, perhaps Conway’s constant has something to do with these peculiar ratios—it would seem fitting that this oddity comes from that source after all. Either way, I hope this post has made clear what an amazing man Conway was, and the shear wonder that his theorems and ideas brought to the mathematical world. He has been an inspiration to myself and many other mathematicians, and for that reason, he will be sorely missed.","link":"/tribute-to-conway/"},{"title":"Upon Reflection: dunnhumby","text":"Yet, I am learning. MichelangeloAge 87 Okay, I know that Michelangelo didn’t really say that. It’s a common misattribution with a traceable origin. Despite this, I have included it here because it captures an element of life that is very important to me - we can always and should always be learning. Learning can be both forward-facing and retrospective, and in this series of blog posts I intend to focus in on the latter type; reflecting on my experiences and opportunities in the field of data science and what I have learnt from those encounters. In this first installment, I plan to discuss my summer internship with the global customer data science company, dunnhumby. I will walk through the company and its values, the project I was working on, and the techniques I used to progress. In doing so, I will discuss the challenges that I had to overcome, the problems I failed to get past, and the lessons I learnt in the process. BackgroundI will introduce the content of my internship by starting at the top and honing in on my individual work one level at a time. The CompanyIf I had to describe dunnhumby in a few words, it would be as the hipster of data science - doing it long before it was cool. Humble BeginningsThe company was formed by husband and wife team Edwina Dunn and Clive Humby in the late eighties. At that time it was a small operation ran out of the kitchen of their West London home. Despite this grassroots foundation, the company soon developed an impressive client base including Cable &amp; Wireless, BMW, and - most importantly - Tesco. Even though the specific term was still decades away from entering the mainstream, this work was essentially just data science. Admittedly, the tools and techniques used to solve analytical problems were vastly different - SVMs hadn’t even been invented by this point - yet the motivation, in particular the focus on science and data-led decision making, was the same. Dunnhumby and TescoDunnhumby originally gained prominence through their work in establishing the Tesco Clubcard. At the time, 1994, Tesco was yet to overtake Sainsburys as the largest UK retailer, and hoped that the creation of a loyalty card could spark this desired growth. Dunnhumby began a series of trials in which they used this new data asset to understand customer shopping behaviour. This was massively successful. The first response from the board came from the current Chairman of Tesco, Lord MacLaurin - “what scares me about this is that you know more about my customers after three months than I know after 30 years”. From then onwards, dunnhumby has continued to expand into new markets and regions, acquiring many adjacent companies to aid in this growth. The Chairman’s quote above has been proliferated throughout all sorts of data science literature; two of my favourite non-fiction books - Big Data: Does Size Matter? and Automating Inequality - both include it. Although dunnhumby has kept a generally understated reputation within society at large, it is clear how massive an impact they have had on the world of data science. This prominence and legacy it was led me to apply to a role at the company as soon as it crossed my radar. ValuesDunnhumby’s work is driven by four overarching values: passion, curiosity, courage, collaboration. I think most will agree that a few of these are quite generic-corporate but the value of courage particularly stood out to me. In my time at the company, I never observed a shyness towards trying new ideas or experimenting. Failure was expected and even relished at times; it was having the courage to give things a go and learn from mistakes that was vital. I think this is an element of their philosophy that makes dunnhumby a good place to work at. The TeamMy original assignment at dunnhumby was to the Security &amp; Operations team. I would have been very happy in this team, developing new security applications with Python. Before I even turned up for my induction though, there had been a change to this plan. This is due to some much appreciated attention from my original manager who, having had a look through this very blog, realised that I would be far more suited for a research data science role. I cannot express how grateful I am for this level of personal care, especially considering the amount of indifference interns are stereotypically managed with. This is how I ended up on the HuYu team. Before I get into what the HuYu team is all about, I’d like to walk through the story of its conception. The Origin of HuYuTesco and dunnhumby have a symbiotic business relationship. In fact, Tesco originally bought a 53% stake in 2001 and have since acquired the company in full. Due to this, dunnhumby is given full access to Tesco Clubcard data. This is used to generate insight for Tesco but also to provide customer science solutions to other companies - a key revenue driver for dunnhumby. In 2015 however, Tesco announced that they were seeking a buyer for the company. In the end no sale was made, but if a purchase was successful this may jepardise dunnhumby’s leading revenue source. This got many cogs turning in the company, could dunnhumby develop it’s own in-house data asset to circumvent any future risk that a deal like this could insight? This is where the HuYu team enters the scene. HuYu is an app developed by dunnhumby which incentivises users to scan their shopping receipts and complete surveys in order to earn points which can be exchanged for shopping vouchers. Any receipts from the eight major UK retailers are accepted including those from home delivery services. HuYu is designed to be highly multi-faceted, featuring many data sources. In the last few weeks of my internship, the team had just rolled out a trial of a new feature allowing users to link up their search and browse data with the app. There are also long term goals to allow users to share their social media data and general purchase history through their bank in exchange for appropriate awards. The goal of HuYu is two-fold. The first is to provide a backup data asset in case dunnhumby does one day lose access to clubcard data. But there is also the aim to produce analyses that would not be possible with use solely of one retailer’s data. Examples of such projects are those on cross-shopping or shopper journeys. This also allows further insight into how a customer comes to making a purchase. HuYu’s long-term goal is to capture the whole complexity of this process, from sofa to store. The HuYu TeamDespite being part of a large, multinational firm, the HuYu team has adopted a distinct start-up vibe. The team is small and budgets are carefully balanced. I was a member of the main part of the team consisting of around five full-time colleagues and a fellow intern. The team also had a handful of members dotted across the globe in Germany and India although the total number of dedicated members was certainly below a dozen. App development was performed by a third-party company with some design and legal assistance from the wider network at dunnhumby. As a result of this start-up-like environment, everyone in the team was very focused on their own duties and responsibilities. That isn’t to say that there wasn’t any room for collaboration, in fact many of the problems we were facing demanded extensive brain-storming session, but it was the case that no one was ever looking over your shoulder, ensuring that you were indeed completing your assigned duties. This also meant that support was hard to come by; if you had a problem, the first port of call would be extensive independent research and testing. Everyone had their own work to be completing with tight deadlines and so it never seemed fair to bother someone with a problem that you hadn’t thoroughly exhausted yourself. I can understand that this environment would not suit all, but for me this was perfect. I loved the freedom and independence I was given and reveled in the opportunity to solve my own problems and then report back semi-regularity to showcase what I had produced. The ProjectMy internship at dunnhumby lasted eight weeks and in the course of this time I was tasked with a project. This revolved around the classification of the receipt items using natural language processing. The Journey of Receipt DataWhen a receipt is scanned using the HuYu app, and after necessary security checks have been made, the image is parsed using Google’s Vision API in order to extract relevant data such as item names and price, as well as transaction level data such as payment method, store location, and shop date/time. The focus of my project was on the item data. On a daily basis, I was working with a large Google BigQuery table containing the name and price of every line item successfully scanned (as well as other meta-data such as retailer name) over the past year or so. This receipt data is of little use in its raw form; it is very difficult to analyse or make sense of it without any manual filtering or tagging. This is where the classification project comes into play. Before I arrived at dunnhumby, there was an elementary model using the ElasticSearch framework for classifying these scanned line items. The classification challenge was to map each receipt name to a location in an existing three-level hierarchy as well as assigning a brand label if appropriate. For example, an occurrence of the receipt name MCVTS CHOC HOBNOB should be mapped to FOOD CUPBOARD &gt; BISCUITS &amp; CEREAL BARS &gt; BISCUITS with brand MCVITIES. Improving ClassificationAs I mentioned above, this existing model was certainly elementary. There were many instances of receipt names that a human could easily identify but the model would fail to do likewise. My main objective during the internship was to either adapt or replace this model to improve the accuracy of classification. My first week was spent performing extensive research on what machine learning techniques could be applied to this problem. I also have some past experience with natural language processing and so I looked into whether the techniques I had previously used - naïve Bayes, random forests, SVMs - could be applied to this problem. In the end, I reached the conclusion that due to the messiness of the data (the OCR methods were far from perfect) and complexity of the response space, all of these traditional ML methods were inappropriate. I therefore set about finding ways in which I could improve the existing model. Project DetailsTechniques and ApproachesSo how did I actually go about improving classification? That is quite a big quite a big question. A open project like this with a goal as broad as ‘improve classification’ can be tackled from many directions, and a lot of the time even indirectly. In this next section I will summarise some of techniques and approaches that I used throughout my internship. It is worth noting though that this is far from a comprehensive list of my work but rather a highlight reel. Natural Language ProcessingThe first few weeks of my internship I spent focused on improving the existing ElasticSearch model. This came down to implementing NLP techniques to help the model gain insight into how classification should occur. Much of this involved ‘teaching’ the model basic rules of English or of the domain-specific language rules that are followed in receipt data. Examples of the generic language rules are pluralisation - understanding that AVOCADO and AVOCADOS are the same thing - white-space handling - SEA BASS and SEABASS have the same meaning - and initialisms - STIR FRY SAUCE can be abbreviated as SFS. There are also some language rules specific to receipt data such as removing vowels from words - humans can easily read WHL MLK as WHOLE MILK - creating edge n-grams - CARAMEL CHOCOLATE BROWNIE can be shortened to CARM CHOC BROWN without loss of meaning - and contractions - replacing STRAWBERRY with S/BERRY. I use the word ‘teaching’ but the processes actually involved creating JSON mapping files which told ElasticSearch how to process the stream of tokens it reads from the scanned receipts and the text in the database it is searching against. I implemented over 10 different NLP techniques in total, many a lot more subtle than those mentioned above. This gave rise to a modest increase in classification accuracy (evaluated using a moderately sized testing dataset of manually classified receipt line items) but I still felt that I could push things further. A recurring difficultly that the model had at this point was with vague receipt names such as ‘CHICKEN’. As humans, we intuitively understand that this most likely refers to fresh chicken - if it was frozen, it probably would have mentioned. Our model does not have any context for this name though so it cannot use that sort of reasoning. In fact, it ends up favouring frozen chicken. This is a general pattern: when presented with a vague receipt name, the model preferred to put it in a more obscure classification. The reason for this is that in smaller, less bought classifications, there are less unique items and so names don’t need to be as long to distinguish between products. Therefore the search document from an unusual classification has a higher proportion of terms matching a given vague search term and so a match to such classification are favoured. This was a difficult problem to overcome and required a lot of failed attempts. My initial approach was to try to use ElasticSearch to search against whole categories rather than individual items. This ended up favouring larger classifications where there was more data to match and so decreased overall accuracy. I then tried weighting classifications by their size but this also had a negative impact on accuracy. In the end, the solution I found to be the best was using a two-tiered search system. First, I would complete a search as described above. I would then look at the confidence score that ElasticSearch returned for the top $n$ (whatever $n$ may be best) matches. If either the top score was too low or not distinct enough from the next $n$ results, I would then and only then implement an aggregate search as described above. This was the perfect balance - if ElasticSearch was already confident in its top result then we added no noise or runtime, yet if confidence was low, fallback methods would be employed. I spent further time throughout my internship on these NLP problems including building custom algorithms for pluralisation handling and vowel removing to deal with some cases that existing algorithms were not suited for (a comical example was COOKED MEAT be identified as GO COOK MEAT TENDERISER because COOKED was stemmed to COOK by the Porter2 stemmer). Overall I managed to improve classification by around 5% in all hierarchy levels. This may sound small, but considering the erratic nature of the OCR algorithms scanning the data and the limitations of the data itself - how do we classify CRISPS at the bottom category level (flavour)? - this was a highly substantial improvement. HuYu ES PackageIn order to improve the ElasticSearch model, I had to go through dozens of iterations each one requiring repeated cycles of tweak, evaluate, and dissect. This was incredibly time-consuming. The existing code that I was using to build models was in the form of a Jupyter notebook with strange dependencies, references to global variables all over the place, and minimal modularity. I felt like there had to be a better approach. I wanted to build models in such a way that my code footprint was as small as possible, evaluation and model explanations were clear, and most importantly edits and new builds could be done as fast as possible. This motivation led me to develop a Python package specifically for developing ElasticSearch models for HuYu data. The package consisted of a few thousand lines of code with full documentation and examples. This took some time to develop and test but I believe it was worth it. It is now much easier to develop such models and more importantly it is very easy to see individual improvements and regressions when going from one model to another, whereas originally model changes could only be evaluated by overall accuracy. Furthermore, due to a caching feature of the package, it is now much quicker to get predictions from existing models without any changes needed to the calling code. I hope that the package will be used to streamline all future model development. I since I designed it to be as flexible and modular as possible, it can be extended for any new business needs that the team comes across. Web ScrapingIn the last few weeks of my internship I spent some time performing web-scraping using Python (specifically the BeautifulSoup package). This involved trawling through the sitemaps of the Aldi and Sainsburys websites to extract product information (name, description, price, weight, reference code, etc.) from all available items. The purpose of this was to set up the foundations for an eventual re-invention of the classification hierarchy we were using to allow easier matching with non-Tesco products. To perform this scraping I had to employ a powerful range of regular expressions and assert a tight testing regime to ensure that all data was being scraped correctly (manually verifying the 40,000 links on the Sainsburys sitemap was not an option!). I also implemented multi-processing to speed up scraping, yet managed to do this whilst still utilising progress bars (through the tqdm package) to make scraping user friendly. ChallengesUnsurprisingly, not everything in my internship went smoothly, and there were many challenges I had to overcome along the way. I will highlight a few here and how I came to overcome them. Google Cloud PlatformApart from the development of the HuYu ES package, all of my code was developed in the cloud using the Google Cloud Platform. This is the first time that I have worked with cloud computing although I am now sure it won’t be the last. In many ways this didn’t present much difficulty; my OS of choice is Ubuntu so I am very comfortable in a Linux shell environment. One big difference to my previous experience, however, is the introduction of firewall protections. On my local system, if I am to spin up an instance of Jupyter, I can simply access it through the relevant port of localhost. This is not the case with cloud computing as the server running Jupyter has a completely different IP address to that of my local machine. This means that port forwarding, SSL certificates, and the likes had to be set up and configured. I learnt a lot about network security in the process and I hope to look into this more in the future. Once these settings were configured very little trouble occurred, yet the initial setup and troubleshooting took up a significant amount of my time. Standard SQLA large part of my work revolved around querying the BigQuery database storing receipt data using Google’s own variant of SQL - Standard SQL. My SQL is fairly strong but there were still challenges I faced in using this. For a start, the Standard SQL dialect is fairly limited when compared to more common choices such as MySQL and PostgreSQL. This makes sense: when working with big data it is much better to have a limited but optimised toolset than a vast array of tools, some of which bring your entire virtual machine to a near halt. This means that I had to rethink the ways in which I would traditionally solve some SQL problems. Without variables, loops, the PIVOT operator, and many other features, I had to spend a fair amount of time researching alternative approaches. In the end, there is no harm in having multiple ways to solve a problem (in fact, there is quite a bit of benefit) and so I am glad to have been forced to adapt to these constraints. Another discrepancy between BiqQuery and my usual SQL work-flow is the awareness that you are being charged for every query you make. The costs of using the GCP are extremely reasonable, yet I still didn’t want to be wasting money on queries that could be ran more efficiently. This meant considering exactly how I phrased my queries, which joins I was using, and at what point I should create a static sample to work with rather than querying the ever-changing source dataset. The data we were working with was never large enough that subtle inefficiencies would cost us dearly but I’m glad to have had an opportunity to practice this awareness so that I am more prepared if I ever work with truly big data. The FutureEight weeks flew by so quickly. I really enjoyed my time working with the team and I would jumped at the opportunity to spend some more time working on my project. Because of the short time I had available, there are many sub-projects that I started but never got a chance to finish. In this section I will talk about some of these and what I believe the next steps are to develop them further. Regression ModelsIn week six of my internship, the HuYu team arranged a company-wide day-long hackathon. The goal of this was let the wider company have access to some of the data that we have been collecting and use this to develop to products. Over fifty people attended this event, many dialing in from around the globe via Microsoft Teams, and some great ideas and MVPs came out of it. Between the time spent explaining the nuances and limitations of my ElasticSearch model to colleagues, I came up with the idea of using a $L_1$-regularised Poisson regression model to help draw insight from the survey data. The idea behind this would be to take the survey data that we have collected and use the responses as the explanatory variable in a model explaining how many times each user buys a certain product, brand, or category. The previous method of linking survey and purchase data was fairly manual, taking a lot of time and perhaps missing out on insights that an analyst doesn’t test. This is the importance of $L_1$-regularisation in my model: this alters the cost function of the regression model so that any variables with no or low influence on the explanatory variable have their corresponding coefficients brought to zero. This then leaves a selection of survey responses which are discovered to be most influential on a given type of purchase behaviour. This is all processed automatically by an R script I produced. As a MVP I believed that this produced strong results and so would certainly be worthwhile implementing properly in the future. Tag-based ClassificationThe current method of hierarchical classification is flawed. It is unreasonable to assume that a given product can only have one classification. For example, a granola bar could be classified in many ways - breakfast-on-the-go, snack, healthy snack, cereal, vegetarian, gluten free. There is also an issue with the existing hierarchy in that it is designed for logistics, not business knowledge. A frozen pizza will be classified as FROZEN FOOD &gt; FROZEN PIZZA &amp; GARLIC BREAD &gt; FROZEN PIZZA but when a receipt just says PIZZA we have no way to even get past the first level of the hierarchy. A classification of READY MEALS &gt; PIZZA &gt; FROZEN PIZZA would be much more appropriate for business needs. I have completed some preliminary work in extracting tags for products using web-scraping, regular expressions, and some handy product APIs. I hope that these will be implemented once a direction for the classification problem is fully decided upon. ReflectionWith the outcomes of my internship out of the way, it is now a good time to reflect on what I produced and what I can learn from my experience. I will split this evaluation into the things that went well, the things that didn’t go so well, and the things that ended up causing significant hassle. The GoodFail-fast, Fail-oftenThroughout the internship, I adopted a philosophy of fail-fast, fail-often. Although this mentality is sometimes critisised in certain contexts (particularly, running an entire company according to this is a great way to head towards bankruptcy), I think the approach was very appropriate for my work. For those unfamiliar, the key concept of this philosophy is to try lots of things and not be afraid to through an idea away or put it aside for later study if it is not working. This is almost a literal embodiment of the dunnhumby values - have the passion and curiosity to seek new ideas, yet have the courage to discard them if things aren’t working. This philosophy is important for making sure that you don’t keep going further and further into a tunnel even when it’s become obvious there’s no light at the end. By adopting this framework in my own work, I was able to quickly test what ideas showed promise and then focus my attention onto developing those further. There were many times where I thought an idea was fruitless, I put it aside and then in a few weeks had a breakthrough that allowed me to push further with it. If I had instead sat hopelessly trying to push the idea before reaching a new perspective on the problem, I could have ended up wasting a lot of time. The key to this approach is to be honest with yourself about you goals and time-frame, and not be afraid to take a step back and evaluate the current intellectual landscape. Thinking Outside of the BoxWhen I was first introduced to my project, I was a little bit intimidated. The broad goal of ‘improving classification’ left me feeling like I had no clear direction. It wasn’t long until I started to see the bright side of this scenario though, that because there was no clear path, I was free to invent my own. I spent a lot of time researching: scanning through the ElasticSearch documentation to find new useful features, reading blog posts on similar NLP problems, studying literature about hierarchical classification. I then took the ideas that I had learnt and tried to morph them into new ones that were more suited for the problem that I was facing. This drive to discover more and then experiment with existing ideas is what led me to implement the top-$n$ fallback method which saw such success in classification accuracy. Other ideas like using Levenshtein edit distance to correct OCR mistakes were also born out of thinking about the bigger picture and trying to step away from what was already implemented. What this process has taught me is that with certain projects it may be beneficial to spend more time in this stage, white-boarding, prototyping, and discovering, to help unearth solutions that I would never have thought of my reaching for the immediately obvious approach. Minimum Viable ProductsOne thing that I discovered during my time at dunnhumby is that I have a surprisingly quick turn-around when it comes to developing minimum viable products. Many of the tasks I have discussed in this post - web-scraping, regression models, top-$n$ fallback - went from conception to MVP in less than a day. I then spent time after this refining and refactoring my code but I am proud that I was able to prove utility is such a short amount of time. This is however a double-edged sword. Developing too quickly can lead to bad code structure which is then a pain to fix in the future. I think the right balance to strike is to spend a short amount of time planning and structuring my code-base before beginning a new task and then engaging in flat-out development with a review period at the end. This format is something I hope to employ in my future work. The BadPandas is Not Your FriendI miss R. Dunnhumby is a very Python-centric company and so to maintain compatibility with my colleagues, the vast majority of my work has been in Python. There are aspects of Python that I really like but the packages pandas and matplotlib are not one of them. There are a few subtleties of pandas that slowly drove me crazy. The first is the inconsistency with base Python behaviour. Take this for an example. 1234567891011121314my_list = []for i in range(3): my_list.append(i + 1) import pandas as pdmy_df = pd.DataFrame(columns=['my_col'])for i in range(3): row = {'my_col': i} my_df.append(row, ignore_index=True) print(my_list)print('-' * 16)print(my_df) [1, 2, 3] ---------------- Empty DataFrame Columns: [my_col] Index: []The logic of both code blocks are exactly the same and yet the output is vastly different. This is because the .append() method for class list acts on the instance of the class directly whereas the equivalent method for class pd.DataFrame creates a copy of the instance, appends to that, and then returns the altered data frame. Since we ran my_df.append(...) in a for loop, this returned data frame is not printed so it is not immediately clear what is even going wrong. I fell for this inconsistency multiple times and there are many other similar instances throughout the package. Even if I did correct the code to say my_df = my_df.append(...) this still doesn’t work well. Yes, this will run, but it is massively inefficient. This is because pandas will create a new copy of my_df in memory for each iteration of the loop. This is fine when my_df is small, but as soon as it becomes even moderately sized, this will grind your computer to a halt. Yes, there are alternatives to get round this and similar issues. My point is though that there shouldn’t be so much confusion and inconsistency in the first place. What this has made me realise is that I will need go out and learn more about pandas before I encounter problems rather than as a troubleshooting step since there are many subtle issues with the package that could go unnoticed if not read about before. Wider NetworksIt is evident in hindsight that I did not adequately take advantage of wide range of talent that dunnhumby has in its network. Despite there being facility to do so, I never properly reached out to the company as a whole to see if there was anyone who had relevant expertise to offer me some insight which I could then look into further by myself. This oversight was epitomised when, in the last week of my internship, I learnt about an interesting unsupervised learning approach which could be used to solve some of the classification issues that I had been unable to tackle. If I had reached out to the company earlier, it is likely that this idea would have reached me earlier too and I may have had time to implement it. Because of by aversion to seek wider support, this idea is now just a footnote in my documentation. For my future placements and internships, I will make sure that I seek out relevant contacts as soon as possible and then utilise their experience appropriately. The UglyDocumentationProgrammers talk about bad documentation so much that it’s almost become a cliché in itself to talk about how we first learnt important documentation is. I have no plan to break from tradition so that is exactly what I plan to discuss. I left documenting my code until way too late in my internship. This was never going to lead to disaster; my code was fairly neat and easy to follow, and comments were regular. Further, I could remember most of the function of my code and why I decided to write it in the way I did. Despite the final push to document everything was a lot of stress which could have been avoided by regular documentation sessions. On top of that, this could have also improved the quality of my documentation considerably. Tangential to this is the importance of re-factoring code. I did not assign enough time to this and so when it came to document my code I found myself having to explain decisions that I wouldn’t myself agree with if I still had time to go back and fix them. This is almost certainly my biggest learning experience from the internship and is something I will take to heart in my upcoming projects and work. Jupyter NotebooksThis one will be short. Manually backup your Jupyter notebooks - you cannot trust the auto-saving or even the manually checkpointing. Further, when Jupyter deletes a file, it is likely that it will be gone forever. For that reason be very careful when batch deleting files as accidentally selecting an important file will cost you dearly. As you may guess, I learnt this the hard way. What’s Next?On the 2nd September I will be beginning a year-long industrial placement with AstraZeneca, working as a research data scientist. I am raring to go and am excited to implement all of the lessons that I have learnt throughout my time at dunnhumby. I also have a few personal projects over the next few months. The first is a course of data analysis using the tidyverse which I will be running at a handful of sixth form colleges. Another is a linear algebra Python package that I am developing with a few colleagues from university to practice version-control and Kanban. You can be expecting a new installment of this series as soon as any of these projects are complete.","link":"/upon-reflection-dunnhumby/"},{"title":"Bank Holiday Bodge: A Wall of Music","text":"(8:57am) The Art of the BodgeAt the time of writing this post, the UK is currently in the midst of a Summer bank holiday. I wanted to take this opportunity to introduce a new format of blog post that I intend to implement on every bank holiday of the following academic year. I have titled this format the ‘Bank Holiday Bodge’. In his delightful video ‘The Art of the Bodge‘, Tom Scott describes bodging as “a wonderful British word that means to patch together”. “A bodge is clumsy, it’s inelegant, it’ll fall apart, but it’ll work” he goes on to elaborate. I am a massive fan of ‘the bodge’. Although I love a well-documented, clear, and precise code-base, sometimes that is overkill for a project; it may be much more suitable to throw together a quick MVP out there and then tidy up the loose ends when it is actually necessary. Time is money after all, and there are many scenarios when a philosophy of build-fast, fail-fast is far more appropriate than devoting days to a project that may turn out to not even solve your problem. In this spirit, I have decided that I will spend my next year of bank holidays working on a variety of short projects. These are projects that I’ve had sitting in my ideas list for months or longer, but have been put off. Writing a polished blog post involves investing a large amount of time and energy and so there are many projects like these that risk waiting eternally to be written about. This blog post format is an attempt to remedy this issue and allow myself to have a short exploration of these ideas without feeling the need to produced a refined work. On top of this, I wish to use these posts as an opportunity to explore my data science workflow. In my other blog posts, mental blocks, mistakes and points of difficulty are often hidden away. In this format, however, I want them to be the focus. I will be writing the content of this post as I work on the technical side of things. I hope that this helps to capture the subtleties of development that may be lost in hindsight. I will go into this project with no preparation other than a few basic ideas for data sources and tooling. The idea being that the setup steps of installing packages, authenticating API access and similar mundane but potentially troublesome tasks are often swept under the rug when showcasing a project. I however, want to give them some limelight. So, without further ado, let’s begin. (9:16am) IntroductionA while back, a stumbled across Mario Klingemann’s incredible RasterFairy library. This is a Python package with the goal of taking any kind of 2D point cloud and transform it into a uniform lattice grid whilst trying to preserve neighborhood relations. A GIF showing this behaviour can be found on the README of the library’s GitHub repository although I have included it below for simplicity. The original point cloud in the GIF above looks very much like the output of typical dimensionality reduction algorithms such as t-SNE and UMAP. This got me thinking. Wouldn’t it be cool if I could take some sort of high-dimensional dataset, flatten it to a plane using dimensionality reduction, and then use the RasterFairy package to create an explorable grid of similarities. Even better, if the observations in the dataset corresponded to some visual element, I could arrange these in the generated grid format to produce a beautiful and explorable wall of data. The only question that remained in my mind was what dataset to use. Without a clear idea of what data to apply this approach to, this concept was left dormant for a while. That is, until I stumbled across the following dataset on Kaggle. This data set relates to the top 100 tracks on Spotify from 2018 and contains a multitude of musical attributes about each song including danceability, key, loudness, and tempo. This dataset was extracted and cleaned by Nadin Tamer using the Spotify Web API and the spotipy library. There is also a 2017 version of the dataset, leaving me with 200 observations to play with. This dataset appealed to me for two main reasons. The first is that the question of song similarity is already very ingrained in the public consciousness; we are very used to the idea of recommendation systems suggesting what song we might next want to listen to based off are currently listening (and obviously a lot more data gleaned from our historic use). Secondly, since every track in the dataset will belong to an album which will have a corresponding piece of cover art, this dataset will hopefully lead to a very attractive image to explore by the end of the project. (9:37am) First Steps(9:37am) Data ImportBefore we can get anywhere we will need to import and clean the two datasets we talked about above. I plan to import these straight from Kaggle using pandas. 1234import pandas as pdtop2017 = pd.read_csv('https://www.kaggle.com/nadintamer/' 'top-tracks-of-2017/downloads/' 'featuresdf.csv/1') Oh. I guess I won’t. Running the code above produced a nasty error. It turns out that you need to be logged in to download a dataset from Kaggle and so the above request attempts to read the login page as a CSV file. Let’s go back to basics and import the CSVs locally. 123import pandas as pdtop2017 = pd.read_csv('Resources/top2017.csv')top2018 = pd.read_csv('Resources/top2018.csv') Let’s take a look at the files to make sure everything is as expected. 1top2017.head() $(document).ready( function () {$('#table0').DataTable();} ); id name artists danceability energy key loudness mode speechiness acousticness instrumentalness liveness valence tempo duration_ms time_signature 0 7qiZfU4dY1lWllzX7mPBI Shape of You Ed Sheeran 0.825 0.652 1.0 -3.183 0.0 0.0802 0.5810 0.000000 0.0931 0.931 95.977 233713.0 4.0 1 5CtI0qwDJkDQGwXD1H1cL Despacito - Remix Luis Fonsi 0.694 0.815 2.0 -4.328 1.0 0.1200 0.2290 0.000000 0.0924 0.813 88.931 228827.0 4.0 2 4aWmUDTfIPGksMNLV2rQP Despacito (Featuring Daddy Yankee) Luis Fonsi 0.660 0.786 2.0 -4.757 1.0 0.1700 0.2090 0.000000 0.1120 0.846 177.833 228200.0 4.0 3 6RUKPb4LETWmmr3iAEQkt Something Just Like This The Chainsmokers 0.617 0.635 11.0 -6.769 0.0 0.0317 0.0498 0.000014 0.1640 0.446 103.019 247160.0 4.0 4 3DXncPQOG4VBw3QHh3S81 I'm the One DJ Khaled 0.609 0.668 7.0 -4.284 1.0 0.0367 0.0552 0.000000 0.1670 0.811 80.924 288600.0 4.0 1top2018.head() $(document).ready( function () {$('#table1').DataTable();} ); id name artists danceability energy key loudness mode speechiness acousticness instrumentalness liveness valence tempo duration_ms time_signature 0 6DCZcSspjsKoFjzjrWoCd God's Plan Drake 0.754 0.449 7.0 -9.211 1.0 0.1090 0.0332 0.000083 0.552 0.357 77.169 198973.0 4.0 1 3ee8Jmje8o58CHK66QrVC SAD! XXXTENTACION 0.740 0.613 8.0 -4.880 1.0 0.1450 0.2580 0.003720 0.123 0.473 75.023 166606.0 4.0 2 0e7ipj03S05BNilyu5bRz rockstar (feat. 21 Savage) Post Malone 0.587 0.535 5.0 -6.090 0.0 0.0898 0.1170 0.000066 0.131 0.140 159.847 218147.0 4.0 3 3swc6WTsr7rl9DqQKQA55 Psycho (feat. Ty Dolla $ign) Post Malone 0.739 0.559 8.0 -8.011 1.0 0.1170 0.5800 0.000000 0.112 0.439 140.124 221440.0 4.0 4 2G7V7zsVDxg1yRsu7Ew9R In My Feelings Drake 0.835 0.626 1.0 -5.833 1.0 0.1250 0.0589 0.000060 0.396 0.350 91.030 217925.0 4.0 Looking good. Next we need to join the datasets. We’ll first add a column to each to store which year the songs were from just in case this comes in handy at a later point. 1234top2017['year'] = 2017top2018['year'] = 2018topCombi = pd.concat([top2017, top2018])topCombi = topCombi.reset_index(drop=True) We now have a dataset of 200 tracks. Since the ended goal is to visualise these in a grid it would be nice to have a number with a nice factorisation. The best factorisation for 200 is $10\\times20$ which is a weird ratio to use. Instead I have decide to throw out 4 random tracks leaving is with 196, a perfect square, which will be much better for visualising. 12345import numpy as npnp.random.seed(1729)drop_indices = np.random.choice(topCombi.index, 4, replace=False)topCombi = topCombi.drop(drop_indices)topCombi = topCombi.reset_index(drop=True) (9:51am) CleaningWe are now ready to perform to basic data cleansing. The most obvious issue with the dataset is that key is encoded using the ordinals 0 to 11. This does a terrible job of capturing the behaviour of musical keys. For a start, musical keys obey a cyclical structure. From a chromatic sense, the note C (0) is just as close to C# (1) as it is to B (11) and yet this dataset puts B and C as far apart as possible. The solution to this is to map the values of this variable into locations around a circle. This can be performed using some basic trigonometry. 123# import numpy as np - done abovetopCombi['key_cos'] = np.cos(topCombi.key * (2.0 * np.pi / 12))topCombi['key_sin'] = np.sin(topCombi.key * (2.0 * np.pi / 12)) The key of C (0) now maps to the pair (0, 1), with C# (1) mapping to (0.866, 0.5) and B (11) mapping to (0.866, -0.5). That is, C is the same distance from C# and B just as we would expect. It could easily be argued that this alone still doesn’t capture the full behaviour of keys. Ask any musician, and they would most likely say that C (0) and F (5) are more closely related than say C (0) and Eb (3) even though the latter two are closer chromatically. This is because they are more closely related through the circle of fifths. It wouldn’t be too difficult to create a feature for this using some basic modular arithmetic. The issue arises when using both the chromatic and circle of fifths approach together. Neither of these perfectly capture the relations of keys by themselves and including both threatens introducing bias towards the key. I have therefore decided to stick with only cyclic chromatic distance as to layman listeners, this is the most intuitive measure of key distance. We can now drop the original key column. 1topCombi = topCombi.drop('key', axis=1) The next step in cleaning the dataset is to normalise some of the columns. The majority of the columns - such as danceability or energy - have a scale going from zero to one (their ranges are usually smaller but values of zero and one are completely valid and values often come close to them). This gives us a problem when looking at the columns tempo, duration_ms, and time_signature. The simplest solution to this is to take each of these columns and use a linear transformation to map them to the range (0, 1). 12345678topCombi['tempo_norm'] = ((topCombi.tempo - np.min(topCombi.tempo)) / (np.max(topCombi.tempo) - np.min(topCombi.tempo)))topCombi['duration_norm'] = ((topCombi.duration_ms - np.min(topCombi.duration_ms)) / (np.max(topCombi.duration_ms) - np.min(topCombi.duration_ms)))# fix time signature mistake for DJ Khaled - No BrainertopCombi['time_signature'] = topCombi.time_signature.replace(5, 4)# time signature is only 3 or 4topCombi['time_signature_norm'] = topCombi.time_signature - 3 As you can see on the third line of code, I had to make a correction to the dataset. This is because ‘No Brainer’ by DJ Khaled was marked as being in 5/4 time. Listening to the song, it is clear that that is not the case and so it was appropriate to correct this. We can now drop the original columns. 1topCombi = topCombi.drop(['tempo', 'duration_ms', 'time_signature'], axis=1) 1topCombi $(document).ready( function () {$('#table2').DataTable();} ); id name artists danceability energy loudness mode speechiness acousticness instrumentalness liveness valence year key_cos key_sin tempo_norm duration_norm time_signature_norm 0 7qiZfU4dY1lWllzX7mPBI Shape of You Ed Sheeran 0.825 0.652 -3.183 0.0 0.0802 0.5810 0.000000 0.0931 0.9310 2017 8.660254e-01 0.500000 0.230067 0.428732 1.0 1 5CtI0qwDJkDQGwXD1H1cL Despacito - Remix Luis Fonsi 0.694 0.815 -4.328 1.0 0.1200 0.2290 0.000000 0.0924 0.8130 2017 5.000000e-01 0.866025 0.177848 0.413580 1.0 2 4aWmUDTfIPGksMNLV2rQP Despacito (Featuring Daddy Yankee) Luis Fonsi 0.660 0.786 -4.757 1.0 0.1700 0.2090 0.000000 0.1120 0.8460 2017 5.000000e-01 0.866025 0.836723 0.411635 1.0 3 6RUKPb4LETWmmr3iAEQkt Something Just Like This The Chainsmokers 0.617 0.635 -6.769 0.0 0.0317 0.0498 0.000014 0.1640 0.4460 2017 8.660254e-01 -0.500000 0.282257 0.470434 1.0 4 3DXncPQOG4VBw3QHh3S81 I'm the One DJ Khaled 0.609 0.668 -4.284 1.0 0.0367 0.0552 0.000000 0.1670 0.8110 2017 -8.660254e-01 -0.500000 0.118506 0.598949 1.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 191 630sXRhIcfwr2e4RdNtjK Rewrite The Stars Zac Efron 0.684 0.619 -7.005 1.0 0.0386 0.0716 0.000000 0.1220 0.2840 2018 5.000000e-01 -0.866025 0.445505 0.378266 1.0 192 2xmrfQpmS2iJExTlklLoA I Miss You (feat. Julia Michaels) Clean Bandit 0.638 0.658 -6.318 1.0 0.0456 0.2450 0.000004 0.0919 0.3300 2018 6.123234e-17 1.000000 0.297502 0.342006 1.0 193 5WvAo7DNuPRmk4APhdPzi No Brainer DJ Khaled 0.552 0.760 -4.706 1.0 0.3420 0.0733 0.000000 0.0865 0.6390 2018 1.000000e+00 0.000000 0.524479 0.510254 1.0 194 1j4kHkkpqZRBwE0A4CN4Y Dusk Till Dawn - Radio Edit ZAYN 0.258 0.437 -6.593 0.0 0.0390 0.1010 0.000001 0.1060 0.0967 2018 8.660254e-01 -0.500000 0.853102 0.445128 1.0 195 3EPXxR3ImUwfayaurPi3c Be Alright Dean Lewis 0.553 0.586 -6.319 1.0 0.0362 0.6970 0.000000 0.0813 0.4430 2018 8.660254e-01 -0.500000 0.457645 0.312932 1.0 196 rows × 18 columns (10:21am) Dimensionality Reduction(10:21am) Basic SolutionNow that are dataset is relatively clean, we can apply dimensionality reduction. I have decided to use UMAP over t-SNE due to its superior efficiency and the ease of fine-tuning the model parameters. I have only ever used UMAP in R before so firstly I will need to install the corresponding Python package and its dependencies. The syntax for using UMAP in Python is incredibly easy. Here we will use default parameters and see what sort of mapping this gives us. 1234# columns to use for dimensionality reductiontopCombi_data = topCombi.drop(['id', 'name', 'artists'], axis=1)import umap When I first imported umap, I received a stream of error messages. This most likely has to do with me using Windows as my operating system (why is stable RTX support for Linux taking so long!?). A quick fix was found on this GitHub issue although I don’t know whether this will lead to me more downstream issues. We’ll have to see, but for now we can carry on with fingers crossed. 12# seed used for reproducibilityembedding = umap.UMAP(random_state=1729).fit_transform(topCombi_data) Good grief. Running this produced an even more terrifying error than the last. Thankfully this problem was already discussed in this GitHub issue. The solution to this seems to be to go through every module file and remove any instance of parallel=True in function calls. I thought I could easily do this with a Powershell find replace command but the inclusion of multiple arguments, of which parallel could be in any place made it difficult to do this and leave a comment saying where I changed things so in the end I decided to use a semi-manual approach. This is far from an ideal solution but it works and so we can continue. Tom Scott would certainly be proud of this bodge work! Let’s now visualise our embedding. We colour by main artists as a sanity check. 1234artist_counts = topCombi.artists.value_counts()small_artists = topCombi.artists.isin(artist_counts.index[artist_counts &lt; 5])main_artists = topCombi.artists.copy()main_artists.loc[small_artists] = 'Other' 12345678910%config InlineBackend.figure_formats = ['svg']import matplotlib.pyplot as plt%matplotlib inlinex = embedding[:, 0]y = embedding[:, 1]plt.scatter(x, y, c=pd.factorize(main_artists)[0])plt.show() Great! It works. The colouring is also very reasonable considering how much an artist’s style can change between songs. There is however some general regions - more purple on right, more yellow on left, etc. - which suggests that the dimensionality reduction was reasonable. I was very close to throwing out UMAP all together and resorting to t-SNE but after seeing a generally acceptable plot produced in a few seconds, I am happy I stuck with it. Now, onto the parameters. (10:58am) Parameter TweaksThere are 3 main parameters which control the behaviour of UMAP. I will walk through them one at a time and explain how I came to choose their value. (10:59am) n_neighborsThis parameter is used to control how UMAP balances local versus global structure in the data. It does this by constraining the number of local neighbours that UMAP will look at when attempting to learn the manifold structure of the data. Therefore, a low value of n_neighbors will push UMAP to concentrate on local structure (possibly at the detriment of the global structure) and a high value of n_neighbors will push UMAP to focus on the bigger picture at the expense of some detail. The default value for this parameter is 15 and typical values range from 2 (very local view) to 200 (highly global view). I have decided to use a value of 10. This is because, although I think some weight should be put towards the global view, I think a focus on what songs are directly similar to one another as opposed to what regions of the larger feature space they fall in is more important. (11:06 am) min_distThe min_dist parameter controls how tightly UMAP is allowed to pack points together. It is literally the minimum distance apart that points are allowed to be in the embedding space. Therefore, low values of min_dist will allow points to more heavily clump together whereas large values will force points to have some separation and preserve the broad topological structure. The default value for min_dist is 0.1 and typical values range from 0.0 to 0.99. I have decided to use a slightly larger value of 0.5. This will help when apply the RasterFairy algorithm as the points will already be fairly evenly distributed although it is not high enough that local structure is impacted to strongly. (11:12 am) metricThe metric parameter simply controls how distance is computed in the ambient space of the input data. Changing the metric can have significant effects on the final embedding and so care should be taken when altering it. The default metric is the standard Euclidean. This seems generally appropriate over other Minkowski style metrics. The Chebyshev ($L_\\infty$) metric could easily put two very similar songs far away in feature space because they have one significant difference whereas the Manhattan ($L_1$) metric may not adequately punish large disparities in multiple features. I haven’t even considered any non-standard metrics as I think the risk of generating a useless embedding is too high. (11:17 am) The Final EmbeddingAfter tweaking the parameters we end up with this embedding. We, again, colour by the main artists. 123456789embedding = umap.UMAP(n_neighbors=10, min_dist=0.5, metric='euclidean', random_state=1729).fit_transform(topCombi_data)x = embedding[:, 0]y = embedding[:, 1]plt.scatter(x, y, c=pd.factorize(main_artists)[0])plt.show() (11:22am) RasterficationWe can now use the RasterFairy package to transform this embedding into a uniform grid of points. I have never used this package before so I will begin by installing it and importing. 1import rasterfairy Thankfully this was much smoother than installing UMAP, sparking no issues at all. The main transformation function works directly on a 2D numpy array so we can apply it straight away. 1grid, dims = rasterfairy.transformPointCloud2D(embedding) This last step caught me out at first. It turns out that the transformPointCloud2D function returns a tuple. The first entry of this is the grid we’re after and the second is the grid dimensions that algorithm decided to use (in this case, 14x14, as we expected). This resulted in some very aggressive error messages from matplotlib. Once I figured out what was wrong, however, it was a very quick fix. We can now visualise the result. 1234x = grid[:, 0]y = grid[:, 1]plt.scatter(x, y, c=pd.factorize(main_artists)[0])plt.show() Looking great! Those three purple points together in the centre are a great sign and we can also see a few green/yellow tracks near each other. Before we continue any further, it is important to join our embedded points back with the corresponding track IDs, names, and artists. 12grid_df = pd.DataFrame(grid, columns=['dim1', 'dim2'])topCombi_grid = topCombi[['id', 'name', 'artists']].join(grid_df) We can now take a look at the final result. 1topCombi_grid.head() $(document).ready( function () {$('#table3').DataTable();} ); id name artists dim1 dim2 0 7qiZfU4dY1lWllzX7mPBI Shape of You Ed Sheeran 7.0 7.0 1 5CtI0qwDJkDQGwXD1H1cL Despacito - Remix Luis Fonsi 4.0 9.0 2 4aWmUDTfIPGksMNLV2rQP Despacito (Featuring Daddy Yankee) Luis Fonsi 5.0 9.0 3 6RUKPb4LETWmmr3iAEQkt Something Just Like This The Chainsmokers 0.0 9.0 4 3DXncPQOG4VBw3QHh3S81 I'm the One DJ Khaled 13.0 1.0 As we can see, both versions Despacito are right next to each other. Things are looking good. Now, all we are left to do is download cover art and pop them into a ‘wall of music’ based on the above grid positions. First though, I’m going to pop out for lunch. Let’s carry on come 1 o’clock. (12.55pm) Building the Wall(12.55pm) Sourcing Cover ArtIn order to download album cover art, I am going to use the Spotify API. An introductory tutorial on how to use this within Python can be found here. When scanning through this guide, I realised that I have made a bit of an oversight with regards to this project. I had assumed that the top tracks data set I was using was a bespoke generation. Since the dataset only existed for two years it seemed reasonable that the song attributes had been generated by Nadin Tamer, the uploader of the dataset. It turns out though, that the Spotify API can give you these attributes for any song. It therefore seems very limiting to look at only that small sample of 200. Never-the-less, we are too far in now to go back. This is a perfect lesson on the art of ‘the bodge’. Yes, using only those 200 tracks is limiting. Yes, if I were to redo this project, I would have used the Spotify API directly. But, it doesn’t matter. Once I have reached a MVP, small tweaks like that are easy to implement. Perhaps if I ever have new inspiration for this project, I will return and use a much wider range a tracks. This oversight aside, I can begin scraping album cover art. The first step in doing this is to apply for Spotify API permissions and credentials as per the tutorial. This is relatively painless. Then after installing the spotipy package, we are ready to start scraping. 123456import spotipyfrom spotipy.oauth2 import SpotifyClientCredentialsmy_creds_manager = SpotifyClientCredentials(client_id=my_id, client_secret=my_secret)sp = spotipy.Spotify(client_credentials_manager=my_creds_manager) First we test that everything is working with an example query. 123name = \"Lobachevsky\"result = sp.search([name])result['tracks']['items'][0]['artists'][0]['name'] &apos;Tom Lehrer&apos;Just as expected. We can now loop through each track ID in our dataset, find the album the track belongs to, and then download the corresponding cover art. {% codeblock lang:python %} for id in topCombi_grid.id: track = sp.track(id) {% endcodeblock %} No luck. Running the code above will return an error, stating that the ID used is invalid. This is very reasonable as manually entering the ID as part of a Spotify URL gives a 404 error. Perhaps the song IDs are mutable and have since changed. We’ll have to resort to plan B - search each song using the given name and artist and then take the top result. Notice how I set market='GB' in the call to the search function. I have no idea what this is needed and it took me quite a while to figure this out. You’d think an API like this would not be region specific, but with it the number of returned results is zero on many searches. At to the peculiarity, setting this to market='US' didn’t help at all. {% codeblock lang:python %} import urllib.request for __, row in topCombi_grid.iterrows(): track_id = row['id'] name = row['name'] artist = row['artists'] result = sp.search(f'{name} {artist}', market='GB') try: # get cover art URL and check size album = result['tracks']['items'][0]['album'] cover_url = album['images'][1]['url'] dim = album['images'][1]['height'] assert dim == 300 # download cover art urllib.request.urlretrieve(cover_url, f'Resources/cover_art/{track_id}.jpg') except IndexError: print(f'No results for {name} by {artist} [{track_id}]') {% endcodeblock %} No results for Te Bot? - Remix by Nio Garcia [3V8UKqhEK5zBkBb6d6ub8] No results for D?jala que vuelva (feat. Manuel Turizo) by Piso 21 [1j6xOGusnyXq3l6IryKF3]For 186 out of the 196 tracks we are using, this worked perfectly. The other 8 tracks however caused some trouble. This is because they contain special characters which haven’t been encoded. I tried to trace back where this issue comes from and it turns out that this present in the original dataset. Since only a few tracks are troublesome, I will manually edit any missed special characters with the nearest ASCII equivalent and re-scrape. 1234567891011replacements = { 'Te Bot? - Remix': 'Te Bote - Remix', '?chame La Culpa': 'Echame La Culpa', 'Ti?sto': 'Tiesto', 'D?jala que vuelva (feat. Manuel Turizo)': 'Dejala que vuelva (feat. Manuel Turizo)', 'Taki Taki (with Selena Gomez, Ozuna &amp; Cardi B)': 'Taki Taki (feat. Selena Gomez, Ozuna &amp; Cardi B)', 'Perfect Duet (Ed Sheeran &amp; Beyonc?)': 'Perfect Duet (Ed Sheeran &amp; Beyonce)', 'Coraz?n (feat. Nego do Borel)': 'Corazon (feat. Nego do Borel)', 'S?guelo Bailando': 'Siguelo Bailando'}topCombi_grid = topCombi_grid.replace(replacements) 123456789101112131415161718import urllib.requestfor index, row in topCombi_grid.iterrows(): track_id = row['id'] name = row['name'] artist = row['artists'] if name in replacements.values() or artist in replacements.values(): result = sp.search(f'{name} {artist}', market='GB') try: # get cover art URL and check size album = result['tracks']['items'][0]['album'] cover_url = album['images'][1]['url'] # 300px versions dim = album['images'][1]['height'] assert dim == 300 # download cover art urllib.request.urlretrieve(cover_url, f'Resources/cover_art/{track_id}.jpg') except IndexError: print(f'No results for {name} by {artist} [{track_id}]') After running both of these code blocks, we still only have 188 pieces of cover art. After some digging, I discovered that this is because some songs are included in both the 2017 and 2018 dataset. Since we already have the 196 points we want for a nice square grid this is too late to fix. I can’t see this presenting any other issues than duplication however so we can let it slide. (2.03pm) ZoomifyingBefore we look at displaying the cover art wall in an interactive format, we need to use Python to ‘glue’ our images together using the grid above. This can be done fairly easily using the built-in Image module of the Python Imaging Library (PIL). My original plan was to the OpenSeadragon viewer to visualise the final wall of songs. This was because it has features for adding overlays which I could use to caption the songs when the user hovers over a certain piece of cover art. After reading through the documentation today in preparation for use, I very quickly decided that this is not the best approach. The method for adding overlays is poorly documented and really knowledge of jQuery, and the Python package is very minimal in its feature set so offers no help. I have instead decided to embed the labels in the combined image itself using the ImageDraw and ImageFont modules from PIL. I will then use a more friendly image viewer called Zoomify. I would much prefer to have a interactive labels on the image but if I want to keep this project under a day long, that will not be possible. 12345678910111213141516171819202122232425262728293031from PIL import Image, ImageDraw, ImageFont# constantsTINT_COLOR = (0, 0, 0) # blackTRANSPARENCY = .50 # degree of transparencyOPACITY = int(255 * TRANSPARENCY)img = Image.new('RGBA', (4200, 4200))fnt = ImageFont.truetype('Resources/Arial Bold.ttf', 11for __, row in topCombi_grid.iterrows(): x_pos = row['dim1'] y_pos = row['dim2'] track_id = row['id'] name = row['name'] artist = row['artists'] # paste cover art onto wall with Image.open(f'Resources/cover_art/{track_id}.jpg') as tmp: img.paste(tmp, (int(x_pos * 300), int(y_pos * 300))) # create transparent background for label overlay = Image.new('RGBA', img.size, TINT_COLOR+(0,)) draw = ImageDraw.Draw(overlay) draw.rectangle(((int(x_pos * 300), int(y_pos * 300)), (int((x_pos + 1) * 300), int(y_pos * 300 + 20))), fill=TINT_COLOR+(OPACITY,)) img = Image.alpha_composite(img, overlay) # add song/artist label d = ImageDraw.Draw(img) d.text((int(x_pos * 300 + 5), int(y_pos * 300 + 5)), f\"{name} - {artist}\", font = fnt, fill=(255, 255, 255))# remove alpha channelimg = img.convert('RGB')img.save(\"Resources/cover_art_wall.jpg\") This leaves us with a whopping 4200x4200 pixel image. Thankfully due to JPG compression, this still only comes to a few megabytes. The finish line is in sight. The last step is to add Zoomify Javascript code to our site and add some code to this page to create a viewer for our image. When that is all complete, this is what we end up with. A full screen version can be viewed here. ## (3.47pm) Evaluation PerformanceAs one-day sprints go, I think this has been quite successful. There were several obstacles along the way that could have put an end to the project yet these were carefully circumvented. The part of this project that took up the most time was certainly the setup of an image viewer. Trying to achieve this took me down many false pathways and in the end I only reached a solution by throwing up my hands in defeat and using an iframe. What made this so challenging is two-fold. Firstly, my knowledge of web development, and in particular Javascript/jQuery, is rather limited. Even if I was more knowledgeable, though, I think this still would have been a real challenge since this website is not directly made of static, mutable files but rather built from small generic components that are then used by the static site generator Hexo to build the final site. This adds a whole other layer of complexity to setting up a viewer. I think this methodology shows promise, though. I am not at all well-versed in the popular music of this decade so it is difficult for me to give a detailed analysis of performance. I can, however, point out a few features which give me some confidence that there is potential. For a start, we can see obvious clumping of tracks by the same artist. Ed Sheeran and Drake in the middle, Migos at the bottom, Marshmello on the right. We can also see similar styles of artist together such as Maroon 5 and Bruno Bars at the top. LimitationsThe limitations of this project are quite clear. The most obvious is the failure to implement an image viewer in the way that I wanted. If I were to redo this project properly, this is where I would focus my resources, perhaps asking someone more knowledgeable on the subject for support. The handling of special characters when collecting cover art prevents instantly scalability, but since this is an artifact of the data source (which I wouldn’t be using if I were to expand this project) I don’t see this as a major limitation Where Next?Now that I am aware of how easy it is to source attributes for any song on Spotify, this opens up to possibility for producing a similar analysis for a wider range of music - perhaps the top 50 albums of the last 50 years. Even further, a bold idea would be to create a web-app that allowed a user to sign into Spotify and create a ‘wall of music’ for themselves. This is certainly beyond my current understanding of web development but as I learn more about the field, this may become a possibility. The time is 4:23pm. I’ll have a read through the post, do a bit of spell checking, and then I think I’ll ‘head home’ early.","link":"/wall-of-music/"},{"title":"Bank Holiday Bodge: Daily XKCD Mailer","text":"For those only interested in the code and setup guide for the mailer, see the corresponding GitHub repository. “Hey, that reminds of that XKCD comic about XYZ! Have you seen that one?” As a generally techy person who spends most of his working and social time with equally techy individuals, I probably hear the above utterance more often than “How are you today?” or “Good Morning” (though perhaps that speaks more towards the introversion of some of my colleagues). Although there are rare occasions when I can answer it with “yes, that one was great!”, I sadly find myself having no clue what they are talking about more often than not. The reason for this is that I discovered the XKCD series relatively late in the comic’s lifespan. By that time, Randall had already released close to 2000 comics, and the thought of catching up seemed daunting. On the other hand, the more mature data scientists, software engineers, and web developers that I know were well into their careers by the time XKCD was first gaining traction, and so were able to keep track of its progression. This desire to be in on the jokes (without having to first sneak a quick Google of the comic on my phone) led me to decide that 2020 would be the year that I finally got caught up on the entire catalogue. The issue was, I didn’t want to view the series in chronological order. It took some time for Randall to develop his style and so what I really wanted was a random selection of his comics from the entire time-span of the series’ existence. I could have simply manually chosen a random selection of comics each day but then I would have to be aware of not repeating choices and making sure I saw the first parts of multi-part comics before the rest. This seemed like far too much fuss. Instead, I decided that Python could lend me a hand. I created a script to scrape the all comics pages of explainxkcd.com and then send me a handful of comics. Furthermore, the script ensures that it never repeats a comic, only sends the first unread comic of a multi-part series, and safely terminates when the user is up-to-date. I then set this script up on Google Cloud Platform (GCP) so that it would execute every weekday at 9am to send an email to my work address with my daily selection of comics. This project has larger splintered off from my blog to form its own GitHub repository. This is where you can find instructions on how to setup the script on GCP. I will use the rest of this post to simply justify some of my choices in developing the script and choosing GCP as its host. The structure of my code is quite standard though I have tried to make it as flexible as possible. That way, with a bit of knowledge of CSS and RegEx, the script can be easily to scrape another series (say PHD comics). There were many ways that I could have executed this script on a daily basis. My first attempt was with PythonAnywhere but had to abandon this idea when I discovered their strict webscraping policy. This left me to choose between Amazon Web Services (AWS) and GCP. I decided to go with the latter as I have more experience with it and, more importantly, you are not required to supply billing information to use their free tier as is the case with AWS. This means that there was no risk of an accidental, surprise billing. I hope that any other XKCD fans find this script useful. Anyway, happy new year, and all the best for 2020!","link":"/xkcd-mailer/"}],"tags":[{"name":"no-coding","slug":"no-coding","link":"/tags/no-coding/"},{"name":"hypothesis-testing","slug":"hypothesis-testing","link":"/tags/hypothesis-testing/"},{"name":"story","slug":"story","link":"/tags/story/"},{"name":"ethics","slug":"ethics","link":"/tags/ethics/"},{"name":"shiny","slug":"shiny","link":"/tags/shiny/"},{"name":"r","slug":"r","link":"/tags/r/"},{"name":"binomial-distribution","slug":"binomial-distribution","link":"/tags/binomial-distribution/"},{"name":"coins","slug":"coins","link":"/tags/coins/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"combinatorics","slug":"combinatorics","link":"/tags/combinatorics/"},{"name":"problem-solving","slug":"problem-solving","link":"/tags/problem-solving/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"animation","slug":"animation","link":"/tags/animation/"},{"name":"bank-holiday-bodge","slug":"bank-holiday-bodge","link":"/tags/bank-holiday-bodge/"},{"name":"excel","slug":"excel","link":"/tags/excel/"},{"name":"solver","slug":"solver","link":"/tags/solver/"},{"name":"linear-programming","slug":"linear-programming","link":"/tags/linear-programming/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"benchmarking","slug":"benchmarking","link":"/tags/benchmarking/"},{"name":"data-mining","slug":"data-mining","link":"/tags/data-mining/"},{"name":"linear-algebra","slug":"linear-algebra","link":"/tags/linear-algebra/"},{"name":"integration","slug":"integration","link":"/tags/integration/"},{"name":"probability","slug":"probability","link":"/tags/probability/"},{"name":"simulation","slug":"simulation","link":"/tags/simulation/"},{"name":"normal-distribution","slug":"normal-distribution","link":"/tags/normal-distribution/"},{"name":"lesson","slug":"lesson","link":"/tags/lesson/"},{"name":"tidyverse","slug":"tidyverse","link":"/tags/tidyverse/"},{"name":"sports","slug":"sports","link":"/tags/sports/"},{"name":"visualisation","slug":"visualisation","link":"/tags/visualisation/"},{"name":"web-development","slug":"web-development","link":"/tags/web-development/"},{"name":"clustering","slug":"clustering","link":"/tags/clustering/"},{"name":"graph-theory","slug":"graph-theory","link":"/tags/graph-theory/"},{"name":"probabilistic-method","slug":"probabilistic-method","link":"/tags/probabilistic-method/"},{"name":"puzzle","slug":"puzzle","link":"/tags/puzzle/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"magrittr","slug":"magrittr","link":"/tags/magrittr/"},{"name":"combinatorical optimisation","slug":"combinatorical-optimisation","link":"/tags/combinatorical-optimisation/"},{"name":"number-theory","slug":"number-theory","link":"/tags/number-theory/"},{"name":"reflection","slug":"reflection","link":"/tags/reflection/"},{"name":"dimensionality-reduction","slug":"dimensionality-reduction","link":"/tags/dimensionality-reduction/"},{"name":"gcp","slug":"gcp","link":"/tags/gcp/"}],"categories":[{"name":"Statistics","slug":"Statistics","link":"/categories/Statistics/"},{"name":"Data Science","slug":"Data-Science","link":"/categories/Data-Science/"},{"name":"Visualisation","slug":"Visualisation","link":"/categories/Visualisation/"},{"name":"Probability","slug":"Statistics/Probability","link":"/categories/Statistics/Probability/"},{"name":"Computer Science","slug":"Computer-Science","link":"/categories/Computer-Science/"},{"name":"Visualisation","slug":"Data-Science/Visualisation","link":"/categories/Data-Science/Visualisation/"},{"name":"Best Practice","slug":"Data-Science/Best-Practice","link":"/categories/Data-Science/Best-Practice/"},{"name":"Operational Research","slug":"Operational-Research","link":"/categories/Operational-Research/"},{"name":"Interactive","slug":"Visualisation/Interactive","link":"/categories/Visualisation/Interactive/"},{"name":"System","slug":"System","link":"/categories/System/"},{"name":"Mathematics","slug":"Mathematics","link":"/categories/Mathematics/"},{"name":"Coding Problems","slug":"Computer-Science/Coding-Problems","link":"/categories/Computer-Science/Coding-Problems/"},{"name":"Meta","slug":"Meta","link":"/categories/Meta/"},{"name":"Machine Learning","slug":"Data-Science/Machine-Learning","link":"/categories/Data-Science/Machine-Learning/"},{"name":"Preprocessing","slug":"Data-Science/Preprocessing","link":"/categories/Data-Science/Preprocessing/"},{"name":"Algorithms","slug":"Computer-Science/Algorithms","link":"/categories/Computer-Science/Algorithms/"},{"name":"Analysis","slug":"Mathematics/Analysis","link":"/categories/Mathematics/Analysis/"},{"name":"Cloud Computing","slug":"Computer-Science/Cloud-Computing","link":"/categories/Computer-Science/Cloud-Computing/"},{"name":"Combinatorics","slug":"Mathematics/Combinatorics","link":"/categories/Mathematics/Combinatorics/"}]}